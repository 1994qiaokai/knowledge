<?xml version='1.0' encoding='UTF-8'?>
<result plugin="junit@1.2-beta-4">
  <suites>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/common/network-common/target/test-reports/org.apache.spark.network.ChunkFetchIntegrationSuite.xml</file>
      <name>org.apache.spark.network.ChunkFetchIntegrationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.487</duration>
      <cases>
        <case>
          <duration>0.054</duration>
          <className>org.apache.spark.network.ChunkFetchIntegrationSuite</className>
          <testName>fetchNonExistentChunk</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.282</duration>
          <className>org.apache.spark.network.ChunkFetchIntegrationSuite</className>
          <testName>fetchFileChunk</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.055</duration>
          <className>org.apache.spark.network.ChunkFetchIntegrationSuite</className>
          <testName>fetchBothChunks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.network.ChunkFetchIntegrationSuite</className>
          <testName>fetchChunkAndNonExistent</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.06</duration>
          <className>org.apache.spark.network.ChunkFetchIntegrationSuite</className>
          <testName>fetchBufferChunk</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/common/network-common/target/test-reports/org.apache.spark.network.ProtocolSuite.xml</file>
      <name>org.apache.spark.network.ProtocolSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.019000001</duration>
      <cases>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.network.ProtocolSuite</className>
          <testName>responses</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.network.ProtocolSuite</className>
          <testName>requests</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/common/network-common/target/test-reports/org.apache.spark.network.RequestTimeoutIntegrationSuite.xml</file>
      <name>org.apache.spark.network.RequestTimeoutIntegrationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>31.348999</duration>
      <cases>
        <case>
          <duration>11.223</duration>
          <className>org.apache.spark.network.RequestTimeoutIntegrationSuite</className>
          <testName>furtherRequestsDelay</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>10.094</duration>
          <className>org.apache.spark.network.RequestTimeoutIntegrationSuite</className>
          <testName>timeoutCleanlyClosesClient</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>10.032</duration>
          <className>org.apache.spark.network.RequestTimeoutIntegrationSuite</className>
          <testName>timeoutInactiveRequests</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/common/network-common/target/test-reports/org.apache.spark.network.RpcIntegrationSuite.xml</file>
      <name>org.apache.spark.network.RpcIntegrationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.491</duration>
      <cases>
        <case>
          <duration>0.134</duration>
          <className>org.apache.spark.network.RpcIntegrationSuite</className>
          <testName>sendOneWayMessage</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.121</duration>
          <className>org.apache.spark.network.RpcIntegrationSuite</className>
          <testName>singleRPC</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.044</duration>
          <className>org.apache.spark.network.RpcIntegrationSuite</className>
          <testName>throwErrorRPC</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.044</duration>
          <className>org.apache.spark.network.RpcIntegrationSuite</className>
          <testName>doubleTrouble</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.056</duration>
          <className>org.apache.spark.network.RpcIntegrationSuite</className>
          <testName>doubleRPC</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.network.RpcIntegrationSuite</className>
          <testName>returnErrorRPC</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.064</duration>
          <className>org.apache.spark.network.RpcIntegrationSuite</className>
          <testName>sendSuccessAndFailure</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/common/network-common/target/test-reports/org.apache.spark.network.StreamSuite.xml</file>
      <name>org.apache.spark.network.StreamSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.65199995</duration>
      <cases>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.network.StreamSuite</className>
          <testName>testSingleStream</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.311</duration>
          <className>org.apache.spark.network.StreamSuite</className>
          <testName>testMultipleStreams</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.26</duration>
          <className>org.apache.spark.network.StreamSuite</className>
          <testName>testConcurrentStreams</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.033</duration>
          <className>org.apache.spark.network.StreamSuite</className>
          <testName>testZeroLengthStream</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/common/network-common/target/test-reports/org.apache.spark.network.TransportClientFactorySuite.xml</file>
      <name>org.apache.spark.network.TransportClientFactorySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>3.112</duration>
      <cases>
        <case>
          <duration>0.394</duration>
          <className>org.apache.spark.network.TransportClientFactorySuite</className>
          <testName>reuseClientsUpToConfigVariable</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.954</duration>
          <className>org.apache.spark.network.TransportClientFactorySuite</className>
          <testName>reuseClientsUpToConfigVariableConcurrent</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.119</duration>
          <className>org.apache.spark.network.TransportClientFactorySuite</className>
          <testName>closeBlockClientsWithFactory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.14</duration>
          <className>org.apache.spark.network.TransportClientFactorySuite</className>
          <testName>neverReturnInactiveClients</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.408</duration>
          <className>org.apache.spark.network.TransportClientFactorySuite</className>
          <testName>closeIdleConnectionForRequestTimeOut</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.097</duration>
          <className>org.apache.spark.network.TransportClientFactorySuite</className>
          <testName>returnDifferentClientsForDifferentServers</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/common/network-common/target/test-reports/org.apache.spark.network.TransportResponseHandlerSuite.xml</file>
      <name>org.apache.spark.network.TransportResponseHandlerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.033</duration>
      <cases>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.network.TransportResponseHandlerSuite</className>
          <testName>testActiveStreams</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.network.TransportResponseHandlerSuite</className>
          <testName>handleSuccessfulFetch</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.network.TransportResponseHandlerSuite</className>
          <testName>handleFailedRPC</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.network.TransportResponseHandlerSuite</className>
          <testName>handleFailedFetch</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.network.TransportResponseHandlerSuite</className>
          <testName>handleSuccessfulRPC</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.network.TransportResponseHandlerSuite</className>
          <testName>clearAllOutstandingRequests</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/common/network-common/target/test-reports/org.apache.spark.network.protocol.MessageWithHeaderSuite.xml</file>
      <name>org.apache.spark.network.protocol.MessageWithHeaderSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.19900002</duration>
      <cases>
        <case>
          <duration>0.194</duration>
          <className>org.apache.spark.network.protocol.MessageWithHeaderSuite</className>
          <testName>testDeallocateReleasesManagedBuffer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.network.protocol.MessageWithHeaderSuite</className>
          <testName>testByteBufBody</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.network.protocol.MessageWithHeaderSuite</className>
          <testName>testShortWrite</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.network.protocol.MessageWithHeaderSuite</className>
          <testName>testSingleWrite</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/common/network-common/target/test-reports/org.apache.spark.network.sasl.SparkSaslSuite.xml</file>
      <name>org.apache.spark.network.sasl.SparkSaslSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.3900001</duration>
      <cases>
        <case>
          <duration>0.432</duration>
          <className>org.apache.spark.network.sasl.SparkSaslSuite</className>
          <testName>testNonMatching</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.074</duration>
          <className>org.apache.spark.network.sasl.SparkSaslSuite</className>
          <testName>testSaslAuthentication</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.network.sasl.SparkSaslSuite</className>
          <testName>testEncryptedMessageChunking</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.267</duration>
          <className>org.apache.spark.network.sasl.SparkSaslSuite</className>
          <testName>testServerAlwaysEncrypt</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.111</duration>
          <className>org.apache.spark.network.sasl.SparkSaslSuite</className>
          <testName>testDataEncryptionIsActuallyEnabled</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.33</duration>
          <className>org.apache.spark.network.sasl.SparkSaslSuite</className>
          <testName>testFileRegionEncryption</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.148</duration>
          <className>org.apache.spark.network.sasl.SparkSaslSuite</className>
          <testName>testSaslEncryption</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.network.sasl.SparkSaslSuite</className>
          <testName>testDelegates</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.network.sasl.SparkSaslSuite</className>
          <testName>testEncryptedMessage</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.network.sasl.SparkSaslSuite</className>
          <testName>testMatching</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.network.sasl.SparkSaslSuite</className>
          <testName>testRpcHandlerDelegate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/common/network-common/target/test-reports/org.apache.spark.network.server.OneForOneStreamManagerSuite.xml</file>
      <name>org.apache.spark.network.server.OneForOneStreamManagerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.021</duration>
      <cases>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.network.server.OneForOneStreamManagerSuite</className>
          <testName>managedBuffersAreFeedWhenConnectionIsClosed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/common/network-common/target/test-reports/org.apache.spark.network.util.TransportFrameDecoderSuite.xml</file>
      <name>org.apache.spark.network.util.TransportFrameDecoderSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.096</duration>
      <cases>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.network.util.TransportFrameDecoderSuite</className>
          <testName>testEmptyFrame</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.network.util.TransportFrameDecoderSuite</className>
          <testName>testNegativeFrameSize</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.network.util.TransportFrameDecoderSuite</className>
          <testName>testSplitLengthField</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.network.util.TransportFrameDecoderSuite</className>
          <testName>testFrameDecoding</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.network.util.TransportFrameDecoderSuite</className>
          <testName>testInterception</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.network.util.TransportFrameDecoderSuite</className>
          <testName>testLargeFrame</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.network.util.TransportFrameDecoderSuite</className>
          <testName>testRetainedFrames</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/common/network-shuffle/target/test-reports/org.apache.spark.network.sasl.SaslIntegrationSuite.xml</file>
      <name>org.apache.spark.network.sasl.SaslIntegrationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.55700004</duration>
      <cases>
        <case>
          <duration>0.398</duration>
          <className>org.apache.spark.network.sasl.SaslIntegrationSuite</className>
          <testName>testGoodClient</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.068</duration>
          <className>org.apache.spark.network.sasl.SaslIntegrationSuite</className>
          <testName>testAppIsolation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.network.sasl.SaslIntegrationSuite</className>
          <testName>testNoSaslClient</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.035</duration>
          <className>org.apache.spark.network.sasl.SaslIntegrationSuite</className>
          <testName>testNoSaslServer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.032</duration>
          <className>org.apache.spark.network.sasl.SaslIntegrationSuite</className>
          <testName>testBadClient</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/common/network-shuffle/target/test-reports/org.apache.spark.network.shuffle.BlockTransferMessagesSuite.xml</file>
      <name>org.apache.spark.network.shuffle.BlockTransferMessagesSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.163</duration>
      <cases>
        <case>
          <duration>0.163</duration>
          <className>org.apache.spark.network.shuffle.BlockTransferMessagesSuite</className>
          <testName>serializeOpenShuffleBlocks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/common/network-shuffle/target/test-reports/org.apache.spark.network.shuffle.ExternalShuffleBlockHandlerSuite.xml</file>
      <name>org.apache.spark.network.shuffle.ExternalShuffleBlockHandlerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.051999997</duration>
      <cases>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.network.shuffle.ExternalShuffleBlockHandlerSuite</className>
          <testName>testOpenShuffleBlocks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.network.shuffle.ExternalShuffleBlockHandlerSuite</className>
          <testName>testRegisterExecutor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.network.shuffle.ExternalShuffleBlockHandlerSuite</className>
          <testName>testBadMessages</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/common/network-shuffle/target/test-reports/org.apache.spark.network.shuffle.ExternalShuffleBlockResolverSuite.xml</file>
      <name>org.apache.spark.network.shuffle.ExternalShuffleBlockResolverSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.125</duration>
      <cases>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.network.shuffle.ExternalShuffleBlockResolverSuite</className>
          <testName>testSortShuffleBlocks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.network.shuffle.ExternalShuffleBlockResolverSuite</className>
          <testName>testBadRequests</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.114</duration>
          <className>org.apache.spark.network.shuffle.ExternalShuffleBlockResolverSuite</className>
          <testName>jsonSerializationOfExecutorRegistration</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/common/network-shuffle/target/test-reports/org.apache.spark.network.shuffle.ExternalShuffleCleanupSuite.xml</file>
      <name>org.apache.spark.network.shuffle.ExternalShuffleCleanupSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.354</duration>
      <cases>
        <case>
          <duration>0.121</duration>
          <className>org.apache.spark.network.shuffle.ExternalShuffleCleanupSuite</className>
          <testName>cleanupOnlyRemovedApp</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.053</duration>
          <className>org.apache.spark.network.shuffle.ExternalShuffleCleanupSuite</className>
          <testName>cleanupUsesExecutor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.062</duration>
          <className>org.apache.spark.network.shuffle.ExternalShuffleCleanupSuite</className>
          <testName>noCleanupAndCleanup</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.118</duration>
          <className>org.apache.spark.network.shuffle.ExternalShuffleCleanupSuite</className>
          <testName>cleanupMultipleExecutors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/common/network-shuffle/target/test-reports/org.apache.spark.network.shuffle.ExternalShuffleIntegrationSuite.xml</file>
      <name>org.apache.spark.network.shuffle.ExternalShuffleIntegrationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.595</duration>
      <cases>
        <case>
          <duration>0.142</duration>
          <className>org.apache.spark.network.shuffle.ExternalShuffleIntegrationSuite</className>
          <testName>testFetchUnregisteredExecutor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.07</duration>
          <className>org.apache.spark.network.shuffle.ExternalShuffleIntegrationSuite</className>
          <testName>testFetchWrongExecutor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.network.shuffle.ExternalShuffleIntegrationSuite</className>
          <testName>testFetchNoServer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.network.shuffle.ExternalShuffleIntegrationSuite</className>
          <testName>testRegisterInvalidExecutor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.23</duration>
          <className>org.apache.spark.network.shuffle.ExternalShuffleIntegrationSuite</className>
          <testName>testFetchThreeSort</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.network.shuffle.ExternalShuffleIntegrationSuite</className>
          <testName>testFetchWrongBlockId</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.network.shuffle.ExternalShuffleIntegrationSuite</className>
          <testName>testFetchNonexistent</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.032</duration>
          <className>org.apache.spark.network.shuffle.ExternalShuffleIntegrationSuite</className>
          <testName>testFetchOneSort</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/common/network-shuffle/target/test-reports/org.apache.spark.network.shuffle.ExternalShuffleSecuritySuite.xml</file>
      <name>org.apache.spark.network.shuffle.ExternalShuffleSecuritySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.15100001</duration>
      <cases>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.network.shuffle.ExternalShuffleSecuritySuite</className>
          <testName>testBadSecret</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.network.shuffle.ExternalShuffleSecuritySuite</className>
          <testName>testBadAppId</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.network.shuffle.ExternalShuffleSecuritySuite</className>
          <testName>testValid</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.061</duration>
          <className>org.apache.spark.network.shuffle.ExternalShuffleSecuritySuite</className>
          <testName>testEncryption</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/common/network-shuffle/target/test-reports/org.apache.spark.network.shuffle.OneForOneBlockFetcherSuite.xml</file>
      <name>org.apache.spark.network.shuffle.OneForOneBlockFetcherSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.014000001</duration>
      <cases>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.network.shuffle.OneForOneBlockFetcherSuite</className>
          <testName>testEmptyBlockFetch</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.network.shuffle.OneForOneBlockFetcherSuite</className>
          <testName>testFailure</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.network.shuffle.OneForOneBlockFetcherSuite</className>
          <testName>testFailureAndSuccess</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.network.shuffle.OneForOneBlockFetcherSuite</className>
          <testName>testFetchThree</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.network.shuffle.OneForOneBlockFetcherSuite</className>
          <testName>testFetchOne</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/common/network-shuffle/target/test-reports/org.apache.spark.network.shuffle.RetryingBlockFetcherSuite.xml</file>
      <name>org.apache.spark.network.shuffle.RetryingBlockFetcherSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.28200004</duration>
      <cases>
        <case>
          <duration>0.224</duration>
          <className>org.apache.spark.network.shuffle.RetryingBlockFetcherSuite</className>
          <testName>testRetryAndUnrecoverable</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.network.shuffle.RetryingBlockFetcherSuite</className>
          <testName>testSingleIOExceptionOnFirst</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.network.shuffle.RetryingBlockFetcherSuite</className>
          <testName>testUnrecoverableFailure</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.network.shuffle.RetryingBlockFetcherSuite</className>
          <testName>testSingleIOExceptionOnSecond</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.network.shuffle.RetryingBlockFetcherSuite</className>
          <testName>testThreeIOExceptions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.network.shuffle.RetryingBlockFetcherSuite</className>
          <testName>testNoFailures</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.network.shuffle.RetryingBlockFetcherSuite</className>
          <testName>testTwoIOExceptions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/common/sketch/target/test-reports/org.apache.spark.util.sketch.BitArraySuite.xml</file>
      <name>org.apache.spark.util.sketch.BitArraySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.016</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.sketch.BitArraySuite</className>
          <testName>error case when create BitArray</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.util.sketch.BitArraySuite</className>
          <testName>bitSize</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.sketch.BitArraySuite</className>
          <testName>set</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.util.sketch.BitArraySuite</className>
          <testName>normal operation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.util.sketch.BitArraySuite</className>
          <testName>merge</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/common/sketch/target/test-reports/org.apache.spark.util.sketch.BloomFilterSuite.xml</file>
      <name>org.apache.spark.util.sketch.BloomFilterSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>5.003</duration>
      <cases>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.util.sketch.BloomFilterSuite</className>
          <testName>accuracy - Byte</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.util.sketch.BloomFilterSuite</className>
          <testName>mergeInPlace - Byte</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.util.sketch.BloomFilterSuite</className>
          <testName>accuracy - Short</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.util.sketch.BloomFilterSuite</className>
          <testName>mergeInPlace - Short</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.056</duration>
          <className>org.apache.spark.util.sketch.BloomFilterSuite</className>
          <testName>accuracy - Int</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.098</duration>
          <className>org.apache.spark.util.sketch.BloomFilterSuite</className>
          <testName>mergeInPlace - Int</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.035</duration>
          <className>org.apache.spark.util.sketch.BloomFilterSuite</className>
          <testName>accuracy - Long</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.046</duration>
          <className>org.apache.spark.util.sketch.BloomFilterSuite</className>
          <testName>mergeInPlace - Long</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.199</duration>
          <className>org.apache.spark.util.sketch.BloomFilterSuite</className>
          <testName>accuracy - String</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.552</duration>
          <className>org.apache.spark.util.sketch.BloomFilterSuite</className>
          <testName>mergeInPlace - String</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.sketch.BloomFilterSuite</className>
          <testName>incompatible merge</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/common/sketch/target/test-reports/org.apache.spark.util.sketch.CountMinSketchSuite.xml</file>
      <name>org.apache.spark.util.sketch.CountMinSketchSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>9.003</duration>
      <cases>
        <case>
          <duration>0.569</duration>
          <className>org.apache.spark.util.sketch.CountMinSketchSuite</className>
          <testName>accuracy - Byte</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.402</duration>
          <className>org.apache.spark.util.sketch.CountMinSketchSuite</className>
          <testName>mergeInPlace - Byte</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.043</duration>
          <className>org.apache.spark.util.sketch.CountMinSketchSuite</className>
          <testName>accuracy - Short</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.428</duration>
          <className>org.apache.spark.util.sketch.CountMinSketchSuite</className>
          <testName>mergeInPlace - Short</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.674</duration>
          <className>org.apache.spark.util.sketch.CountMinSketchSuite</className>
          <testName>accuracy - Int</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.398</duration>
          <className>org.apache.spark.util.sketch.CountMinSketchSuite</className>
          <testName>mergeInPlace - Int</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.796</duration>
          <className>org.apache.spark.util.sketch.CountMinSketchSuite</className>
          <testName>accuracy - Long</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.375</duration>
          <className>org.apache.spark.util.sketch.CountMinSketchSuite</className>
          <testName>mergeInPlace - Long</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.943</duration>
          <className>org.apache.spark.util.sketch.CountMinSketchSuite</className>
          <testName>accuracy - String</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.373</duration>
          <className>org.apache.spark.util.sketch.CountMinSketchSuite</className>
          <testName>mergeInPlace - String</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.util.sketch.CountMinSketchSuite</className>
          <testName>incompatible merge</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/common/unsafe/target/test-reports/org.apache.spark.unsafe.PlatformUtilSuite.xml</file>
      <name>org.apache.spark.unsafe.PlatformUtilSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.043</duration>
      <cases>
        <case>
          <duration>0.041</duration>
          <className>org.apache.spark.unsafe.PlatformUtilSuite</className>
          <testName>overlappingCopyMemory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.unsafe.PlatformUtilSuite</className>
          <testName>memoryDebugFillEnabledInTest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/common/unsafe/target/test-reports/org.apache.spark.unsafe.array.LongArraySuite.xml</file>
      <name>org.apache.spark.unsafe.array.LongArraySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.002</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.unsafe.array.LongArraySuite</className>
          <testName>basicTest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/common/unsafe/target/test-reports/org.apache.spark.unsafe.hash.Murmur3_x86_32Suite.xml</file>
      <name>org.apache.spark.unsafe.hash.Murmur3_x86_32Suite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.297</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.unsafe.hash.Murmur3_x86_32Suite</className>
          <testName>testKnownLongInputs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.unsafe.hash.Murmur3_x86_32Suite</className>
          <testName>testKnownIntegerInputs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.unsafe.hash.Murmur3_x86_32Suite</className>
          <testName>randomizedStressTest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.055</duration>
          <className>org.apache.spark.unsafe.hash.Murmur3_x86_32Suite</className>
          <testName>randomizedStressTestPaddedStrings</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.199</duration>
          <className>org.apache.spark.unsafe.hash.Murmur3_x86_32Suite</className>
          <testName>randomizedStressTestBytes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/common/unsafe/target/test-reports/org.apache.spark.unsafe.types.CalendarIntervalSuite.xml</file>
      <name>org.apache.spark.unsafe.types.CalendarIntervalSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.012000001</duration>
      <cases>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.unsafe.types.CalendarIntervalSuite</className>
          <testName>addTest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.unsafe.types.CalendarIntervalSuite</className>
          <testName>fromStringTest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.unsafe.types.CalendarIntervalSuite</className>
          <testName>equalsTest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.unsafe.types.CalendarIntervalSuite</className>
          <testName>fromYearMonthStringTest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.unsafe.types.CalendarIntervalSuite</className>
          <testName>toStringTest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.unsafe.types.CalendarIntervalSuite</className>
          <testName>subtractTest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.unsafe.types.CalendarIntervalSuite</className>
          <testName>fromSingleUnitStringTest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.unsafe.types.CalendarIntervalSuite</className>
          <testName>fromDayTimeStringTest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/common/unsafe/target/test-reports/org.apache.spark.unsafe.types.UTF8StringPropertyCheckSuite.xml</file>
      <name>org.apache.spark.unsafe.types.UTF8StringPropertyCheckSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.764</duration>
      <cases>
        <case>
          <duration>0.318</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringPropertyCheckSuite</className>
          <testName>toString</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.122</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringPropertyCheckSuite</className>
          <testName>numChars</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.072</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringPropertyCheckSuite</className>
          <testName>startsWith</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.049</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringPropertyCheckSuite</className>
          <testName>endsWith</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringPropertyCheckSuite</className>
          <testName>toUpperCase</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringPropertyCheckSuite</className>
          <testName>toLowerCase</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringPropertyCheckSuite</className>
          <testName>compare</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.23</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringPropertyCheckSuite</className>
          <testName>substring</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.102</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringPropertyCheckSuite</className>
          <testName>contains</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringPropertyCheckSuite</className>
          <testName>trim, trimLeft, trimRight</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringPropertyCheckSuite</className>
          <testName>reverse</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.133</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringPropertyCheckSuite</className>
          <testName>indexOf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringPropertyCheckSuite</className>
          <testName>repeat</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringPropertyCheckSuite</className>
          <testName>lpad, rpad</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.271</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringPropertyCheckSuite</className>
          <testName>concat</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.229</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringPropertyCheckSuite</className>
          <testName>concatWs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringPropertyCheckSuite</className>
          <testName>split</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.04</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringPropertyCheckSuite</className>
          <testName>levenshteinDistance</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringPropertyCheckSuite</className>
          <testName>hashCode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringPropertyCheckSuite</className>
          <testName>equals</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/common/unsafe/target/test-reports/org.apache.spark.unsafe.types.UTF8StringSuite.xml</file>
      <name>org.apache.spark.unsafe.types.UTF8StringSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.032</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringSuite</className>
          <testName>titleCase</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringSuite</className>
          <testName>concatTest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringSuite</className>
          <testName>soundex</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringSuite</className>
          <testName>basicTest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringSuite</className>
          <testName>startsWith</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringSuite</className>
          <testName>compareTo</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringSuite</className>
          <testName>levenshteinDistance</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringSuite</className>
          <testName>upperAndLower</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringSuite</className>
          <testName>createBlankString</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringSuite</className>
          <testName>prefix</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringSuite</className>
          <testName>concatWsTest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringSuite</className>
          <testName>repeat</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringSuite</className>
          <testName>contains</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringSuite</className>
          <testName>emptyStringTest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringSuite</className>
          <testName>substringSQL</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringSuite</className>
          <testName>substring_index</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringSuite</className>
          <testName>pad</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringSuite</className>
          <testName>split</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringSuite</className>
          <testName>trims</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringSuite</className>
          <testName>findInSet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringSuite</className>
          <testName>substring</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringSuite</className>
          <testName>translate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringSuite</className>
          <testName>reverse</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringSuite</className>
          <testName>endsWith</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.unsafe.types.UTF8StringSuite</className>
          <testName>indexOf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.AccumulatorSuite.xml</file>
      <name>org.apache.spark.AccumulatorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.86</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.AccumulatorSuite</className>
          <testName>accumulator serialization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.076</duration>
          <className>org.apache.spark.AccumulatorSuite</className>
          <testName>basic accumulation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.AccumulatorSuite</className>
          <testName>value not assignable from tasks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.179</duration>
          <className>org.apache.spark.AccumulatorSuite</className>
          <testName>add value to collection accumulators</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.141</duration>
          <className>org.apache.spark.AccumulatorSuite</className>
          <testName>value not readable in tasks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.385</duration>
          <className>org.apache.spark.AccumulatorSuite</className>
          <testName>collection accumulators</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.136</duration>
          <className>org.apache.spark.AccumulatorSuite</className>
          <testName>localValue readable in tasks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.666</duration>
          <className>org.apache.spark.AccumulatorSuite</className>
          <testName>garbage collection</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.229</duration>
          <className>org.apache.spark.AccumulatorSuite</className>
          <testName>get accum</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.AccumulatorSuite</className>
          <testName>string accumulator param</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.CheckpointSuite.xml</file>
      <name>org.apache.spark.CheckpointSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>8.400998</duration>
      <cases>
        <case>
          <duration>0.137</duration>
          <className>org.apache.spark.CheckpointSuite</className>
          <testName>basic checkpointing [reliable checkpoint]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.CheckpointSuite</className>
          <testName>basic checkpointing [local checkpoint]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.414</duration>
          <className>org.apache.spark.CheckpointSuite</className>
          <testName>checkpointing partitioners [reliable checkpoint]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.921</duration>
          <className>org.apache.spark.CheckpointSuite</className>
          <testName>RDDs with one-to-one dependencies [reliable checkpoint]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.702</duration>
          <className>org.apache.spark.CheckpointSuite</className>
          <testName>RDDs with one-to-one dependencies [local checkpoint]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.117</duration>
          <className>org.apache.spark.CheckpointSuite</className>
          <testName>ParallelCollectionRDD [reliable checkpoint]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.CheckpointSuite</className>
          <testName>ParallelCollectionRDD [local checkpoint]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.1</duration>
          <className>org.apache.spark.CheckpointSuite</className>
          <testName>BlockRDD [reliable checkpoint]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.CheckpointSuite</className>
          <testName>BlockRDD [local checkpoint]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.179</duration>
          <className>org.apache.spark.CheckpointSuite</className>
          <testName>ShuffleRDD [reliable checkpoint]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.093</duration>
          <className>org.apache.spark.CheckpointSuite</className>
          <testName>ShuffleRDD [local checkpoint]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.404</duration>
          <className>org.apache.spark.CheckpointSuite</className>
          <testName>UnionRDD [reliable checkpoint]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.134</duration>
          <className>org.apache.spark.CheckpointSuite</className>
          <testName>UnionRDD [local checkpoint]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.784</duration>
          <className>org.apache.spark.CheckpointSuite</className>
          <testName>CartesianRDD [reliable checkpoint]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.403</duration>
          <className>org.apache.spark.CheckpointSuite</className>
          <testName>CartesianRDD [local checkpoint]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.392</duration>
          <className>org.apache.spark.CheckpointSuite</className>
          <testName>CoalescedRDD [reliable checkpoint]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.119</duration>
          <className>org.apache.spark.CheckpointSuite</className>
          <testName>CoalescedRDD [local checkpoint]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.431</duration>
          <className>org.apache.spark.CheckpointSuite</className>
          <testName>CoGroupedRDD [reliable checkpoint]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.174</duration>
          <className>org.apache.spark.CheckpointSuite</className>
          <testName>CoGroupedRDD [local checkpoint]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.598</duration>
          <className>org.apache.spark.CheckpointSuite</className>
          <testName>ZippedPartitionsRDD [reliable checkpoint]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.187</duration>
          <className>org.apache.spark.CheckpointSuite</className>
          <testName>ZippedPartitionsRDD [local checkpoint]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.558</duration>
          <className>org.apache.spark.CheckpointSuite</className>
          <testName>PartitionerAwareUnionRDD [reliable checkpoint]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.228</duration>
          <className>org.apache.spark.CheckpointSuite</className>
          <testName>PartitionerAwareUnionRDD [local checkpoint]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.CheckpointSuite</className>
          <testName>CheckpointRDD with zero partitions [reliable checkpoint]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.CheckpointSuite</className>
          <testName>CheckpointRDD with zero partitions [local checkpoint]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.162</duration>
          <className>org.apache.spark.CheckpointSuite</className>
          <testName>checkpointAllMarkedAncestors [reliable checkpoint]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.CheckpointSuite</className>
          <testName>checkpointAllMarkedAncestors [local checkpoint]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.ContextCleanerSuite.xml</file>
      <name>org.apache.spark.ContextCleanerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>17.656</duration>
      <cases>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.ContextCleanerSuite</className>
          <testName>cleanup RDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.069</duration>
          <className>org.apache.spark.ContextCleanerSuite</className>
          <testName>cleanup shuffle</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.ContextCleanerSuite</className>
          <testName>cleanup broadcast</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.693</duration>
          <className>org.apache.spark.ContextCleanerSuite</className>
          <testName>automatically cleanup RDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.546</duration>
          <className>org.apache.spark.ContextCleanerSuite</className>
          <testName>automatically cleanup shuffle</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.44</duration>
          <className>org.apache.spark.ContextCleanerSuite</className>
          <testName>automatically cleanup broadcast</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.645</duration>
          <className>org.apache.spark.ContextCleanerSuite</className>
          <testName>automatically cleanup normal checkpoint</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.375</duration>
          <className>org.apache.spark.ContextCleanerSuite</className>
          <testName>automatically clean up local checkpoint</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.662</duration>
          <className>org.apache.spark.ContextCleanerSuite</className>
          <testName>automatically cleanup RDD + shuffle + broadcast</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>6.174</duration>
          <className>org.apache.spark.ContextCleanerSuite</className>
          <testName>automatically cleanup RDD + shuffle + broadcast in distributed mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.DistributedSuite.xml</file>
      <name>org.apache.spark.DistributedSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>147.32698</duration>
      <cases>
        <case>
          <duration>3.888</duration>
          <className>org.apache.spark.DistributedSuite</className>
          <testName>task throws not serializable exception</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.DistributedSuite</className>
          <testName>local-cluster format</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.483</duration>
          <className>org.apache.spark.DistributedSuite</className>
          <testName>simple groupByKey</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.635</duration>
          <className>org.apache.spark.DistributedSuite</className>
          <testName>groupByKey where map output sizes exceed maxMbInFlight</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.57</duration>
          <className>org.apache.spark.DistributedSuite</className>
          <testName>accumulators</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.239</duration>
          <className>org.apache.spark.DistributedSuite</className>
          <testName>broadcast variables</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.384</duration>
          <className>org.apache.spark.DistributedSuite</className>
          <testName>repeatedly failing task</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>7.621</duration>
          <className>org.apache.spark.DistributedSuite</className>
          <testName>repeatedly failing task that crashes JVM</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>9.566</duration>
          <className>org.apache.spark.DistributedSuite</className>
          <testName>repeatedly failing task that crashes JVM with a zero exit code (SPARK-16925)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.104</duration>
          <className>org.apache.spark.DistributedSuite</className>
          <testName>caching</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.21</duration>
          <className>org.apache.spark.DistributedSuite</className>
          <testName>caching on disk</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.322</duration>
          <className>org.apache.spark.DistributedSuite</className>
          <testName>caching in memory, replicated</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.114</duration>
          <className>org.apache.spark.DistributedSuite</className>
          <testName>caching in memory, serialized, replicated</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.084</duration>
          <className>org.apache.spark.DistributedSuite</className>
          <testName>caching on disk, replicated</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.447</duration>
          <className>org.apache.spark.DistributedSuite</className>
          <testName>caching in memory and disk, replicated</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.01</duration>
          <className>org.apache.spark.DistributedSuite</className>
          <testName>caching in memory and disk, serialized, replicated</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.45</duration>
          <className>org.apache.spark.DistributedSuite</className>
          <testName>compute without caching when no partitions fit in memory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.223</duration>
          <className>org.apache.spark.DistributedSuite</className>
          <testName>compute when only some partitions fit in memory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.522</duration>
          <className>org.apache.spark.DistributedSuite</className>
          <testName>passing environment variables to cluster</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>6.785</duration>
          <className>org.apache.spark.DistributedSuite</className>
          <testName>recover from node failures</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>8.238</duration>
          <className>org.apache.spark.DistributedSuite</className>
          <testName>recover from repeated node failures during shuffle-map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>41.091</duration>
          <className>org.apache.spark.DistributedSuite</className>
          <testName>recover from repeated node failures during shuffle-reduce</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>8.469</duration>
          <className>org.apache.spark.DistributedSuite</className>
          <testName>recover from node failures with replication</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.87</duration>
          <className>org.apache.spark.DistributedSuite</className>
          <testName>unpersist RDDs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.DriverSuite.xml</file>
      <name>org.apache.spark.DriverSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>-0.001</duration>
      <cases>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.DriverSuite</className>
          <testName>driver should exit after finishing without cleanup (SPARK-530)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.ExecutorAllocationManagerSuite.xml</file>
      <name>org.apache.spark.ExecutorAllocationManagerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.5749999</duration>
      <cases>
        <case>
          <duration>0.337</duration>
          <className>org.apache.spark.ExecutorAllocationManagerSuite</className>
          <testName>verify min/max executors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.046</duration>
          <className>org.apache.spark.ExecutorAllocationManagerSuite</className>
          <testName>starting state</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.06</duration>
          <className>org.apache.spark.ExecutorAllocationManagerSuite</className>
          <testName>add executors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.052</duration>
          <className>org.apache.spark.ExecutorAllocationManagerSuite</className>
          <testName>add executors capped by num pending tasks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.049</duration>
          <className>org.apache.spark.ExecutorAllocationManagerSuite</className>
          <testName>cancel pending executors when no longer needed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.093</duration>
          <className>org.apache.spark.ExecutorAllocationManagerSuite</className>
          <testName>remove executors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.055</duration>
          <className>org.apache.spark.ExecutorAllocationManagerSuite</className>
          <testName>remove multiple executors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.057</duration>
          <className>org.apache.spark.ExecutorAllocationManagerSuite</className>
          <testName>interleaving add and remove</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.ExecutorAllocationManagerSuite</className>
          <testName>starting/canceling add timer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.ExecutorAllocationManagerSuite</className>
          <testName>starting/canceling remove timers</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.05</duration>
          <className>org.apache.spark.ExecutorAllocationManagerSuite</className>
          <testName>mock polling loop with no events</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.049</duration>
          <className>org.apache.spark.ExecutorAllocationManagerSuite</className>
          <testName>mock polling loop add behavior</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.057</duration>
          <className>org.apache.spark.ExecutorAllocationManagerSuite</className>
          <testName>mock polling loop remove behavior</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.055</duration>
          <className>org.apache.spark.ExecutorAllocationManagerSuite</className>
          <testName>listeners trigger add executors correctly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.051</duration>
          <className>org.apache.spark.ExecutorAllocationManagerSuite</className>
          <testName>listeners trigger remove executors correctly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.077</duration>
          <className>org.apache.spark.ExecutorAllocationManagerSuite</className>
          <testName>listeners trigger add and remove executor callbacks correctly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.046</duration>
          <className>org.apache.spark.ExecutorAllocationManagerSuite</className>
          <testName>SPARK-4951: call onTaskStart before onBlockManagerAdded</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.ExecutorAllocationManagerSuite</className>
          <testName>SPARK-4951: onExecutorAdded should not add a busy executor to removeTimes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.051</duration>
          <className>org.apache.spark.ExecutorAllocationManagerSuite</className>
          <testName>avoid ramp up when target &lt; running executors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.045</duration>
          <className>org.apache.spark.ExecutorAllocationManagerSuite</className>
          <testName>avoid ramp down initial executors until first job is submitted</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.045</duration>
          <className>org.apache.spark.ExecutorAllocationManagerSuite</className>
          <testName>avoid ramp down initial executors until idle executor is timeout</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.056</duration>
          <className>org.apache.spark.ExecutorAllocationManagerSuite</className>
          <testName>get pending task number and related locality preference</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.049</duration>
          <className>org.apache.spark.ExecutorAllocationManagerSuite</className>
          <testName>SPARK-8366: maxNumExecutorsNeeded should properly handle failed tasks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.052</duration>
          <className>org.apache.spark.ExecutorAllocationManagerSuite</className>
          <testName>reset the state of allocation manager</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.ExternalShuffleServiceSuite.xml</file>
      <name>org.apache.spark.ExternalShuffleServiceSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>61.921</duration>
      <cases>
        <case>
          <duration>0.427</duration>
          <className>org.apache.spark.ExternalShuffleServiceSuite</className>
          <testName>groupByKey without compression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.334</duration>
          <className>org.apache.spark.ExternalShuffleServiceSuite</className>
          <testName>shuffle non-zero block size</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>5.504</duration>
          <className>org.apache.spark.ExternalShuffleServiceSuite</className>
          <testName>shuffle serializer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>9.162</duration>
          <className>org.apache.spark.ExternalShuffleServiceSuite</className>
          <testName>zero sized blocks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>8.372</duration>
          <className>org.apache.spark.ExternalShuffleServiceSuite</className>
          <testName>zero sized blocks without kryo</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.381</duration>
          <className>org.apache.spark.ExternalShuffleServiceSuite</className>
          <testName>shuffle on mutable pairs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.127</duration>
          <className>org.apache.spark.ExternalShuffleServiceSuite</className>
          <testName>sorting on mutable pairs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.387</duration>
          <className>org.apache.spark.ExternalShuffleServiceSuite</className>
          <testName>cogroup using mutable pairs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.556</duration>
          <className>org.apache.spark.ExternalShuffleServiceSuite</className>
          <testName>subtract mutable pairs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.831</duration>
          <className>org.apache.spark.ExternalShuffleServiceSuite</className>
          <testName>sort with Java non serializable class - Kryo</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.547</duration>
          <className>org.apache.spark.ExternalShuffleServiceSuite</className>
          <testName>sort with Java non serializable class - Java</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.614</duration>
          <className>org.apache.spark.ExternalShuffleServiceSuite</className>
          <testName>shuffle with different compression settings (SPARK-3426)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.511</duration>
          <className>org.apache.spark.ExternalShuffleServiceSuite</className>
          <testName>[SPARK-4085] rerun map stage if reduce stage cannot find its local shuffle file</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.935</duration>
          <className>org.apache.spark.ExternalShuffleServiceSuite</className>
          <testName>metrics for shuffle without aggregation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.857</duration>
          <className>org.apache.spark.ExternalShuffleServiceSuite</className>
          <testName>metrics for shuffle with aggregation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.083</duration>
          <className>org.apache.spark.ExternalShuffleServiceSuite</className>
          <testName>multiple simultaneous attempts for one task (SPARK-8029)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>5.293</duration>
          <className>org.apache.spark.ExternalShuffleServiceSuite</className>
          <testName>using external shuffle service</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.FailureSuite.xml</file>
      <name>org.apache.spark.FailureSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.897</duration>
      <cases>
        <case>
          <duration>0.065</duration>
          <className>org.apache.spark.FailureSuite</className>
          <testName>failure in a single-stage job</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.1</duration>
          <className>org.apache.spark.FailureSuite</className>
          <testName>failure in a two-stage job</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.084</duration>
          <className>org.apache.spark.FailureSuite</className>
          <testName>failure in a map stage</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.092</duration>
          <className>org.apache.spark.FailureSuite</className>
          <testName>failure because task results are not serializable</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.065</duration>
          <className>org.apache.spark.FailureSuite</className>
          <testName>failure because task closure is not serializable</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.075</duration>
          <className>org.apache.spark.FailureSuite</className>
          <testName>managed memory leak error should not mask other failures (SPARK-9266</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.075</duration>
          <className>org.apache.spark.FailureSuite</className>
          <testName>last failure cause is sent back to driver</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.063</duration>
          <className>org.apache.spark.FailureSuite</className>
          <testName>failure cause stacktrace is sent back to driver if exception is not serializable</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.061</duration>
          <className>org.apache.spark.FailureSuite</className>
          <testName>failure cause stacktrace is sent back to driver if exception is not deserializable</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.071</duration>
          <className>org.apache.spark.FailureSuite</className>
          <testName>failure in tasks in a submitMapStage</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.086</duration>
          <className>org.apache.spark.FailureSuite</className>
          <testName>failure because cached RDD partitions are missing from DiskStore (SPARK-15736)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.06</duration>
          <className>org.apache.spark.FailureSuite</className>
          <testName>SPARK-16304: Link error should not crash executor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.FileSuite.xml</file>
      <name>org.apache.spark.FileSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>4.8270006</duration>
      <cases>
        <case>
          <duration>0.261</duration>
          <className>org.apache.spark.FileSuite</className>
          <testName>text files</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.413</duration>
          <className>org.apache.spark.FileSuite</className>
          <testName>text files (compressed)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.195</duration>
          <className>org.apache.spark.FileSuite</className>
          <testName>SequenceFiles</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.293</duration>
          <className>org.apache.spark.FileSuite</className>
          <testName>SequenceFile (compressed)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.137</duration>
          <className>org.apache.spark.FileSuite</className>
          <testName>SequenceFile with writable key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.142</duration>
          <className>org.apache.spark.FileSuite</className>
          <testName>SequenceFile with writable value</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.137</duration>
          <className>org.apache.spark.FileSuite</className>
          <testName>SequenceFile with writable key and value</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.263</duration>
          <className>org.apache.spark.FileSuite</className>
          <testName>implicit conversions in reading SequenceFiles</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.151</duration>
          <className>org.apache.spark.FileSuite</className>
          <testName>object files of ints</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.145</duration>
          <className>org.apache.spark.FileSuite</className>
          <testName>object files of complex types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.272</duration>
          <className>org.apache.spark.FileSuite</className>
          <testName>object files of classes from a JAR</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.152</duration>
          <className>org.apache.spark.FileSuite</className>
          <testName>write SequenceFile using new Hadoop API</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.15</duration>
          <className>org.apache.spark.FileSuite</className>
          <testName>read SequenceFile using new Hadoop API</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.115</duration>
          <className>org.apache.spark.FileSuite</className>
          <testName>binary file input as byte array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.137</duration>
          <className>org.apache.spark.FileSuite</className>
          <testName>portabledatastream caching tests</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.145</duration>
          <className>org.apache.spark.FileSuite</className>
          <testName>portabledatastream persist disk storage</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.102</duration>
          <className>org.apache.spark.FileSuite</className>
          <testName>portabledatastream flatmap tests</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.085</duration>
          <className>org.apache.spark.FileSuite</className>
          <testName>fixed record length binary file as byte array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.065</duration>
          <className>org.apache.spark.FileSuite</className>
          <testName>negative binary record length should raise an exception</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.075</duration>
          <className>org.apache.spark.FileSuite</className>
          <testName>file caching</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.FileSuite</className>
          <testName>prevent user from overwriting the empty directory (old Hadoop API)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.101</duration>
          <className>org.apache.spark.FileSuite</className>
          <testName>prevent user from overwriting the non-empty directory (old Hadoop API)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.185</duration>
          <className>org.apache.spark.FileSuite</className>
          <testName>allow user to disable the output directory existence checking (old Hadoop API</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.039</duration>
          <className>org.apache.spark.FileSuite</className>
          <testName>prevent user from overwriting the empty directory (new Hadoop API)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.106</duration>
          <className>org.apache.spark.FileSuite</className>
          <testName>prevent user from overwriting the non-empty directory (new Hadoop API)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.158</duration>
          <className>org.apache.spark.FileSuite</className>
          <testName>allow user to disable the output directory existence checking (new Hadoop API</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.117</duration>
          <className>org.apache.spark.FileSuite</className>
          <testName>save Hadoop Dataset through old Hadoop API</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.099</duration>
          <className>org.apache.spark.FileSuite</className>
          <testName>save Hadoop Dataset through new Hadoop API</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.202</duration>
          <className>org.apache.spark.FileSuite</className>
          <testName>Get input files via old Hadoop API</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.167</duration>
          <className>org.apache.spark.FileSuite</className>
          <testName>Get input files via new Hadoop API</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.181</duration>
          <className>org.apache.spark.FileSuite</className>
          <testName>ignoreCorruptFiles should work both HadoopRDD and NewHadoopRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.FutureActionSuite.xml</file>
      <name>org.apache.spark.FutureActionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.046</duration>
      <cases>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.FutureActionSuite</className>
          <testName>simple async action</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.FutureActionSuite</className>
          <testName>complex async action</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.HeartbeatReceiverSuite.xml</file>
      <name>org.apache.spark.HeartbeatReceiverSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>6.035</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.HeartbeatReceiverSuite</className>
          <testName>task scheduler is set correctly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.HeartbeatReceiverSuite</className>
          <testName>normal heartbeat</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.HeartbeatReceiverSuite</className>
          <testName>reregister if scheduler is not ready yet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.HeartbeatReceiverSuite</className>
          <testName>reregister if heartbeat from unregistered executor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.HeartbeatReceiverSuite</className>
          <testName>reregister if heartbeat from removed executor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.HeartbeatReceiverSuite</className>
          <testName>expire dead hosts</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>6.015</duration>
          <className>org.apache.spark.HeartbeatReceiverSuite</className>
          <testName>expire dead hosts should kill executors with replacement (SPARK-8119)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.ImplicitOrderingSuite.xml</file>
      <name>org.apache.spark.ImplicitOrderingSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.101</duration>
      <cases>
        <case>
          <duration>0.101</duration>
          <className>org.apache.spark.ImplicitOrderingSuite</className>
          <testName>basic inference of Orderings</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.InternalAccumulatorSuite.xml</file>
      <name>org.apache.spark.InternalAccumulatorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.549</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.InternalAccumulatorSuite</className>
          <testName>internal accumulators in TaskContext</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.074</duration>
          <className>org.apache.spark.InternalAccumulatorSuite</className>
          <testName>internal accumulators in a stage</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.486</duration>
          <className>org.apache.spark.InternalAccumulatorSuite</className>
          <testName>internal accumulators in multiple stages</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.906</duration>
          <className>org.apache.spark.InternalAccumulatorSuite</className>
          <testName>internal accumulators in resubmitted stages</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.082</duration>
          <className>org.apache.spark.InternalAccumulatorSuite</className>
          <testName>internal accumulators are registered for cleanups</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.JavaAPISuite.xml</file>
      <name>org.apache.spark.JavaAPISuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>14.287002</duration>
      <cases>
        <case>
          <duration>0.153</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>groupByOnPairRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.133</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>binaryFilesCaching</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.113</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>sparkContextUnion</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.113</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>checkpointAndComputation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.122</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>leftOuterJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.064</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>keyByOnPairRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>getNumPartitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.153</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>wholeTextFiles</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.12</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>binaryFiles</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.065</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>foldReduce</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.193</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>writeWithNewAPIHadoopFile</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.177</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>hadoopFile</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.123</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>lookup</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.057</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>countAsync</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.15</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>textFiles</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.103</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>binaryRecords</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.056</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>toLocalIterator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.072</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>repartitionAndSortWithinPartitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.054</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>reduce</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.086</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>sample</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.132</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>sortBy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.094</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>mapsFromPairsToPairs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.113</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>flatMap</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.138</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>cogroup3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.151</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>cogroup4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.082</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>randomSplit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.08</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>persist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.071</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>foreach</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.169</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>hadoopFileCompressed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.11</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>accumulators</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.172</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>textFilesCompressed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.059</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>testAsyncActionCancellation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.161</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>checkpointAndRestore</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.107</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>sortByKey</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.105</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>aggregateByKey</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.094</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.056</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>max</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.08</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>min</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.069</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>top</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.07</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>zip</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.063</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>fold</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.088</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>glom</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.091</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>take</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.12</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>javaDoubleRDDHistoGram</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.06</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>collectUnderlyingScalaRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.065</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>keyBy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.068</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>mapPartitionsWithIndex</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.142</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>sampleByKey</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.181</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>intersection</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.067</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>aggregate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.061</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>cartesian</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.928</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>countApproxDistinctByKey</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.182</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>readWithNewAPIHadoopFile</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>testRegisterKryoClasses</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.156</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>groupBy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.159</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>sampleByKeyExact</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.089</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>mapPartitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.073</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>takeOrdered</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.099</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>foldByKey</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.191</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>objectFilesOfInts</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.57</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>treeAggregate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.053</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>testGetPersistentRDDs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.111</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>approximateResults</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.668</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>treeReduce</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.102</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>collectAsMapAndSerialize</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.129</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>countApproxDistinct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.26</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>javaDoubleRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.062</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>mapOnPairRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.062</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>testAsyncActionErrorWrapping</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.062</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>naturalMax</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.062</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>naturalMin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.172</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>sequenceFile</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.153</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>collectPartitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.12</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>cogroup</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.132</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>reduceByKey</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.16</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>repartition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.064</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>iterator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.056</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>emptyRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.072</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>zipWithIndex</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.069</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>foreachPartition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.109</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>combineByKey</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.07</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>takeAsync</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.096</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>collectAsMapWithIntArrayValues</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.191</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>objectFilesOfComplexTypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.077</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>zipWithUniqueId</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.069</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>collectAsync</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.07</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>foreachAsync</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.069</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>zipPartitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.086</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>reduceOnJavaDoubleRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.118</duration>
          <className>org.apache.spark.JavaAPISuite</className>
          <testName>isEmpty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.JavaJdbcRDDSuite.xml</file>
      <name>org.apache.spark.JavaJdbcRDDSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.458</duration>
      <cases>
        <case>
          <duration>0.458</duration>
          <className>org.apache.spark.JavaJdbcRDDSuite</className>
          <testName>testJavaJdbcRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.JobCancellationSuite.xml</file>
      <name>org.apache.spark.JobCancellationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>15.5060005</duration>
      <cases>
        <case>
          <duration>4.761</duration>
          <className>org.apache.spark.JobCancellationSuite</className>
          <testName>local mode, FIFO scheduler</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.493</duration>
          <className>org.apache.spark.JobCancellationSuite</className>
          <testName>local mode, fair scheduler</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.856</duration>
          <className>org.apache.spark.JobCancellationSuite</className>
          <testName>cluster mode, FIFO scheduler</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.326</duration>
          <className>org.apache.spark.JobCancellationSuite</className>
          <testName>cluster mode, fair scheduler</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.201</duration>
          <className>org.apache.spark.JobCancellationSuite</className>
          <testName>do not put partially executed partitions into cache</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.134</duration>
          <className>org.apache.spark.JobCancellationSuite</className>
          <testName>job group</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.123</duration>
          <className>org.apache.spark.JobCancellationSuite</className>
          <testName>inherited job group (SPARK-6629)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.158</duration>
          <className>org.apache.spark.JobCancellationSuite</className>
          <testName>job group with interruption</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.454</duration>
          <className>org.apache.spark.JobCancellationSuite</className>
          <testName>two jobs sharing the same stage</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.MapOutputTrackerSuite.xml</file>
      <name>org.apache.spark.MapOutputTrackerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>25.491999</duration>
      <cases>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.MapOutputTrackerSuite</className>
          <testName>master start and stop</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.018</duration>
          <className>org.apache.spark.MapOutputTrackerSuite</className>
          <testName>master register shuffle and fetch</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.MapOutputTrackerSuite</className>
          <testName>master register and unregister shuffle</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.MapOutputTrackerSuite</className>
          <testName>master register shuffle and unregister map output and fetch</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.291</duration>
          <className>org.apache.spark.MapOutputTrackerSuite</className>
          <testName>remote fetch</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.MapOutputTrackerSuite</className>
          <testName>remote fetch below max RPC message size</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.MapOutputTrackerSuite</className>
          <testName>min broadcast size exceeds max RPC message size</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.MapOutputTrackerSuite</className>
          <testName>getLocationsWithLargestOutputs with multiple outputs in same machine</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>25.085</duration>
          <className>org.apache.spark.MapOutputTrackerSuite</className>
          <testName>remote fetch using broadcast</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.PartitioningSuite.xml</file>
      <name>org.apache.spark.PartitioningSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>3.1330001</duration>
      <cases>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.PartitioningSuite</className>
          <testName>HashPartitioner equality</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.09</duration>
          <className>org.apache.spark.PartitioningSuite</className>
          <testName>RangePartitioner equality</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.114</duration>
          <className>org.apache.spark.PartitioningSuite</className>
          <testName>RangePartitioner getPartition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.PartitioningSuite</className>
          <testName>RangePartitioner for keys that are not Comparable (but with Ordering)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.065</duration>
          <className>org.apache.spark.PartitioningSuite</className>
          <testName>sketch</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.PartitioningSuite</className>
          <testName>determineBounds</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.468</duration>
          <className>org.apache.spark.PartitioningSuite</className>
          <testName>RangePartitioner should run only one job if data is roughly balanced</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.103</duration>
          <className>org.apache.spark.PartitioningSuite</className>
          <testName>RangePartitioner should work well on unbalanced data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.PartitioningSuite</className>
          <testName>RangePartitioner should return a single partition for empty RDDs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.PartitioningSuite</className>
          <testName>HashPartitioner not equal to RangePartitioner</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.076</duration>
          <className>org.apache.spark.PartitioningSuite</className>
          <testName>partitioner preservation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.PartitioningSuite</className>
          <testName>partitioning Java arrays should fail</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.134</duration>
          <className>org.apache.spark.PartitioningSuite</className>
          <testName>zero-length partitions should be correctly handled</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.SSLOptionsSuite.xml</file>
      <name>org.apache.spark.SSLOptionsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.138</duration>
      <cases>
        <case>
          <duration>0.125</duration>
          <className>org.apache.spark.SSLOptionsSuite</className>
          <testName>test resolving property file as spark conf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.SSLOptionsSuite</className>
          <testName>test resolving property with defaults specified</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.SSLOptionsSuite</className>
          <testName>test whether defaults can be overridden</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.SecurityManagerSuite.xml</file>
      <name>org.apache.spark.SecurityManagerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.37599993</duration>
      <cases>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.SecurityManagerSuite</className>
          <testName>set security with conf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.SecurityManagerSuite</className>
          <testName>set security with conf for groups</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.SecurityManagerSuite</className>
          <testName>set security with api</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.SecurityManagerSuite</className>
          <testName>set security with api for groups</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.SecurityManagerSuite</className>
          <testName>set security modify acls</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.SecurityManagerSuite</className>
          <testName>set security modify acls for groups</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.105</duration>
          <className>org.apache.spark.SecurityManagerSuite</className>
          <testName>set security admin acls</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.SecurityManagerSuite</className>
          <testName>set security admin acls for groups</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.077</duration>
          <className>org.apache.spark.SecurityManagerSuite</className>
          <testName>set security with * in acls</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.038</duration>
          <className>org.apache.spark.SecurityManagerSuite</className>
          <testName>set security with * in acls for groups</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.04</duration>
          <className>org.apache.spark.SecurityManagerSuite</className>
          <testName>security for groups default behavior</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.SecurityManagerSuite</className>
          <testName>ssl on setup</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.SecurityManagerSuite</className>
          <testName>ssl off setup</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.SecurityManagerSuite</className>
          <testName>missing secret authentication key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.SecurityManagerSuite</className>
          <testName>secret authentication key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.ShuffleNettySuite.xml</file>
      <name>org.apache.spark.ShuffleNettySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>43.547</duration>
      <cases>
        <case>
          <duration>0.114</duration>
          <className>org.apache.spark.ShuffleNettySuite</className>
          <testName>groupByKey without compression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.617</duration>
          <className>org.apache.spark.ShuffleNettySuite</className>
          <testName>shuffle non-zero block size</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.6</duration>
          <className>org.apache.spark.ShuffleNettySuite</className>
          <testName>shuffle serializer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>6.689</duration>
          <className>org.apache.spark.ShuffleNettySuite</className>
          <testName>zero sized blocks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>6.982</duration>
          <className>org.apache.spark.ShuffleNettySuite</className>
          <testName>zero sized blocks without kryo</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.519</duration>
          <className>org.apache.spark.ShuffleNettySuite</className>
          <testName>shuffle on mutable pairs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.381</duration>
          <className>org.apache.spark.ShuffleNettySuite</className>
          <testName>sorting on mutable pairs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.463</duration>
          <className>org.apache.spark.ShuffleNettySuite</className>
          <testName>cogroup using mutable pairs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.499</duration>
          <className>org.apache.spark.ShuffleNettySuite</className>
          <testName>subtract mutable pairs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.023</duration>
          <className>org.apache.spark.ShuffleNettySuite</className>
          <testName>sort with Java non serializable class - Kryo</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.101</duration>
          <className>org.apache.spark.ShuffleNettySuite</className>
          <testName>sort with Java non serializable class - Java</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.358</duration>
          <className>org.apache.spark.ShuffleNettySuite</className>
          <testName>shuffle with different compression settings (SPARK-3426)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.337</duration>
          <className>org.apache.spark.ShuffleNettySuite</className>
          <testName>[SPARK-4085] rerun map stage if reduce stage cannot find its local shuffle file</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.174</duration>
          <className>org.apache.spark.ShuffleNettySuite</className>
          <testName>metrics for shuffle without aggregation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.628</duration>
          <className>org.apache.spark.ShuffleNettySuite</className>
          <testName>metrics for shuffle with aggregation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.062</duration>
          <className>org.apache.spark.ShuffleNettySuite</className>
          <testName>multiple simultaneous attempts for one task (SPARK-8029)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.SortShuffleSuite.xml</file>
      <name>org.apache.spark.SortShuffleSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>44.051006</duration>
      <cases>
        <case>
          <duration>0.102</duration>
          <className>org.apache.spark.SortShuffleSuite</className>
          <testName>groupByKey without compression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.541</duration>
          <className>org.apache.spark.SortShuffleSuite</className>
          <testName>shuffle non-zero block size</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.88</duration>
          <className>org.apache.spark.SortShuffleSuite</className>
          <testName>shuffle serializer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>6.729</duration>
          <className>org.apache.spark.SortShuffleSuite</className>
          <testName>zero sized blocks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>6.655</duration>
          <className>org.apache.spark.SortShuffleSuite</className>
          <testName>zero sized blocks without kryo</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.413</duration>
          <className>org.apache.spark.SortShuffleSuite</className>
          <testName>shuffle on mutable pairs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.548</duration>
          <className>org.apache.spark.SortShuffleSuite</className>
          <testName>sorting on mutable pairs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.837</duration>
          <className>org.apache.spark.SortShuffleSuite</className>
          <testName>cogroup using mutable pairs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.311</duration>
          <className>org.apache.spark.SortShuffleSuite</className>
          <testName>subtract mutable pairs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.806</duration>
          <className>org.apache.spark.SortShuffleSuite</className>
          <testName>sort with Java non serializable class - Kryo</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.427</duration>
          <className>org.apache.spark.SortShuffleSuite</className>
          <testName>sort with Java non serializable class - Java</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.4</duration>
          <className>org.apache.spark.SortShuffleSuite</className>
          <testName>shuffle with different compression settings (SPARK-3426)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.361</duration>
          <className>org.apache.spark.SortShuffleSuite</className>
          <testName>[SPARK-4085] rerun map stage if reduce stage cannot find its local shuffle file</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.217</duration>
          <className>org.apache.spark.SortShuffleSuite</className>
          <testName>metrics for shuffle without aggregation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.575</duration>
          <className>org.apache.spark.SortShuffleSuite</className>
          <testName>metrics for shuffle with aggregation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.054</duration>
          <className>org.apache.spark.SortShuffleSuite</className>
          <testName>multiple simultaneous attempts for one task (SPARK-8029)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.101</duration>
          <className>org.apache.spark.SortShuffleSuite</className>
          <testName>SortShuffleManager properly cleans up files for shuffles that use the serialized path</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.094</duration>
          <className>org.apache.spark.SortShuffleSuite</className>
          <testName>SortShuffleManager properly cleans up files for shuffles that use the deserialized path</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.SparkConfSuite.xml</file>
      <name>org.apache.spark.SparkConfSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.1410002</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.SparkConfSuite</className>
          <testName>Test byteString conversion</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.SparkConfSuite</className>
          <testName>Test timeString conversion</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.SparkConfSuite</className>
          <testName>loading from system properties</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.SparkConfSuite</className>
          <testName>initializing without loading defaults</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.SparkConfSuite</className>
          <testName>named set methods</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.SparkConfSuite</className>
          <testName>basic get and set</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.SparkConfSuite</className>
          <testName>creating SparkContext without master and app name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.SparkConfSuite</className>
          <testName>creating SparkContext without master</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.SparkConfSuite</className>
          <testName>creating SparkContext without app name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.SparkConfSuite</className>
          <testName>creating SparkContext with both master and app name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.051</duration>
          <className>org.apache.spark.SparkConfSuite</className>
          <testName>SparkContext property overriding</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.SparkConfSuite</className>
          <testName>nested property names</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.004</duration>
          <className>org.apache.spark.SparkConfSuite</className>
          <testName>Thread safeness - SPARK-5425</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.SparkConfSuite</className>
          <testName>register kryo classes through registerKryoClasses</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.SparkConfSuite</className>
          <testName>register kryo classes through registerKryoClasses and custom registrator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.SparkConfSuite</className>
          <testName>register kryo classes through conf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.SparkConfSuite</className>
          <testName>deprecated configs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.SparkConfSuite</className>
          <testName>akka deprecated configs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.SparkConfSuite</className>
          <testName>SPARK-13727</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.SparkConfSuite</className>
          <testName>SPARK-17240: SparkConf should be serializable (java)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.SparkConfSuite</className>
          <testName>SPARK-17240: SparkConf should be serializable (kryo)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.SparkContextInfoSuite.xml</file>
      <name>org.apache.spark.SparkContextInfoSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.15900001</duration>
      <cases>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.SparkContextInfoSuite</className>
          <testName>getPersistentRDDs only returns RDDs that are marked as cached</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.033</duration>
          <className>org.apache.spark.SparkContextInfoSuite</className>
          <testName>getPersistentRDDs returns an immutable map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.052</duration>
          <className>org.apache.spark.SparkContextInfoSuite</className>
          <testName>getRDDStorageInfo only reports on RDDs that actually persist data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.SparkContextInfoSuite</className>
          <testName>call sites report correct locations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.SparkContextSchedulerCreationSuite.xml</file>
      <name>org.apache.spark.SparkContextSchedulerCreationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.524</duration>
      <cases>
        <case>
          <duration>0.041</duration>
          <className>org.apache.spark.SparkContextSchedulerCreationSuite</className>
          <testName>bad-master</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.038</duration>
          <className>org.apache.spark.SparkContextSchedulerCreationSuite</className>
          <testName>local</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.04</duration>
          <className>org.apache.spark.SparkContextSchedulerCreationSuite</className>
          <testName>local-*</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.04</duration>
          <className>org.apache.spark.SparkContextSchedulerCreationSuite</className>
          <testName>local-n</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.SparkContextSchedulerCreationSuite</className>
          <testName>local-*-n-failures</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.041</duration>
          <className>org.apache.spark.SparkContextSchedulerCreationSuite</className>
          <testName>local-n-failures</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.SparkContextSchedulerCreationSuite</className>
          <testName>bad-local-n</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.SparkContextSchedulerCreationSuite</className>
          <testName>bad-local-n-failures</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.SparkContextSchedulerCreationSuite</className>
          <testName>local-default-parallelism</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.172</duration>
          <className>org.apache.spark.SparkContextSchedulerCreationSuite</className>
          <testName>local-cluster</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.SparkContextSuite.xml</file>
      <name>org.apache.spark.SparkContextSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>6.294</duration>
      <cases>
        <case>
          <duration>0.09</duration>
          <className>org.apache.spark.SparkContextSuite</className>
          <testName>Only one SparkContext may be active at a time</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.SparkContextSuite</className>
          <testName>Can still construct a new SparkContext after failing to construct a previous one</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.115</duration>
          <className>org.apache.spark.SparkContextSuite</className>
          <testName>Check for multiple SparkContexts can be disabled via undocumented debug option</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.093</duration>
          <className>org.apache.spark.SparkContextSuite</className>
          <testName>Test getOrCreate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.SparkContextSuite</className>
          <testName>BytesWritable implicit conversion is correct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.12</duration>
          <className>org.apache.spark.SparkContextSuite</className>
          <testName>basic case for addFile and listFiles</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.065</duration>
          <className>org.apache.spark.SparkContextSuite</className>
          <testName>add and list jar files</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.SparkContextSuite</className>
          <testName>SPARK-17650: malformed url&apos;s throw exceptions before bricking Executors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.112</duration>
          <className>org.apache.spark.SparkContextSuite</className>
          <testName>addFile recursive works</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.053</duration>
          <className>org.apache.spark.SparkContextSuite</className>
          <testName>addFile recursive can&apos;t add directories by default</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.243</duration>
          <className>org.apache.spark.SparkContextSuite</className>
          <testName>cannot call addFile with different paths that have the same filename</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.SparkContextSuite</className>
          <testName>addJar can be called twice with same file in local-mode (SPARK-16787)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.041</duration>
          <className>org.apache.spark.SparkContextSuite</className>
          <testName>addFile can be called twice with same file in local-mode (SPARK-16787)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.118</duration>
          <className>org.apache.spark.SparkContextSuite</className>
          <testName>addJar can be called twice with same file in non-local-mode (SPARK-16787)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.136</duration>
          <className>org.apache.spark.SparkContextSuite</className>
          <testName>addFile can be called twice with same file in non-local-mode (SPARK-16787)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.067</duration>
          <className>org.apache.spark.SparkContextSuite</className>
          <testName>Cancelling job group should not cause SparkContext to shutdown (SPARK-6414)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.555</duration>
          <className>org.apache.spark.SparkContextSuite</className>
          <testName>Comma separated paths for newAPIHadoopFile/wholeTextFiles/binaryFiles (SPARK-7155)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.124</duration>
          <className>org.apache.spark.SparkContextSuite</className>
          <testName>Default path for file based RDDs is properly set (SPARK-12517)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.058</duration>
          <className>org.apache.spark.SparkContextSuite</className>
          <testName>stop() must not throw any exception</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.038</duration>
          <className>org.apache.spark.SparkContextSuite</className>
          <testName>No exception when both num-executors and dynamic allocation set</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.SparkContextSuite</className>
          <testName>localProperties are inherited by spawned threads</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.055</duration>
          <className>org.apache.spark.SparkContextSuite</className>
          <testName>localProperties do not cross-talk between threads</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.039</duration>
          <className>org.apache.spark.SparkContextSuite</className>
          <testName>log level case-insensitive and reset log level</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.StatusTrackerSuite.xml</file>
      <name>org.apache.spark.StatusTrackerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.308</duration>
      <cases>
        <case>
          <duration>0.104</duration>
          <className>org.apache.spark.StatusTrackerSuite</className>
          <testName>basic status API usage</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.07</duration>
          <className>org.apache.spark.StatusTrackerSuite</className>
          <testName>getJobIdsForGroup()</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.063</duration>
          <className>org.apache.spark.StatusTrackerSuite</className>
          <testName>getJobIdsForGroup() with takeAsync()</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.071</duration>
          <className>org.apache.spark.StatusTrackerSuite</className>
          <testName>getJobIdsForGroup() with takeAsync() across multiple partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.ThreadingSuite.xml</file>
      <name>org.apache.spark.ThreadingSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.2409999</duration>
      <cases>
        <case>
          <duration>0.118</duration>
          <className>org.apache.spark.ThreadingSuite</className>
          <testName>accessing SparkContext form a different thread</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.465</duration>
          <className>org.apache.spark.ThreadingSuite</className>
          <testName>accessing SparkContext form multiple threads</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.288</duration>
          <className>org.apache.spark.ThreadingSuite</className>
          <testName>accessing multi-threaded SparkContext form multiple threads</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.169</duration>
          <className>org.apache.spark.ThreadingSuite</className>
          <testName>parallel job execution</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.05</duration>
          <className>org.apache.spark.ThreadingSuite</className>
          <testName>set local properties in different thread</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.071</duration>
          <className>org.apache.spark.ThreadingSuite</className>
          <testName>set and get local properties in parent-children thread</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.08</duration>
          <className>org.apache.spark.ThreadingSuite</className>
          <testName>mutation in parent local property does not affect child (SPARK-10563)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.UnpersistSuite.xml</file>
      <name>org.apache.spark.UnpersistSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.08</duration>
      <cases>
        <case>
          <duration>0.08</duration>
          <className>org.apache.spark.UnpersistSuite</className>
          <testName>unpersist RDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.api.java.OptionalSuite.xml</file>
      <name>org.apache.spark.api.java.OptionalSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.003</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.api.java.OptionalSuite</className>
          <testName>testAbsentGet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.api.java.OptionalSuite</className>
          <testName>testOfWithNull</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.api.java.OptionalSuite</className>
          <testName>testEmpty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.api.java.OptionalSuite</className>
          <testName>testOf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.api.java.OptionalSuite</className>
          <testName>testFromNullable</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.api.java.OptionalSuite</className>
          <testName>testAbsent</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.api.java.OptionalSuite</className>
          <testName>testOfNullable</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.api.java.OptionalSuite</className>
          <testName>testEmptyGet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.api.python.PythonBroadcastSuite.xml</file>
      <name>org.apache.spark.api.python.PythonBroadcastSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.008</duration>
      <cases>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.api.python.PythonBroadcastSuite</className>
          <testName>PythonBroadcast can be serialized with Kryo (SPARK-4882)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.api.python.PythonRDDSuite.xml</file>
      <name>org.apache.spark.api.python.PythonRDDSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.007</duration>
      <cases>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.api.python.PythonRDDSuite</className>
          <testName>Writing large strings to the worker</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.api.python.PythonRDDSuite</className>
          <testName>Handle nulls gracefully</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.api.python.SerDeUtilSuite.xml</file>
      <name>org.apache.spark.api.python.SerDeUtilSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.062</duration>
      <cases>
        <case>
          <duration>0.035</duration>
          <className>org.apache.spark.api.python.SerDeUtilSuite</className>
          <testName>Converting an empty pair RDD to python does not throw an exception (SPARK-5441)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.api.python.SerDeUtilSuite</className>
          <testName>Converting an empty python RDD to pair RDD does not throw an exception (SPARK-5441)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.broadcast.BroadcastSuite.xml</file>
      <name>org.apache.spark.broadcast.BroadcastSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>14.688999</duration>
      <cases>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.broadcast.BroadcastSuite</className>
          <testName>Using TorrentBroadcast locally</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.073</duration>
          <className>org.apache.spark.broadcast.BroadcastSuite</className>
          <testName>Accessing TorrentBroadcast variables from multiple threads</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.069</duration>
          <className>org.apache.spark.broadcast.BroadcastSuite</className>
          <testName>Accessing TorrentBroadcast variables in a local cluster</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.broadcast.BroadcastSuite</className>
          <testName>TorrentBroadcast&apos;s blockifyObject and unblockifyObject are inverses</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.172</duration>
          <className>org.apache.spark.broadcast.BroadcastSuite</className>
          <testName>Test Lazy Broadcast variables with TorrentBroadcast</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.101</duration>
          <className>org.apache.spark.broadcast.BroadcastSuite</className>
          <testName>Unpersisting TorrentBroadcast on executors only in local mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.097</duration>
          <className>org.apache.spark.broadcast.BroadcastSuite</className>
          <testName>Unpersisting TorrentBroadcast on executors and driver in local mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.594</duration>
          <className>org.apache.spark.broadcast.BroadcastSuite</className>
          <testName>Unpersisting TorrentBroadcast on executors only in distributed mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.371</duration>
          <className>org.apache.spark.broadcast.BroadcastSuite</className>
          <testName>Unpersisting TorrentBroadcast on executors and driver in distributed mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.039</duration>
          <className>org.apache.spark.broadcast.BroadcastSuite</className>
          <testName>Using broadcast after destroy prints callsite</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.broadcast.BroadcastSuite</className>
          <testName>Broadcast variables cannot be created after SparkContext is stopped (SPARK-5065)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.broadcast.BroadcastSuite</className>
          <testName>Forbid broadcasting RDD directly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.deploy.ClientSuite.xml</file>
      <name>org.apache.spark.deploy.ClientSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.001</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.deploy.ClientSuite</className>
          <testName>correctly validates driver jar URL&apos;s</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.deploy.JsonProtocolSuite.xml</file>
      <name>org.apache.spark.deploy.JsonProtocolSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.035</duration>
      <cases>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.deploy.JsonProtocolSuite</className>
          <testName>writeApplicationInfo</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.deploy.JsonProtocolSuite</className>
          <testName>writeWorkerInfo</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.deploy.JsonProtocolSuite</className>
          <testName>writeApplicationDescription</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.deploy.JsonProtocolSuite</className>
          <testName>writeExecutorRunner</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.deploy.JsonProtocolSuite</className>
          <testName>writeDriverInfo</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.deploy.JsonProtocolSuite</className>
          <testName>writeMasterState</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.deploy.JsonProtocolSuite</className>
          <testName>writeWorkerState</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.deploy.LogUrlsStandaloneSuite.xml</file>
      <name>org.apache.spark.deploy.LogUrlsStandaloneSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>6.798</duration>
      <cases>
        <case>
          <duration>3.182</duration>
          <className>org.apache.spark.deploy.LogUrlsStandaloneSuite</className>
          <testName>verify that correct log urls get propagated from workers</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.616</duration>
          <className>org.apache.spark.deploy.LogUrlsStandaloneSuite</className>
          <testName>verify that log urls reflect SPARK_PUBLIC_DNS (SPARK-6175)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.deploy.PythonRunnerSuite.xml</file>
      <name>org.apache.spark.deploy.PythonRunnerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.008</duration>
      <cases>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.deploy.PythonRunnerSuite</className>
          <testName>format path</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.deploy.PythonRunnerSuite</className>
          <testName>format paths</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.deploy.RPackageUtilsSuite.xml</file>
      <name>org.apache.spark.deploy.RPackageUtilsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.2770002</duration>
      <cases>
        <case>
          <duration>0.334</duration>
          <className>org.apache.spark.deploy.RPackageUtilsSuite</className>
          <testName>pick which jars to unpack using the manifest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.307</duration>
          <className>org.apache.spark.deploy.RPackageUtilsSuite</className>
          <testName>build an R package from a jar end to end</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.248</duration>
          <className>org.apache.spark.deploy.RPackageUtilsSuite</className>
          <testName>jars that don&apos;t exist are skipped and print warning</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.383</duration>
          <className>org.apache.spark.deploy.RPackageUtilsSuite</className>
          <testName>faulty R package shows documentation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.deploy.RPackageUtilsSuite</className>
          <testName>SparkR zipping works properly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.deploy.SparkSubmitSuite.xml</file>
      <name>org.apache.spark.deploy.SparkSubmitSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>24.986998</duration>
      <cases>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.deploy.SparkSubmitSuite</className>
          <testName>prints usage on empty input</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.deploy.SparkSubmitSuite</className>
          <testName>prints usage with only --help</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.deploy.SparkSubmitSuite</className>
          <testName>prints error with unrecognized options</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.deploy.SparkSubmitSuite</className>
          <testName>handle binary specified but not class</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.deploy.SparkSubmitSuite</className>
          <testName>handles arguments with --key=val</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.deploy.SparkSubmitSuite</className>
          <testName>handles arguments to user program</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.deploy.SparkSubmitSuite</className>
          <testName>handles arguments to user program with name collision</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.deploy.SparkSubmitSuite</className>
          <testName>specify deploy mode through configuration</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.deploy.SparkSubmitSuite</className>
          <testName>handles YARN cluster mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.deploy.SparkSubmitSuite</className>
          <testName>handles YARN client mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.deploy.SparkSubmitSuite</className>
          <testName>handles standalone cluster mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.deploy.SparkSubmitSuite</className>
          <testName>handles legacy standalone cluster mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.deploy.SparkSubmitSuite</className>
          <testName>handles standalone client mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.deploy.SparkSubmitSuite</className>
          <testName>handles mesos client mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.deploy.SparkSubmitSuite</className>
          <testName>handles confs with flag equivalents</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>5.245</duration>
          <className>org.apache.spark.deploy.SparkSubmitSuite</className>
          <testName>launch simple application with spark-submit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>8.884</duration>
          <className>org.apache.spark.deploy.SparkSubmitSuite</className>
          <testName>includes jars passed in through --jars</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>9.624</duration>
          <className>org.apache.spark.deploy.SparkSubmitSuite</className>
          <testName>includes jars passed in through --packages</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.deploy.SparkSubmitSuite</className>
          <testName>correctly builds R packages included in a jar with --packages</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.deploy.SparkSubmitSuite</className>
          <testName>include an external JAR in SparkR</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.deploy.SparkSubmitSuite</className>
          <testName>resolves command line argument paths correctly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.deploy.SparkSubmitSuite</className>
          <testName>resolves config paths correctly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.175</duration>
          <className>org.apache.spark.deploy.SparkSubmitSuite</className>
          <testName>user classpath first in driver</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.deploy.SparkSubmitSuite</className>
          <testName>conf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.deploy.SparkSubmitSuite</className>
          <testName>comma separated list of files are unioned correctly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.deploy.SparkSubmitUtilsSuite.xml</file>
      <name>org.apache.spark.deploy.SparkSubmitUtilsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.624</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.deploy.SparkSubmitUtilsSuite</className>
          <testName>incorrect maven coordinate throws error</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.deploy.SparkSubmitUtilsSuite</className>
          <testName>create repo resolvers</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.077</duration>
          <className>org.apache.spark.deploy.SparkSubmitUtilsSuite</className>
          <testName>add dependencies works correctly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.deploy.SparkSubmitUtilsSuite</className>
          <testName>excludes works correctly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.24</duration>
          <className>org.apache.spark.deploy.SparkSubmitUtilsSuite</className>
          <testName>ivy path works correctly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.824</duration>
          <className>org.apache.spark.deploy.SparkSubmitUtilsSuite</className>
          <testName>search for artifact at local repositories</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.deploy.SparkSubmitUtilsSuite</className>
          <testName>dependency not found throws RuntimeException</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.251</duration>
          <className>org.apache.spark.deploy.SparkSubmitUtilsSuite</className>
          <testName>neglects Spark and Spark&apos;s dependencies</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.183</duration>
          <className>org.apache.spark.deploy.SparkSubmitUtilsSuite</className>
          <testName>exclude dependencies end to end</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.deploy.StandaloneDynamicAllocationSuite.xml</file>
      <name>org.apache.spark.deploy.StandaloneDynamicAllocationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>6.349</duration>
      <cases>
        <case>
          <duration>0.179</duration>
          <className>org.apache.spark.deploy.StandaloneDynamicAllocationSuite</className>
          <testName>dynamic allocation default behavior</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.216</duration>
          <className>org.apache.spark.deploy.StandaloneDynamicAllocationSuite</className>
          <testName>dynamic allocation with max cores &lt;= cores per worker</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.234</duration>
          <className>org.apache.spark.deploy.StandaloneDynamicAllocationSuite</className>
          <testName>dynamic allocation with max cores &gt; cores per worker</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.375</duration>
          <className>org.apache.spark.deploy.StandaloneDynamicAllocationSuite</className>
          <testName>dynamic allocation with cores per executor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.245</duration>
          <className>org.apache.spark.deploy.StandaloneDynamicAllocationSuite</className>
          <testName>dynamic allocation with cores per executor AND max cores</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.359</duration>
          <className>org.apache.spark.deploy.StandaloneDynamicAllocationSuite</className>
          <testName>kill the same executor twice (SPARK-9795)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.988</duration>
          <className>org.apache.spark.deploy.StandaloneDynamicAllocationSuite</className>
          <testName>the pending replacement executors should not be lost (SPARK-10515)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.631</duration>
          <className>org.apache.spark.deploy.StandaloneDynamicAllocationSuite</className>
          <testName>disable force kill for busy executors (SPARK-9552)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.122</duration>
          <className>org.apache.spark.deploy.StandaloneDynamicAllocationSuite</className>
          <testName>initial executor limit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.deploy.client.AppClientSuite.xml</file>
      <name>org.apache.spark.deploy.client.AppClientSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.065</duration>
      <cases>
        <case>
          <duration>0.052</duration>
          <className>org.apache.spark.deploy.client.AppClientSuite</className>
          <testName>interface methods of AppClient using local Master</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.deploy.client.AppClientSuite</className>
          <testName>request from AppClient before initialized with master</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.deploy.history.ApplicationCacheSuite.xml</file>
      <name>org.apache.spark.deploy.history.ApplicationCacheSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.091000006</duration>
      <cases>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.deploy.history.ApplicationCacheSuite</className>
          <testName>Completed UI get</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.deploy.history.ApplicationCacheSuite</className>
          <testName>Test that if an attempt ID is is set, it must be used in lookups</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.deploy.history.ApplicationCacheSuite</className>
          <testName>Incomplete apps refreshed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.052</duration>
          <className>org.apache.spark.deploy.history.ApplicationCacheSuite</className>
          <testName>Large Scale Application Eviction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.deploy.history.ApplicationCacheSuite</className>
          <testName>Attempts are Evicted</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.deploy.history.ApplicationCacheSuite</className>
          <testName>Instantiate Filter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.deploy.history.ApplicationCacheSuite</className>
          <testName>redirect includes query params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.deploy.history.FsHistoryProviderSuite.xml</file>
      <name>org.apache.spark.deploy.history.FsHistoryProviderSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.529</duration>
      <cases>
        <case>
          <duration>0.05</duration>
          <className>org.apache.spark.deploy.history.FsHistoryProviderSuite</className>
          <testName>Parse application logs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.deploy.history.FsHistoryProviderSuite</className>
          <testName>SPARK-3697: ignore directories that cannot be read</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.039</duration>
          <className>org.apache.spark.deploy.history.FsHistoryProviderSuite</className>
          <testName>history file is renamed from inprogress to completed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.deploy.history.FsHistoryProviderSuite</className>
          <testName>Parse logs that application is not started</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.deploy.history.FsHistoryProviderSuite</className>
          <testName>SPARK-5582: empty log directory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.083</duration>
          <className>org.apache.spark.deploy.history.FsHistoryProviderSuite</className>
          <testName>apps with multiple attempts with order</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.073</duration>
          <className>org.apache.spark.deploy.history.FsHistoryProviderSuite</className>
          <testName>log cleaner</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.deploy.history.FsHistoryProviderSuite</className>
          <testName>Event log copy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.deploy.history.FsHistoryProviderSuite</className>
          <testName>SPARK-8372: new logs with no app ID are ignored</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.069</duration>
          <className>org.apache.spark.deploy.history.FsHistoryProviderSuite</className>
          <testName>provider correctly checks whether fs is in safe mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.deploy.history.FsHistoryProviderSuite</className>
          <testName>provider waits for safe mode to finish before initializing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.deploy.history.FsHistoryProviderSuite</className>
          <testName>provider reports error after FS leaves safe mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.deploy.history.FsHistoryProviderSuite</className>
          <testName>ignore hidden files</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.deploy.history.HistoryServerArgumentsSuite.xml</file>
      <name>org.apache.spark.deploy.history.HistoryServerArgumentsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.006</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.deploy.history.HistoryServerArgumentsSuite</className>
          <testName>No Arguments Parsing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.deploy.history.HistoryServerArgumentsSuite</className>
          <testName>Directory Arguments Parsing --dir or -d</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.deploy.history.HistoryServerArgumentsSuite</className>
          <testName>Directory Param can also be set directly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.deploy.history.HistoryServerArgumentsSuite</className>
          <testName>Properties File Arguments Parsing --properties-file</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.deploy.history.HistoryServerSuite.xml</file>
      <name>org.apache.spark.deploy.history.HistoryServerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>32.079</duration>
      <cases>
        <case>
          <duration>1.709</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>application list json</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.53</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>completed app list json</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.433</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>running app list json</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.444</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>minDate app list json</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.42</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>maxDate app list json</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.416</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>maxDate2 app list json</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.434</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>limit app list json</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.429</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>one app json</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.454</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>one app multi-attempt json</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.514</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>job list json</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.461</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>job list from multi-attempt app json(1)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.427</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>job list from multi-attempt app json(2)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.419</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>one job json</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.508</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>succeeded job list json</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.422</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>succeeded&amp;failed job list json</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.455</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>executor list json</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.44</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>stage list json</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.408</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>complete stage list json</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.434</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>failed stage list json</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.547</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>one stage json</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.461</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>one stage attempt json</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.524</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>stage task summary w shuffle write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.482</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>stage task summary w shuffle read</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.466</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>stage task summary w/ custom quantiles</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.541</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>stage task list</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.492</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>stage task list w/ offset &amp; length</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.482</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>stage task list w/ sortBy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.472</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>stage task list w/ sortBy short names: -runtime</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.444</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>stage task list w/ sortBy short names: runtime</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.385</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>stage list with accumulable json</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.415</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>stage with accumulable json</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.396</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>stage task list from multi-attempt app json(1)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.39</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>stage task list from multi-attempt app json(2)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.376</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>rdd list storage json</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.386</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>download all logs for app with multiple attempts</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.387</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>download one log for app with multiple attempts</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.516</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>response codes on bad paths</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>proxyBase)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>9.86</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>proxyBase)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.271</duration>
          <className>org.apache.spark.deploy.history.HistoryServerSuite</className>
          <testName>incomplete apps get refreshed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.deploy.master.MasterSuite.xml</file>
      <name>org.apache.spark.deploy.master.MasterSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.93100005</duration>
      <cases>
        <case>
          <duration>0.054</duration>
          <className>org.apache.spark.deploy.master.MasterSuite</className>
          <testName>can use a custom recovery mode factory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.133</duration>
          <className>org.apache.spark.deploy.master.MasterSuite</className>
          <testName>master/worker web ui available</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.209</duration>
          <className>org.apache.spark.deploy.master.MasterSuite</className>
          <testName>master/worker web ui available with reverseProxy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.deploy.master.MasterSuite</className>
          <testName>basic scheduling - spread out</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.deploy.master.MasterSuite</className>
          <testName>basic scheduling - no spread out</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.deploy.master.MasterSuite</className>
          <testName>basic scheduling with more memory - spread out</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.046</duration>
          <className>org.apache.spark.deploy.master.MasterSuite</className>
          <testName>basic scheduling with more memory - no spread out</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.deploy.master.MasterSuite</className>
          <testName>scheduling with max cores - spread out</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.deploy.master.MasterSuite</className>
          <testName>scheduling with max cores - no spread out</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.deploy.master.MasterSuite</className>
          <testName>scheduling with cores per executor - spread out</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.deploy.master.MasterSuite</className>
          <testName>scheduling with cores per executor - no spread out</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.deploy.master.MasterSuite</className>
          <testName>scheduling with cores per executor AND max cores - spread out</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.deploy.master.MasterSuite</className>
          <testName>scheduling with cores per executor AND max cores - no spread out</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.deploy.master.MasterSuite</className>
          <testName>scheduling with executor limit - spread out</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.deploy.master.MasterSuite</className>
          <testName>scheduling with executor limit - no spread out</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.deploy.master.MasterSuite</className>
          <testName>scheduling with executor limit AND max cores - spread out</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.049</duration>
          <className>org.apache.spark.deploy.master.MasterSuite</className>
          <testName>scheduling with executor limit AND max cores - no spread out</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.deploy.master.MasterSuite</className>
          <testName>scheduling with executor limit AND cores per executor - spread out</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.deploy.master.MasterSuite</className>
          <testName>scheduling with executor limit AND cores per executor - no spread out</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.deploy.master.MasterSuite</className>
          <testName>scheduling with executor limit AND cores per executor AND max cores - spread out</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.deploy.master.MasterSuite</className>
          <testName>scheduling with executor limit AND cores per executor AND max cores - no spread out</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.045</duration>
          <className>org.apache.spark.deploy.master.MasterSuite</className>
          <testName>SPARK-13604: Master should ask Worker kill unknown executors and drivers</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.deploy.master.PersistenceEngineSuite.xml</file>
      <name>org.apache.spark.deploy.master.PersistenceEngineSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.6009998</duration>
      <cases>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.deploy.master.PersistenceEngineSuite</className>
          <testName>FileSystemPersistenceEngine</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.574</duration>
          <className>org.apache.spark.deploy.master.PersistenceEngineSuite</className>
          <testName>ZooKeeperPersistenceEngine</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.deploy.master.ui.MasterWebUISuite.xml</file>
      <name>org.apache.spark.deploy.master.ui.MasterWebUISuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.22</duration>
      <cases>
        <case>
          <duration>0.113</duration>
          <className>org.apache.spark.deploy.master.ui.MasterWebUISuite</className>
          <testName>kill application</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.107</duration>
          <className>org.apache.spark.deploy.master.ui.MasterWebUISuite</className>
          <testName>kill driver</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.deploy.rest.StandaloneRestSubmitSuite.xml</file>
      <name>org.apache.spark.deploy.rest.StandaloneRestSubmitSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.40800005</duration>
      <cases>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.deploy.rest.StandaloneRestSubmitSuite</className>
          <testName>construct submit request</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.119</duration>
          <className>org.apache.spark.deploy.rest.StandaloneRestSubmitSuite</className>
          <testName>create submission</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.018</duration>
          <className>org.apache.spark.deploy.rest.StandaloneRestSubmitSuite</className>
          <testName>create submission from main method</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.deploy.rest.StandaloneRestSubmitSuite</className>
          <testName>kill submission</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.deploy.rest.StandaloneRestSubmitSuite</className>
          <testName>request submission status</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.deploy.rest.StandaloneRestSubmitSuite</className>
          <testName>create then kill</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.deploy.rest.StandaloneRestSubmitSuite</className>
          <testName>create then request status</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.deploy.rest.StandaloneRestSubmitSuite</className>
          <testName>create then kill then request status</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.018</duration>
          <className>org.apache.spark.deploy.rest.StandaloneRestSubmitSuite</className>
          <testName>kill or request status before create</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.deploy.rest.StandaloneRestSubmitSuite</className>
          <testName>good request paths</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.deploy.rest.StandaloneRestSubmitSuite</className>
          <testName>good request paths, bad requests</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.deploy.rest.StandaloneRestSubmitSuite</className>
          <testName>bad request paths</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.deploy.rest.StandaloneRestSubmitSuite</className>
          <testName>server returns unknown fields</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.deploy.rest.StandaloneRestSubmitSuite</className>
          <testName>client handles faulty server</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.deploy.rest.StandaloneRestSubmitSuite</className>
          <testName>client does not send &apos;SPARK_ENV_LOADED&apos; env var by default</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.deploy.rest.StandaloneRestSubmitSuite</className>
          <testName>client includes mesos env vars</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.deploy.rest.SubmitRestProtocolSuite.xml</file>
      <name>org.apache.spark.deploy.rest.SubmitRestProtocolSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.021000002</duration>
      <cases>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.deploy.rest.SubmitRestProtocolSuite</className>
          <testName>validate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.deploy.rest.SubmitRestProtocolSuite</className>
          <testName>request to and from JSON</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.deploy.rest.SubmitRestProtocolSuite</className>
          <testName>response to and from JSON</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.deploy.rest.SubmitRestProtocolSuite</className>
          <testName>CreateSubmissionRequest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.deploy.rest.SubmitRestProtocolSuite</className>
          <testName>CreateSubmissionResponse</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.deploy.rest.SubmitRestProtocolSuite</className>
          <testName>KillSubmissionResponse</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.deploy.rest.SubmitRestProtocolSuite</className>
          <testName>SubmissionStatusResponse</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.deploy.rest.SubmitRestProtocolSuite</className>
          <testName>ErrorResponse</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.deploy.worker.CommandUtilsSuite.xml</file>
      <name>org.apache.spark.deploy.worker.CommandUtilsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.011</duration>
      <cases>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.deploy.worker.CommandUtilsSuite</className>
          <testName>set libraryPath correctly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.deploy.worker.CommandUtilsSuite</className>
          <testName>auth secret shouldn&apos;t appear in java opts</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.deploy.worker.DriverRunnerTest.xml</file>
      <name>org.apache.spark.deploy.worker.DriverRunnerTest</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.085999995</duration>
      <cases>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.deploy.worker.DriverRunnerTest</className>
          <testName>Process succeeds instantly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.deploy.worker.DriverRunnerTest</className>
          <testName>Process failing several times and then succeeding</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.deploy.worker.DriverRunnerTest</className>
          <testName>Process doesn&apos;t restart if not supervised</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.deploy.worker.DriverRunnerTest</className>
          <testName>Process doesn&apos;t restart if killed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.deploy.worker.DriverRunnerTest</className>
          <testName>Reset of backoff counter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.deploy.worker.DriverRunnerTest</className>
          <testName>Kill process finalized with state KILLED</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.deploy.worker.DriverRunnerTest</className>
          <testName>Finalized with state FINISHED</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.deploy.worker.DriverRunnerTest</className>
          <testName>Finalized with state FAILED</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.deploy.worker.DriverRunnerTest</className>
          <testName>Handle exception starting process</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.deploy.worker.ExecutorRunnerTest.xml</file>
      <name>org.apache.spark.deploy.worker.ExecutorRunnerTest</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.003</duration>
      <cases>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.deploy.worker.ExecutorRunnerTest</className>
          <testName>command includes appId</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.deploy.worker.WorkerArgumentsTest.xml</file>
      <name>org.apache.spark.deploy.worker.WorkerArgumentsTest</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.012</duration>
      <cases>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.deploy.worker.WorkerArgumentsTest</className>
          <testName>Memory can&apos;t be set to 0 when cmd line args leave off M or G</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.deploy.worker.WorkerArgumentsTest</className>
          <testName>Memory can&apos;t be set to 0 when SPARK_WORKER_MEMORY env property leaves off M or G</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.deploy.worker.WorkerArgumentsTest</className>
          <testName>Memory correctly set when SPARK_WORKER_MEMORY env property appends G</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.deploy.worker.WorkerArgumentsTest</className>
          <testName>Memory correctly set from args with M appended to memory value</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.deploy.worker.WorkerSuite.xml</file>
      <name>org.apache.spark.deploy.worker.WorkerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.153</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.deploy.worker.WorkerSuite</className>
          <testName>test isUseLocalNodeSSLConfig</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.deploy.worker.WorkerSuite</className>
          <testName>test maybeUpdateSSLSettings</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.deploy.worker.WorkerSuite</className>
          <testName>test clearing of finishedExecutors (small number of executors)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.deploy.worker.WorkerSuite</className>
          <testName>test clearing of finishedExecutors (more executors)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.deploy.worker.WorkerSuite</className>
          <testName>test clearing of finishedDrivers (small number of drivers)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.056</duration>
          <className>org.apache.spark.deploy.worker.WorkerSuite</className>
          <testName>test clearing of finishedDrivers (more drivers)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.deploy.worker.WorkerWatcherSuite.xml</file>
      <name>org.apache.spark.deploy.worker.WorkerWatcherSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.063999996</duration>
      <cases>
        <case>
          <duration>0.033</duration>
          <className>org.apache.spark.deploy.worker.WorkerWatcherSuite</className>
          <testName>WorkerWatcher shuts down on valid disassociation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.deploy.worker.WorkerWatcherSuite</className>
          <testName>WorkerWatcher stays alive on invalid disassociation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.deploy.worker.ui.LogPageSuite.xml</file>
      <name>org.apache.spark.deploy.worker.ui.LogPageSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.018</duration>
      <cases>
        <case>
          <duration>0.018</duration>
          <className>org.apache.spark.deploy.worker.ui.LogPageSuite</className>
          <testName>get logs simple</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.executor.ExecutorSuite.xml</file>
      <name>org.apache.spark.executor.ExecutorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.035</duration>
      <cases>
        <case>
          <duration>0.035</duration>
          <className>org.apache.spark.executor.ExecutorSuite</className>
          <testName>TaskRunner</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.executor.TaskMetricsSuite.xml</file>
      <name>org.apache.spark.executor.TaskMetricsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.004</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.executor.TaskMetricsSuite</className>
          <testName>mutating values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.executor.TaskMetricsSuite</className>
          <testName>mutating shuffle read metrics values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.executor.TaskMetricsSuite</className>
          <testName>mutating shuffle write metrics values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.executor.TaskMetricsSuite</className>
          <testName>mutating input metrics values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.executor.TaskMetricsSuite</className>
          <testName>mutating output metrics values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.executor.TaskMetricsSuite</className>
          <testName>merging multiple shuffle read metrics</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.executor.TaskMetricsSuite</className>
          <testName>additional accumulables</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.input.WholeTextFileRecordReaderSuite.xml</file>
      <name>org.apache.spark.input.WholeTextFileRecordReaderSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.43400002</duration>
      <cases>
        <case>
          <duration>0.257</duration>
          <className>org.apache.spark.input.WholeTextFileRecordReaderSuite</className>
          <testName>Correctness of WholeTextFileRecordReader</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.177</duration>
          <className>org.apache.spark.input.WholeTextFileRecordReaderSuite</className>
          <testName>Correctness of WholeTextFileRecordReader with GzipCodec</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.internal.config.ConfigEntrySuite.xml</file>
      <name>org.apache.spark.internal.config.ConfigEntrySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.011000001</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.internal.config.ConfigEntrySuite</className>
          <testName>conf entry: int</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.internal.config.ConfigEntrySuite</className>
          <testName>conf entry: long</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.internal.config.ConfigEntrySuite</className>
          <testName>conf entry: double</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.internal.config.ConfigEntrySuite</className>
          <testName>conf entry: boolean</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.internal.config.ConfigEntrySuite</className>
          <testName>conf entry: optional</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.internal.config.ConfigEntrySuite</className>
          <testName>conf entry: fallback</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.internal.config.ConfigEntrySuite</className>
          <testName>conf entry: time</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.internal.config.ConfigEntrySuite</className>
          <testName>conf entry: bytes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.internal.config.ConfigEntrySuite</className>
          <testName>conf entry: string seq</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.internal.config.ConfigEntrySuite</className>
          <testName>conf entry: int seq</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.internal.config.ConfigEntrySuite</className>
          <testName>conf entry: transformation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.internal.config.ConfigEntrySuite</className>
          <testName>conf entry: valid values check</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.internal.config.ConfigEntrySuite</className>
          <testName>conf entry: conversion error</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.internal.config.ConfigEntrySuite</className>
          <testName>default value handling is null-safe</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.internal.config.ConfigEntrySuite</className>
          <testName>variable expansion of spark config entries</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.internal.config.ConfigReaderSuite.xml</file>
      <name>org.apache.spark.internal.config.ConfigReaderSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.003</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.internal.config.ConfigReaderSuite</className>
          <testName>variable expansion</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.internal.config.ConfigReaderSuite</className>
          <testName>circular references</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.internal.config.ConfigReaderSuite</className>
          <testName>spark conf provider filters config keys</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.io.ChunkedByteBufferSuite.xml</file>
      <name>org.apache.spark.io.ChunkedByteBufferSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.010000001</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.io.ChunkedByteBufferSuite</className>
          <testName>no chunks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.io.ChunkedByteBufferSuite</className>
          <testName>getChunks() duplicates chunks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.io.ChunkedByteBufferSuite</className>
          <testName>copy() does not affect original buffer&apos;s position</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.io.ChunkedByteBufferSuite</className>
          <testName>writeFully() does not affect original buffer&apos;s position</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.io.ChunkedByteBufferSuite</className>
          <testName>toArray()</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.io.ChunkedByteBufferSuite</className>
          <testName>toArray() throws UnsupportedOperationException if size exceeds 2GB</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.io.ChunkedByteBufferSuite</className>
          <testName>toInputStream()</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.io.CompressionCodecSuite.xml</file>
      <name>org.apache.spark.io.CompressionCodecSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.016</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.io.CompressionCodecSuite</className>
          <testName>default compression codec</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.io.CompressionCodecSuite</className>
          <testName>lz4 compression codec</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.io.CompressionCodecSuite</className>
          <testName>lz4 compression codec short form</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.io.CompressionCodecSuite</className>
          <testName>lz4 supports concatenation of serialized streams</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.io.CompressionCodecSuite</className>
          <testName>lzf compression codec</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.io.CompressionCodecSuite</className>
          <testName>lzf compression codec short form</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.io.CompressionCodecSuite</className>
          <testName>lzf supports concatenation of serialized streams</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.io.CompressionCodecSuite</className>
          <testName>snappy compression codec</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.io.CompressionCodecSuite</className>
          <testName>snappy compression codec short form</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.io.CompressionCodecSuite</className>
          <testName>snappy supports concatenation of serialized streams</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.io.CompressionCodecSuite</className>
          <testName>bad compression codec</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.io.NioBufferedFileInputStreamSuite.xml</file>
      <name>org.apache.spark.io.NioBufferedFileInputStreamSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.34000003</duration>
      <cases>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.io.NioBufferedFileInputStreamSuite</className>
          <testName>testReadMultipleBytes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.16</duration>
          <className>org.apache.spark.io.NioBufferedFileInputStreamSuite</className>
          <testName>testSkipFromFileChannel</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.io.NioBufferedFileInputStreamSuite</className>
          <testName>testNegativeBytesSkippedAfterRead</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.io.NioBufferedFileInputStreamSuite</className>
          <testName>testBytesSkippedAfterEOF</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.io.NioBufferedFileInputStreamSuite</className>
          <testName>testBytesSkippedAfterRead</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.io.NioBufferedFileInputStreamSuite</className>
          <testName>testBytesSkipped</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.io.NioBufferedFileInputStreamSuite</className>
          <testName>testReadOneByte</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.launcher.LauncherBackendSuite.xml</file>
      <name>org.apache.spark.launcher.LauncherBackendSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>6.219</duration>
      <cases>
        <case>
          <duration>2.792</duration>
          <className>org.apache.spark.launcher.LauncherBackendSuite</className>
          <testName>local: launcher handle</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.427</duration>
          <className>org.apache.spark.launcher.LauncherBackendSuite</className>
          <testName>standalone/client: launcher handle</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.launcher.SparkLauncherSuite.xml</file>
      <name>org.apache.spark.launcher.SparkLauncherSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.871</duration>
      <cases>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.launcher.SparkLauncherSuite</className>
          <testName>testRedirectErrorToOutput</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.launcher.SparkLauncherSuite</className>
          <testName>testSparkArgumentHandling</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.launcher.SparkLauncherSuite</className>
          <testName>testRedirectToLogWithOthersFails</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.launcher.SparkLauncherSuite</className>
          <testName>testRedirectsSimple</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.launcher.SparkLauncherSuite</className>
          <testName>testRedirectTwiceFails</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.launcher.SparkLauncherSuite</className>
          <testName>testRedirectLastWins</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.868</duration>
          <className>org.apache.spark.launcher.SparkLauncherSuite</className>
          <testName>testChildProcLauncher</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.launcher.SparkLauncherSuite</className>
          <testName>testRedirectToLog</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.memory.StaticMemoryManagerSuite.xml</file>
      <name>org.apache.spark.memory.StaticMemoryManagerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.6389999</duration>
      <cases>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.memory.StaticMemoryManagerSuite</className>
          <testName>single task requesting on-heap execution memory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.memory.StaticMemoryManagerSuite</className>
          <testName>two tasks requesting full on-heap execution memory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.memory.StaticMemoryManagerSuite</className>
          <testName>two tasks cannot grow past 1 / N of on-heap execution memory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.303</duration>
          <className>org.apache.spark.memory.StaticMemoryManagerSuite</className>
          <testName>tasks can block to get at least 1 / 2N of on-heap execution memory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.305</duration>
          <className>org.apache.spark.memory.StaticMemoryManagerSuite</className>
          <testName>cleanUpAllAllocatedMemory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.memory.StaticMemoryManagerSuite</className>
          <testName>tasks should not be granted a negative amount of execution memory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.memory.StaticMemoryManagerSuite</className>
          <testName>off-heap execution allocations cannot exceed limit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.memory.StaticMemoryManagerSuite</className>
          <testName>basic execution memory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.memory.StaticMemoryManagerSuite</className>
          <testName>basic storage memory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.memory.StaticMemoryManagerSuite</className>
          <testName>execution and storage isolation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.memory.StaticMemoryManagerSuite</className>
          <testName>unroll memory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.memory.TaskMemoryManagerSuite.xml</file>
      <name>org.apache.spark.memory.TaskMemoryManagerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.003</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.memory.TaskMemoryManagerSuite</className>
          <testName>shouldNotForceSpillingInDifferentModes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.memory.TaskMemoryManagerSuite</className>
          <testName>encodePageNumberAndOffsetOnHeap</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.memory.TaskMemoryManagerSuite</className>
          <testName>leakedPageMemoryIsDetected</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.memory.TaskMemoryManagerSuite</className>
          <testName>encodePageNumberAndOffsetOffHeap</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.memory.TaskMemoryManagerSuite</className>
          <testName>cooperativeSpilling</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.memory.TaskMemoryManagerSuite</className>
          <testName>offHeapConfigurationBackwardsCompatibility</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.memory.UnifiedMemoryManagerSuite.xml</file>
      <name>org.apache.spark.memory.UnifiedMemoryManagerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.6219998</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.memory.UnifiedMemoryManagerSuite</className>
          <testName>single task requesting on-heap execution memory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.memory.UnifiedMemoryManagerSuite</className>
          <testName>two tasks requesting full on-heap execution memory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.memory.UnifiedMemoryManagerSuite</className>
          <testName>two tasks cannot grow past 1 / N of on-heap execution memory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.302</duration>
          <className>org.apache.spark.memory.UnifiedMemoryManagerSuite</className>
          <testName>tasks can block to get at least 1 / 2N of on-heap execution memory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.303</duration>
          <className>org.apache.spark.memory.UnifiedMemoryManagerSuite</className>
          <testName>cleanUpAllAllocatedMemory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.memory.UnifiedMemoryManagerSuite</className>
          <testName>tasks should not be granted a negative amount of execution memory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.memory.UnifiedMemoryManagerSuite</className>
          <testName>off-heap execution allocations cannot exceed limit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.memory.UnifiedMemoryManagerSuite</className>
          <testName>basic execution memory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.memory.UnifiedMemoryManagerSuite</className>
          <testName>basic storage memory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.memory.UnifiedMemoryManagerSuite</className>
          <testName>execution evicts storage</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.memory.UnifiedMemoryManagerSuite</className>
          <testName>execution memory requests smaller than free memory should evict storage (SPARK-12165)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.memory.UnifiedMemoryManagerSuite</className>
          <testName>storage does not evict execution</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.memory.UnifiedMemoryManagerSuite</className>
          <testName>small heap</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.memory.UnifiedMemoryManagerSuite</className>
          <testName>insufficient executor memory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.memory.UnifiedMemoryManagerSuite</className>
          <testName>execution can evict cached blocks when there are multiple active tasks (SPARK-12155)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.memory.UnifiedMemoryManagerSuite</className>
          <testName>SPARK-15260: atomically resize memory pools</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.metrics.InputOutputMetricsSuite.xml</file>
      <name>org.apache.spark.metrics.InputOutputMetricsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.72</duration>
      <cases>
        <case>
          <duration>0.129</duration>
          <className>org.apache.spark.metrics.InputOutputMetricsSuite</className>
          <testName>input metrics for old hadoop with coalesce</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.155</duration>
          <className>org.apache.spark.metrics.InputOutputMetricsSuite</className>
          <testName>input metrics with cache and coalesce</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.088</duration>
          <className>org.apache.spark.metrics.InputOutputMetricsSuite</className>
          <testName>input metrics for new Hadoop API with coalesce</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.069</duration>
          <className>org.apache.spark.metrics.InputOutputMetricsSuite</className>
          <testName>input metrics when reading text file</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.metrics.InputOutputMetricsSuite</className>
          <testName>input metrics on records read - simple</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.074</duration>
          <className>org.apache.spark.metrics.InputOutputMetricsSuite</className>
          <testName>input metrics on records read - more stages</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.044</duration>
          <className>org.apache.spark.metrics.InputOutputMetricsSuite</className>
          <testName>input metrics on records - New Hadoop API</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.124</duration>
          <className>org.apache.spark.metrics.InputOutputMetricsSuite</className>
          <testName>input metrics on records read with cache</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.15</duration>
          <className>org.apache.spark.metrics.InputOutputMetricsSuite</className>
          <testName>input read/write and shuffle read/write metrics all line up</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.4</duration>
          <className>org.apache.spark.metrics.InputOutputMetricsSuite</className>
          <testName>input metrics with interleaved reads</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.11</duration>
          <className>org.apache.spark.metrics.InputOutputMetricsSuite</className>
          <testName>output metrics on records written</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.127</duration>
          <className>org.apache.spark.metrics.InputOutputMetricsSuite</className>
          <testName>output metrics on records written - new Hadoop API</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.078</duration>
          <className>org.apache.spark.metrics.InputOutputMetricsSuite</className>
          <testName>output metrics when writing text file</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.059</duration>
          <className>org.apache.spark.metrics.InputOutputMetricsSuite</className>
          <testName>input metrics with old CombineFileInputFormat</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.079</duration>
          <className>org.apache.spark.metrics.InputOutputMetricsSuite</className>
          <testName>input metrics with new CombineFileInputFormat</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.metrics.MetricsConfigSuite.xml</file>
      <name>org.apache.spark.metrics.MetricsConfigSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.0060000005</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.metrics.MetricsConfigSuite</className>
          <testName>MetricsConfig with default properties</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.metrics.MetricsConfigSuite</className>
          <testName>MetricsConfig with properties set from a file</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.metrics.MetricsConfigSuite</className>
          <testName>MetricsConfig with properties set from a Spark configuration</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.metrics.MetricsConfigSuite</className>
          <testName>MetricsConfig with properties set from a file and a Spark configuration</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.metrics.MetricsConfigSuite</className>
          <testName>MetricsConfig with subProperties</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.metrics.MetricsSystemSuite.xml</file>
      <name>org.apache.spark.metrics.MetricsSystemSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.033</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.metrics.MetricsSystemSuite</className>
          <testName>MetricsSystem with default config</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.metrics.MetricsSystemSuite</className>
          <testName>MetricsSystem with sources add</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.metrics.MetricsSystemSuite</className>
          <testName>MetricsSystem with Driver instance</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.metrics.MetricsSystemSuite</className>
          <testName>id is not set</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.metrics.MetricsSystemSuite</className>
          <testName>id is not set</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.metrics.MetricsSystemSuite</className>
          <testName>MetricsSystem with Executor instance</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.metrics.MetricsSystemSuite</className>
          <testName>id is not set</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.metrics.MetricsSystemSuite</className>
          <testName>id is not set</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.metrics.MetricsSystemSuite</className>
          <testName>MetricsSystem with instance which is neither Driver nor Executor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.metrics.MetricsSystemSuite</className>
          <testName>MetricsSystem with Executor instance, with custom namespace</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.metrics.MetricsSystemSuite</className>
          <testName>MetricsSystem with Executor instance, custom namespace which is not set</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.metrics.MetricsSystemSuite</className>
          <testName>id not set</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.metrics.MetricsSystemSuite</className>
          <testName>MetricsSystem with non-driver, non-executor instance with custom namespace</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.network.netty.NettyBlockTransferSecuritySuite.xml</file>
      <name>org.apache.spark.network.netty.NettyBlockTransferSecuritySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.659</duration>
      <cases>
        <case>
          <duration>0.251</duration>
          <className>org.apache.spark.network.netty.NettyBlockTransferSecuritySuite</className>
          <testName>security default off</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.221</duration>
          <className>org.apache.spark.network.netty.NettyBlockTransferSecuritySuite</className>
          <testName>security on same password</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.network.netty.NettyBlockTransferSecuritySuite</className>
          <testName>security on mismatch password</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.117</duration>
          <className>org.apache.spark.network.netty.NettyBlockTransferSecuritySuite</className>
          <testName>security mismatch auth off on server</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.039</duration>
          <className>org.apache.spark.network.netty.NettyBlockTransferSecuritySuite</className>
          <testName>security mismatch auth off on client</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.network.netty.NettyBlockTransferServiceSuite.xml</file>
      <name>org.apache.spark.network.netty.NettyBlockTransferServiceSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.024</duration>
      <cases>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.network.netty.NettyBlockTransferServiceSuite</className>
          <testName>can bind to a random port</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.network.netty.NettyBlockTransferServiceSuite</className>
          <testName>can bind to two random ports</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.network.netty.NettyBlockTransferServiceSuite</className>
          <testName>can bind to a specific port</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.network.netty.NettyBlockTransferServiceSuite</className>
          <testName>can bind to a specific port twice and the second increments</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.partial.CountEvaluatorSuite.xml</file>
      <name>org.apache.spark.partial.CountEvaluatorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.0050000004</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.partial.CountEvaluatorSuite</className>
          <testName>test count 0</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.partial.CountEvaluatorSuite</className>
          <testName>test count &gt;= 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.partial.MeanEvaluatorSuite.xml</file>
      <name>org.apache.spark.partial.MeanEvaluatorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.003</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.partial.MeanEvaluatorSuite</className>
          <testName>test count 0</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.partial.MeanEvaluatorSuite</className>
          <testName>test count 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.partial.MeanEvaluatorSuite</className>
          <testName>test count &gt; 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.partial.SumEvaluatorSuite.xml</file>
      <name>org.apache.spark.partial.SumEvaluatorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.011000001</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.partial.SumEvaluatorSuite</className>
          <testName>correct handling of count 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.partial.SumEvaluatorSuite</className>
          <testName>correct handling of count 0</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.partial.SumEvaluatorSuite</className>
          <testName>correct handling of NaN</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.partial.SumEvaluatorSuite</className>
          <testName>correct handling of &gt; 1 values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.partial.SumEvaluatorSuite</className>
          <testName>test count &gt; 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.rdd.AsyncRDDActionsSuite.xml</file>
      <name>org.apache.spark.rdd.AsyncRDDActionsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>3.0249999</duration>
      <cases>
        <case>
          <duration>0.018</duration>
          <className>org.apache.spark.rdd.AsyncRDDActionsSuite</className>
          <testName>countAsync</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.rdd.AsyncRDDActionsSuite</className>
          <testName>collectAsync</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.rdd.AsyncRDDActionsSuite</className>
          <testName>foreachAsync</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.rdd.AsyncRDDActionsSuite</className>
          <testName>foreachPartitionAsync</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.802</duration>
          <className>org.apache.spark.rdd.AsyncRDDActionsSuite</className>
          <testName>takeAsync</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.rdd.AsyncRDDActionsSuite</className>
          <testName>async success handling</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.rdd.AsyncRDDActionsSuite</className>
          <testName>async failure handling</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.rdd.AsyncRDDActionsSuite</className>
          <testName>FutureAction result, infinite wait</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.rdd.AsyncRDDActionsSuite</className>
          <testName>FutureAction result, finite wait</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.rdd.AsyncRDDActionsSuite</className>
          <testName>FutureAction result, timeout</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.rdd.AsyncRDDActionsSuite</className>
          <testName>SimpleFutureAction callback must not consume a thread while waiting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.rdd.AsyncRDDActionsSuite</className>
          <testName>ComplexFutureAction callback must not consume a thread while waiting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.rdd.DoubleRDDSuite.xml</file>
      <name>org.apache.spark.rdd.DoubleRDDSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.28</duration>
      <cases>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.rdd.DoubleRDDSuite</className>
          <testName>sum</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.rdd.DoubleRDDSuite</className>
          <testName>WorksOnEmpty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.rdd.DoubleRDDSuite</className>
          <testName>WorksWithOutOfRangeWithOneBucket</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.05</duration>
          <className>org.apache.spark.rdd.DoubleRDDSuite</className>
          <testName>WorksInRangeWithOneBucket</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.rdd.DoubleRDDSuite</className>
          <testName>WorksInRangeWithOneBucketExactMatch</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.rdd.DoubleRDDSuite</className>
          <testName>WorksWithOutOfRangeWithTwoBuckets</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.rdd.DoubleRDDSuite</className>
          <testName>WorksWithOutOfRangeWithTwoUnEvenBuckets</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.rdd.DoubleRDDSuite</className>
          <testName>WorksInRangeWithTwoBuckets</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.rdd.DoubleRDDSuite</className>
          <testName>WorksInRangeWithTwoBucketsAndNaN</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.rdd.DoubleRDDSuite</className>
          <testName>WorksInRangeWithTwoUnevenBuckets</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.033</duration>
          <className>org.apache.spark.rdd.DoubleRDDSuite</className>
          <testName>WorksMixedRangeWithTwoUnevenBuckets</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.rdd.DoubleRDDSuite</className>
          <testName>WorksMixedRangeWithFourUnevenBuckets</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.rdd.DoubleRDDSuite</className>
          <testName>WorksMixedRangeWithUnevenBucketsAndNaN</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.rdd.DoubleRDDSuite</className>
          <testName>WorksMixedRangeWithUnevenBucketsAndNaNAndNaNRange</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.rdd.DoubleRDDSuite</className>
          <testName>WorksMixedRangeWithUnevenBucketsAndNaNAndNaNRangeAndInfinity</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.rdd.DoubleRDDSuite</className>
          <testName>WorksWithOutOfRangeWithInfiniteBuckets</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.rdd.DoubleRDDSuite</className>
          <testName>ThrowsExceptionOnInvalidBucketArray</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.033</duration>
          <className>org.apache.spark.rdd.DoubleRDDSuite</className>
          <testName>WorksWithoutBucketsBasic</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.rdd.DoubleRDDSuite</className>
          <testName>WorksWithoutBucketsBasicSingleElement</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.rdd.DoubleRDDSuite</className>
          <testName>WorksWithoutBucketsBasicNoRange</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.rdd.DoubleRDDSuite</className>
          <testName>WorksWithoutBucketsBasicTwo</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.057</duration>
          <className>org.apache.spark.rdd.DoubleRDDSuite</className>
          <testName>WorksWithDoubleValuesAtMinMax</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.rdd.DoubleRDDSuite</className>
          <testName>WorksWithoutBucketsWithMoreRequestedThanElements</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.rdd.DoubleRDDSuite</className>
          <testName>WorksWithoutBucketsForLargerDatasets</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.rdd.DoubleRDDSuite</className>
          <testName>WorksWithoutBucketsWithNonIntegralBucketEdges</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.556</duration>
          <className>org.apache.spark.rdd.DoubleRDDSuite</className>
          <testName>WorksWithHugeRange</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.05</duration>
          <className>org.apache.spark.rdd.DoubleRDDSuite</className>
          <testName>ThrowsExceptionOnInvalidRDDs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.rdd.JdbcRDDSuite.xml</file>
      <name>org.apache.spark.rdd.JdbcRDDSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.365</duration>
      <cases>
        <case>
          <duration>0.143</duration>
          <className>org.apache.spark.rdd.JdbcRDDSuite</className>
          <testName>basic functionality</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.222</duration>
          <className>org.apache.spark.rdd.JdbcRDDSuite</className>
          <testName>large id overflow</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.rdd.LocalCheckpointSuite.xml</file>
      <name>org.apache.spark.rdd.LocalCheckpointSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>10.678</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.rdd.LocalCheckpointSuite</className>
          <testName>transform storage level</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.044</duration>
          <className>org.apache.spark.rdd.LocalCheckpointSuite</className>
          <testName>basic lineage truncation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.rdd.LocalCheckpointSuite</className>
          <testName>basic lineage truncation - caching before checkpointing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.04</duration>
          <className>org.apache.spark.rdd.LocalCheckpointSuite</className>
          <testName>basic lineage truncation - caching after checkpointing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.068</duration>
          <className>org.apache.spark.rdd.LocalCheckpointSuite</className>
          <testName>indirect lineage truncation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.rdd.LocalCheckpointSuite</className>
          <testName>indirect lineage truncation - caching before checkpointing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.046</duration>
          <className>org.apache.spark.rdd.LocalCheckpointSuite</className>
          <testName>indirect lineage truncation - caching after checkpointing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.495</duration>
          <className>org.apache.spark.rdd.LocalCheckpointSuite</className>
          <testName>checkpoint without draining iterator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.37</duration>
          <className>org.apache.spark.rdd.LocalCheckpointSuite</className>
          <testName>checkpoint without draining iterator - caching before checkpointing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.411</duration>
          <className>org.apache.spark.rdd.LocalCheckpointSuite</className>
          <testName>checkpoint without draining iterator - caching after checkpointing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.rdd.LocalCheckpointSuite</className>
          <testName>checkpoint blocks exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.rdd.LocalCheckpointSuite</className>
          <testName>checkpoint blocks exist - caching before checkpointing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.rdd.LocalCheckpointSuite</className>
          <testName>checkpoint blocks exist - caching after checkpointing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.04</duration>
          <className>org.apache.spark.rdd.LocalCheckpointSuite</className>
          <testName>missing checkpoint block fails with informative message</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.rdd.PairRDDFunctionsSuite.xml</file>
      <name>org.apache.spark.rdd.PairRDDFunctionsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>11.023002</duration>
      <cases>
        <case>
          <duration>0.032</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>aggregateByKey</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>groupByKey</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>groupByKey with duplicates</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>groupByKey with negative key hash codes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>groupByKey with many output partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.146</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>sampleByKey</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>5.462</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>sampleByKeyExact</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>reduceByKey</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>reduceByKey with collectAsMap</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>reduceByKey with many output partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>reduceByKey with partitioner</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.093</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>countApproxDistinctByKey</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>join all-to-all</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>leftOuterJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>cogroup with empty RDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>cogroup with groupByed RDD having 0 partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>rightOuterJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>fullOuterJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>join with no matches</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.038</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>join with many output partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>groupWith</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>groupWith3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.044</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>groupWith4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>zero-partition RDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>keys and values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>default partitioner uses partition size</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>default partitioner uses largest partitioner</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>subtract</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>subtract with narrow dependency</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>subtractByKey</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>subtractByKey with narrow dependency</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>foldByKey</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.041</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>foldByKey with mutable result type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.075</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>saveNewAPIHadoopFile should call setConf if format is configurable</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.052</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>saveAsHadoopFile should respect configured output committers</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.038</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>close() in saveNewAPIHadoopFile</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.051</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>close() in saveAsHadoopFile</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.086</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>lookup</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.046</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>lookup with partitioner</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.rdd.PairRDDFunctionsSuite</className>
          <testName>lookup with bad partitioner</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.rdd.ParallelCollectionSplitSuite.xml</file>
      <name>org.apache.spark.rdd.ParallelCollectionSplitSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.5179999</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.rdd.ParallelCollectionSplitSuite</className>
          <testName>one element per slice</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.rdd.ParallelCollectionSplitSuite</className>
          <testName>one slice</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.rdd.ParallelCollectionSplitSuite</className>
          <testName>equal slices</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.rdd.ParallelCollectionSplitSuite</className>
          <testName>non-equal slices</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.rdd.ParallelCollectionSplitSuite</className>
          <testName>splitting exclusive range</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.rdd.ParallelCollectionSplitSuite</className>
          <testName>splitting inclusive range</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.rdd.ParallelCollectionSplitSuite</className>
          <testName>empty data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.rdd.ParallelCollectionSplitSuite</className>
          <testName>zero slices</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.rdd.ParallelCollectionSplitSuite</className>
          <testName>negative number of slices</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.rdd.ParallelCollectionSplitSuite</className>
          <testName>exclusive ranges sliced into ranges</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.rdd.ParallelCollectionSplitSuite</className>
          <testName>inclusive ranges sliced into ranges</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.rdd.ParallelCollectionSplitSuite</className>
          <testName>identical slice sizes between Range and NumericRange</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.rdd.ParallelCollectionSplitSuite</className>
          <testName>identical slice sizes between List and NumericRange</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.rdd.ParallelCollectionSplitSuite</className>
          <testName>large ranges don&apos;t overflow</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.369</duration>
          <className>org.apache.spark.rdd.ParallelCollectionSplitSuite</className>
          <testName>random array tests</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.055</duration>
          <className>org.apache.spark.rdd.ParallelCollectionSplitSuite</className>
          <testName>random exclusive range tests</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.05</duration>
          <className>org.apache.spark.rdd.ParallelCollectionSplitSuite</className>
          <testName>random inclusive range tests</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.rdd.ParallelCollectionSplitSuite</className>
          <testName>exclusive ranges of longs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.rdd.ParallelCollectionSplitSuite</className>
          <testName>inclusive ranges of longs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.rdd.ParallelCollectionSplitSuite</className>
          <testName>exclusive ranges of doubles</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.rdd.ParallelCollectionSplitSuite</className>
          <testName>inclusive ranges of doubles</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.rdd.ParallelCollectionSplitSuite</className>
          <testName>MinValue</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.rdd.ParallelCollectionSplitSuite</className>
          <testName>MinValue</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.rdd.PartitionPruningRDDSuite.xml</file>
      <name>org.apache.spark.rdd.PartitionPruningRDDSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.043</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.rdd.PartitionPruningRDDSuite</className>
          <testName>Pruned Partitions inherit locality prefs correctly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.041</duration>
          <className>org.apache.spark.rdd.PartitionPruningRDDSuite</className>
          <testName>Pruned Partitions can be unioned</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.rdd.PartitionwiseSampledRDDSuite.xml</file>
      <name>org.apache.spark.rdd.PartitionwiseSampledRDDSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.186</duration>
      <cases>
        <case>
          <duration>0.079</duration>
          <className>org.apache.spark.rdd.PartitionwiseSampledRDDSuite</className>
          <testName>seed distribution</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.107</duration>
          <className>org.apache.spark.rdd.PartitionwiseSampledRDDSuite</className>
          <testName>concurrency</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.rdd.PipedRDDSuite.xml</file>
      <name>org.apache.spark.rdd.PipedRDDSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.503</duration>
      <cases>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.rdd.PipedRDDSuite</className>
          <testName>basic pipe</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.044</duration>
          <className>org.apache.spark.rdd.PipedRDDSuite</className>
          <testName>basic pipe with tokenization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.032</duration>
          <className>org.apache.spark.rdd.PipedRDDSuite</className>
          <testName>failure in iterating over pipe input</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.081</duration>
          <className>org.apache.spark.rdd.PipedRDDSuite</className>
          <testName>advanced pipe</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.076</duration>
          <className>org.apache.spark.rdd.PipedRDDSuite</className>
          <testName>pipe with empty partition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.rdd.PipedRDDSuite</className>
          <testName>pipe with env variable</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.rdd.PipedRDDSuite</className>
          <testName>pipe with process which cannot be launched due to bad command</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.rdd.PipedRDDSuite</className>
          <testName>pipe with process which is launched but fails with non-zero exit status</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.065</duration>
          <className>org.apache.spark.rdd.PipedRDDSuite</className>
          <testName>basic pipe with separate working directory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.049</duration>
          <className>org.apache.spark.rdd.PipedRDDSuite</className>
          <testName>test pipe exports map_input_file</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.035</duration>
          <className>org.apache.spark.rdd.PipedRDDSuite</className>
          <testName>test pipe exports mapreduce_map_input_file</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.rdd.RDDOperationScopeSuite.xml</file>
      <name>org.apache.spark.rdd.RDDOperationScopeSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.028</duration>
      <cases>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.rdd.RDDOperationScopeSuite</className>
          <testName>equals and hashCode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.rdd.RDDOperationScopeSuite</className>
          <testName>getAllScopes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.rdd.RDDOperationScopeSuite</className>
          <testName>json de/serialization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.rdd.RDDOperationScopeSuite</className>
          <testName>withScope</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.rdd.RDDOperationScopeSuite</className>
          <testName>withScope with partial nesting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.rdd.RDDOperationScopeSuite</className>
          <testName>withScope with multiple layers of nesting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.rdd.RDDSuite.xml</file>
      <name>org.apache.spark.rdd.RDDSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>28.788</duration>
      <cases>
        <case>
          <duration>0.367</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>basic operations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>serialization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.095</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>countApproxDistinct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>union</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.069</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>union parallel partition listing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>union creates UnionRDD if at least one RDD has no partitioner</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>union creates PartitionAwareUnionRDD if all RDDs have partitioners</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>PartitionAwareUnionRDD raises exception if at least one RDD has no partitioner</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.084</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>partitioner aware union</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>UnionRDD partition serialized size should be small</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>aggregate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.33</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>treeAggregate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.327</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>treeReduce</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>basic caching</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>caching with failures</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.151</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>empty RDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.262</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>repartitioned RDDs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.391</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>repartitioned RDDs perform load balancing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.182</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>coalesced RDDs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.057</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>coalesced RDDs with locality</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.04</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>coalesced RDDs with partial locality</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.322</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>coalesced RDDs with locality, large scale (10K partitions)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.476</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>coalesced RDDs with partial locality, large scale (10K partitions)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>coalesced RDDs with locality, fail first pass</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>zipped RDDs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>partition pruning</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.4</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>collect large number of empty partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.357</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>take</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.086</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>top with predefined ordering</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>top with custom ordering</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>takeOrdered with predefined ordering</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>takeOrdered with limit 0</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>takeOrdered with custom ordering</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.061</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>isEmpty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>sample preserves partitioner</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>16.183</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>takeSample</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>takeSample from an empty rdd</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.315</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>randomSplit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>runJob on an invalid partition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>sort an empty RDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.095</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>sortByKey</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.101</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>sortByKey ascending parameter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.065</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>sortByKey with explicit ordering</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>repartitionAndSortWithinPartitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.058</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>intersection</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.055</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>intersection strips duplicates in an input</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>zipWithIndex</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>zipWithIndex with a single partition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>zipWithIndex chained with other RDDs (SPARK-4433)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>zipWithUniqueId</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.038</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>retag with implicit ClassTag</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>parent method</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>getNarrowAncestors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>getNarrowAncestors with multiple parents</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>getNarrowAncestors with cycles</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>task serialization exception should not hang scheduler</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>partitions() fails fast when partitions indicies are incorrect (SPARK-13021)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>nested RDDs are not supported (SPARK-5063)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>actions cannot be performed inside of transformations (SPARK-5063)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.329</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>custom RDD coalescer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>cannot run actions after SparkContext has been stopped (SPARK-5063)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.rdd.RDDSuite</className>
          <testName>cannot call methods on a stopped SparkContext (SPARK-5063)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.rdd.SortingSuite.xml</file>
      <name>org.apache.spark.rdd.SortingSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.432</duration>
      <cases>
        <case>
          <duration>0.081</duration>
          <className>org.apache.spark.rdd.SortingSuite</className>
          <testName>sortByKey</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.097</duration>
          <className>org.apache.spark.rdd.SortingSuite</className>
          <testName>large array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.081</duration>
          <className>org.apache.spark.rdd.SortingSuite</className>
          <testName>large array with one split</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.21</duration>
          <className>org.apache.spark.rdd.SortingSuite</className>
          <testName>large array with many partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.081</duration>
          <className>org.apache.spark.rdd.SortingSuite</className>
          <testName>sort descending</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.076</duration>
          <className>org.apache.spark.rdd.SortingSuite</className>
          <testName>sort descending with one split</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.147</duration>
          <className>org.apache.spark.rdd.SortingSuite</className>
          <testName>sort descending with many partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.108</duration>
          <className>org.apache.spark.rdd.SortingSuite</className>
          <testName>more partitions than elements</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.052</duration>
          <className>org.apache.spark.rdd.SortingSuite</className>
          <testName>empty RDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.096</duration>
          <className>org.apache.spark.rdd.SortingSuite</className>
          <testName>partition balancing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.105</duration>
          <className>org.apache.spark.rdd.SortingSuite</className>
          <testName>partition balancing for descending sort</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.085</duration>
          <className>org.apache.spark.rdd.SortingSuite</className>
          <testName>get a range of elements in a sorted RDD that is on one partition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.087</duration>
          <className>org.apache.spark.rdd.SortingSuite</className>
          <testName>get a range of elements over multiple partitions in a descendingly sorted RDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.rdd.SortingSuite</className>
          <testName>get a range of elements in an array not partitioned by a range partitioner</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.097</duration>
          <className>org.apache.spark.rdd.SortingSuite</className>
          <testName>get a range of elements over multiple partitions but not taking up full partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.rdd.ZippedPartitionsSuite.xml</file>
      <name>org.apache.spark.rdd.ZippedPartitionsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.022</duration>
      <cases>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.rdd.ZippedPartitionsSuite</className>
          <testName>print sizes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.rpc.RpcAddressSuite.xml</file>
      <name>org.apache.spark.rpc.RpcAddressSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.002</duration>
      <cases>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.rpc.RpcAddressSuite</className>
          <testName>hostPort</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.rpc.RpcAddressSuite</className>
          <testName>fromSparkURL</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.rpc.RpcAddressSuite</className>
          <testName>fromSparkURL: a typo url</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.rpc.RpcAddressSuite</className>
          <testName>fromSparkURL: invalid scheme</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.rpc.RpcAddressSuite</className>
          <testName>toSparkURL</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.rpc.netty.InboxSuite.xml</file>
      <name>org.apache.spark.rpc.netty.InboxSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.028</duration>
      <cases>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.rpc.netty.InboxSuite</className>
          <testName>post</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.rpc.netty.InboxSuite</className>
          <testName>post: with reply</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.rpc.netty.InboxSuite</className>
          <testName>post: multiple threads</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.rpc.netty.InboxSuite</className>
          <testName>post: Associated</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.rpc.netty.InboxSuite</className>
          <testName>post: Disassociated</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.rpc.netty.InboxSuite</className>
          <testName>post: AssociationError</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.rpc.netty.NettyRpcAddressSuite.xml</file>
      <name>org.apache.spark.rpc.netty.NettyRpcAddressSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.0</duration>
      <cases>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.rpc.netty.NettyRpcAddressSuite</className>
          <testName>toString</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.rpc.netty.NettyRpcAddressSuite</className>
          <testName>toString for client mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.rpc.netty.NettyRpcEnvSuite.xml</file>
      <name>org.apache.spark.rpc.netty.NettyRpcEnvSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.9309999</duration>
      <cases>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.rpc.netty.NettyRpcEnvSuite</className>
          <testName>send a message locally</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.073</duration>
          <className>org.apache.spark.rpc.netty.NettyRpcEnvSuite</className>
          <testName>send a message remotely</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.rpc.netty.NettyRpcEnvSuite</className>
          <testName>send a RpcEndpointRef</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.rpc.netty.NettyRpcEnvSuite</className>
          <testName>ask a message locally</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.rpc.netty.NettyRpcEnvSuite</className>
          <testName>ask a message remotely</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.076</duration>
          <className>org.apache.spark.rpc.netty.NettyRpcEnvSuite</className>
          <testName>ask a message timeout</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.rpc.netty.NettyRpcEnvSuite</className>
          <testName>onStart and onStop</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.rpc.netty.NettyRpcEnvSuite</className>
          <testName>onError: error in onStart</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.rpc.netty.NettyRpcEnvSuite</className>
          <testName>onError: error in onStop</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.rpc.netty.NettyRpcEnvSuite</className>
          <testName>onError: error in receive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.rpc.netty.NettyRpcEnvSuite</className>
          <testName>self: call in onStart</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.rpc.netty.NettyRpcEnvSuite</className>
          <testName>self: call in receive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.rpc.netty.NettyRpcEnvSuite</className>
          <testName>self: call in onStop</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.941</duration>
          <className>org.apache.spark.rpc.netty.NettyRpcEnvSuite</className>
          <testName>call receive in sequence</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.rpc.netty.NettyRpcEnvSuite</className>
          <testName>stop(RpcEndpointRef) reentrant</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.rpc.netty.NettyRpcEnvSuite</className>
          <testName>sendWithReply</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.062</duration>
          <className>org.apache.spark.rpc.netty.NettyRpcEnvSuite</className>
          <testName>sendWithReply: remotely</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.rpc.netty.NettyRpcEnvSuite</className>
          <testName>sendWithReply: error</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.rpc.netty.NettyRpcEnvSuite</className>
          <testName>sendWithReply: remotely error</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.064</duration>
          <className>org.apache.spark.rpc.netty.NettyRpcEnvSuite</className>
          <testName>network events in sever RpcEnv when another RpcEnv is in server mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.11</duration>
          <className>org.apache.spark.rpc.netty.NettyRpcEnvSuite</className>
          <testName>network events in sever RpcEnv when another RpcEnv is in client mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.071</duration>
          <className>org.apache.spark.rpc.netty.NettyRpcEnvSuite</className>
          <testName>network events in client RpcEnv when another RpcEnv is in server mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.04</duration>
          <className>org.apache.spark.rpc.netty.NettyRpcEnvSuite</className>
          <testName>sendWithReply: unserializable error</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.rpc.netty.NettyRpcEnvSuite</className>
          <testName>port conflict</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.074</duration>
          <className>org.apache.spark.rpc.netty.NettyRpcEnvSuite</className>
          <testName>send with authentication</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.054</duration>
          <className>org.apache.spark.rpc.netty.NettyRpcEnvSuite</className>
          <testName>ask with authentication</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.rpc.netty.NettyRpcEnvSuite</className>
          <testName>construct RpcTimeout with conf property</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.rpc.netty.NettyRpcEnvSuite</className>
          <testName>ask a message timeout on Future using RpcTimeout</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.114</duration>
          <className>org.apache.spark.rpc.netty.NettyRpcEnvSuite</className>
          <testName>file server</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.077</duration>
          <className>org.apache.spark.rpc.netty.NettyRpcEnvSuite</className>
          <testName>shutdown should not fire onDisconnected events</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.rpc.netty.NettyRpcEnvSuite</className>
          <testName>isInRPCThread</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.rpc.netty.NettyRpcEnvSuite</className>
          <testName>non-existent endpoint</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.rpc.netty.NettyRpcEnvSuite</className>
          <testName>advertise address different from bind address</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.rpc.netty.NettyRpcHandlerSuite.xml</file>
      <name>org.apache.spark.rpc.netty.NettyRpcHandlerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.01</duration>
      <cases>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.rpc.netty.NettyRpcHandlerSuite</className>
          <testName>receive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.rpc.netty.NettyRpcHandlerSuite</className>
          <testName>connectionTerminated</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.scheduler.AdaptiveSchedulingSuite.xml</file>
      <name>org.apache.spark.scheduler.AdaptiveSchedulingSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.263</duration>
      <cases>
        <case>
          <duration>0.078</duration>
          <className>org.apache.spark.scheduler.AdaptiveSchedulingSuite</className>
          <testName>simple use of submitMapStage</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.058</duration>
          <className>org.apache.spark.scheduler.AdaptiveSchedulingSuite</className>
          <testName>fetching multiple map output partitions per reduce</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.06</duration>
          <className>org.apache.spark.scheduler.AdaptiveSchedulingSuite</className>
          <testName>fetching all map output partitions in one reduce</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.067</duration>
          <className>org.apache.spark.scheduler.AdaptiveSchedulingSuite</className>
          <testName>more reduce tasks than map output partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.scheduler.BasicSchedulerIntegrationSuite.xml</file>
      <name>org.apache.spark.scheduler.BasicSchedulerIntegrationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.62700003</duration>
      <cases>
        <case>
          <duration>0.081</duration>
          <className>org.apache.spark.scheduler.BasicSchedulerIntegrationSuite</className>
          <testName>super simple job</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.149</duration>
          <className>org.apache.spark.scheduler.BasicSchedulerIntegrationSuite</className>
          <testName>multi-stage job</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.329</duration>
          <className>org.apache.spark.scheduler.BasicSchedulerIntegrationSuite</className>
          <testName>job with fetch failure</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.068</duration>
          <className>org.apache.spark.scheduler.BasicSchedulerIntegrationSuite</className>
          <testName>job failure after 4 attempts</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.scheduler.BlacklistIntegrationSuite.xml</file>
      <name>org.apache.spark.scheduler.BlacklistIntegrationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.366</duration>
      <cases>
        <case>
          <duration>0.08</duration>
          <className>org.apache.spark.scheduler.BlacklistIntegrationSuite</className>
          <testName>If preferred node is bad, without blacklist job will fail</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.122</duration>
          <className>org.apache.spark.scheduler.BlacklistIntegrationSuite</className>
          <testName>With default settings, job can succeed despite multiple bad executors on node</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.1</duration>
          <className>org.apache.spark.scheduler.BlacklistIntegrationSuite</className>
          <testName>Bad node with multiple executors, job will still succeed with the right confs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.064</duration>
          <className>org.apache.spark.scheduler.BlacklistIntegrationSuite</className>
          <testName>SPARK-15865 Progress with fewer executors than maxTaskFailures</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.scheduler.BlacklistTrackerSuite.xml</file>
      <name>org.apache.spark.scheduler.BlacklistTrackerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.006</duration>
      <cases>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.scheduler.BlacklistTrackerSuite</className>
          <testName>blacklist still respects legacy configs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.scheduler.BlacklistTrackerSuite</className>
          <testName>check blacklist configuration invariants</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.scheduler.CoarseGrainedSchedulerBackendSuite.xml</file>
      <name>org.apache.spark.scheduler.CoarseGrainedSchedulerBackendSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>3.785</duration>
      <cases>
        <case>
          <duration>3.785</duration>
          <className>org.apache.spark.scheduler.CoarseGrainedSchedulerBackendSuite</className>
          <testName>serialized task larger than max RPC message size</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.scheduler.DAGSchedulerSuite.xml</file>
      <name>org.apache.spark.scheduler.DAGSchedulerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.6599998</duration>
      <cases>
        <case>
          <duration>0.045</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>[SPARK-3353] parent stage should have lower stage id</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.018</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>[SPARK-13902] Ensure no duplicate stages are created</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>zero split job</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>run trivial job</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>run trivial job w/ dependency</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>equals and hashCode AccumulableInfo</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.033</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>cache location preferences w/ dependency</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>regression test for getCacheLocs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>getMissingParentStages should consider all ancestor RDDs&apos; cache statuses</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.06</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>avoid exponential blowup when getting preferred locs list</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>unserializable task</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>trivial job failure</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>trivial job cancellation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>job cancellation no-kill backend</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>run trivial shuffle</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.041</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>run trivial shuffle with fetch failure</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.067</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>shuffle files not lost when slave lost with shuffle service</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.066</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>shuffle files lost when worker lost with shuffle service</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.059</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>shuffle files lost when worker lost without shuffle service</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.06</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>shuffle files not lost when executor failure with shuffle service</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.056</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>shuffle files lost when executor failure without shuffle service</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>Single stage fetch failure should not abort the stage</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>Multiple consecutive stage fetch failures should lead to job being aborted</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>Failures in different stages should not trigger an overall abort</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.06</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>Non-consecutive stage failures don&apos;t trigger abort</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.018</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>trivial shuffle with multiple fetch failures</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>late fetch failures don&apos;t cause multiple concurrent attempts for the same map stage</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>extremely late fetch failures don&apos;t cause multiple concurrent attempts for the same stage</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>task events always posted in speculation / when stage is killed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>ignore late map task completions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>run shuffle with map stage failure</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>shuffle fetch failure in a reused shuffle dependency</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>don&apos;t submit stage until its dependencies map outputs are registered (SPARK-5259)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>register map outputs correctly after ExecutorLost and task Resubmitted</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>failure of stage used by two jobs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>stage used by two jobs, the first no longer active (SPARK-6880)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.018</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>stage used by two jobs, some fetch failures, and the first job no longer active (SPARK-6880)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>run trivial shuffle with out-of-band failure and retry</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.041</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>recursive shuffle failures</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>cached post-shuffle</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>misbehaved accumulator should not crash DAGScheduler and SparkContext</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>misbehaved resultHandler should not crash DAGScheduler and SparkContext</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>getPartitions exceptions should not crash DAGScheduler and SparkContext (SPARK-8606)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>getPreferredLocations errors should not crash DAGScheduler and SparkContext (SPARK-8606)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>accumulator not calculated for resubmitted result stage</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>accumulators are updated on exception failures</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>reduce tasks should be placed locally with map output</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>reduce task locality preferences should only include machines with largest map outputs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>stages with both narrow and shuffle dependencies use narrow ones for locality</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>Spark exceptions should include call site in stack trace</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>catch errors in event loop</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>simple map stage submission</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>map stage submission with reduce stage also depending on the data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>map stage submission with fetch failure</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>map stage submission with multiple shared stages and failures</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>map stage submission with executor failure late map task completions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>getShuffleDependencies correctly returns only direct shuffle parents</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.383</duration>
          <className>org.apache.spark.scheduler.DAGSchedulerSuite</className>
          <testName>SPARK-17644: After one stage is aborted for too many failed attempts, subsequent stagesstill behave correctly on fetch failures</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.scheduler.EventLoggingListenerSuite.xml</file>
      <name>org.apache.spark.scheduler.EventLoggingListenerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>13.13</duration>
      <cases>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.scheduler.EventLoggingListenerSuite</className>
          <testName>Verify log file exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.scheduler.EventLoggingListenerSuite</className>
          <testName>Basic event logging</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.085</duration>
          <className>org.apache.spark.scheduler.EventLoggingListenerSuite</className>
          <testName>Basic event logging with compression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.548</duration>
          <className>org.apache.spark.scheduler.EventLoggingListenerSuite</className>
          <testName>End-to-end event logging</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>9.398</duration>
          <className>org.apache.spark.scheduler.EventLoggingListenerSuite</className>
          <testName>End-to-end event logging with compression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.054</duration>
          <className>org.apache.spark.scheduler.EventLoggingListenerSuite</className>
          <testName>Log overwriting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.scheduler.EventLoggingListenerSuite</className>
          <testName>Event log name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.scheduler.ExternalClusterManagerSuite.xml</file>
      <name>org.apache.spark.scheduler.ExternalClusterManagerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.037</duration>
      <cases>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.scheduler.ExternalClusterManagerSuite</className>
          <testName>launch of backend and scheduler</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.scheduler.JobWaiterSuite.xml</file>
      <name>org.apache.spark.scheduler.JobWaiterSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.005</duration>
      <cases>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.scheduler.JobWaiterSuite</className>
          <testName>call jobFailed multiple times</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.scheduler.MapStatusSuite.xml</file>
      <name>org.apache.spark.scheduler.MapStatusSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.35500002</duration>
      <cases>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.scheduler.MapStatusSuite</className>
          <testName>compressSize</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.scheduler.MapStatusSuite</className>
          <testName>decompressSize</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.333</duration>
          <className>org.apache.spark.scheduler.MapStatusSuite</className>
          <testName>MapStatus should never report non-empty blocks&apos; sizes as 0</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.scheduler.MapStatusSuite</className>
          <testName>HighlyCompressedMapStatus</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.scheduler.MapStatusSuite</className>
          <testName>HighlyCompressedMapStatus: estimated size should be the average non-empty block size</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.scheduler.MapStatusSuite</className>
          <testName>RoaringBitmap: runOptimize succeeded</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.scheduler.MapStatusSuite</className>
          <testName>RoaringBitmap: runOptimize failed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.scheduler.OutputCommitCoordinatorIntegrationSuite.xml</file>
      <name>org.apache.spark.scheduler.OutputCommitCoordinatorIntegrationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.231</duration>
      <cases>
        <case>
          <duration>0.231</duration>
          <className>org.apache.spark.scheduler.OutputCommitCoordinatorIntegrationSuite</className>
          <testName>commitTask()</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.scheduler.OutputCommitCoordinatorSuite.xml</file>
      <name>org.apache.spark.scheduler.OutputCommitCoordinatorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>5.136</duration>
      <cases>
        <case>
          <duration>0.071</duration>
          <className>org.apache.spark.scheduler.OutputCommitCoordinatorSuite</className>
          <testName>Only one of two duplicate commit tasks should commit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.054</duration>
          <className>org.apache.spark.scheduler.OutputCommitCoordinatorSuite</className>
          <testName>If commit fails, if task is retried it should not be locked, and will succeed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>5.005</duration>
          <className>org.apache.spark.scheduler.OutputCommitCoordinatorSuite</className>
          <testName>Job should not complete if all commits are denied</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.scheduler.OutputCommitCoordinatorSuite</className>
          <testName>Only authorized committer failures can clear the authorized committer lock (SPARK-6614)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.scheduler.PoolSuite.xml</file>
      <name>org.apache.spark.scheduler.PoolSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.147</duration>
      <cases>
        <case>
          <duration>0.046</duration>
          <className>org.apache.spark.scheduler.PoolSuite</className>
          <testName>FIFO Scheduler Test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.058</duration>
          <className>org.apache.spark.scheduler.PoolSuite</className>
          <testName>Fair Scheduler Test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.scheduler.PoolSuite</className>
          <testName>Nested Pool Test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.scheduler.ReplayListenerSuite.xml</file>
      <name>org.apache.spark.scheduler.ReplayListenerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>22.513</duration>
      <cases>
        <case>
          <duration>0.13</duration>
          <className>org.apache.spark.scheduler.ReplayListenerSuite</className>
          <testName>Simple replay</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>5.572</duration>
          <className>org.apache.spark.scheduler.ReplayListenerSuite</className>
          <testName>End-to-end replay</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>16.811</duration>
          <className>org.apache.spark.scheduler.ReplayListenerSuite</className>
          <testName>End-to-end replay with compression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.scheduler.SparkListenerSuite.xml</file>
      <name>org.apache.spark.scheduler.SparkListenerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.098</duration>
      <cases>
        <case>
          <duration>0.059</duration>
          <className>org.apache.spark.scheduler.SparkListenerSuite</className>
          <testName>stop in listener</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.06</duration>
          <className>org.apache.spark.scheduler.SparkListenerSuite</className>
          <testName>basic creation and shutdown of LiveListenerBus</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.062</duration>
          <className>org.apache.spark.scheduler.SparkListenerSuite</className>
          <testName>stop() waits for the event queue to completely drain</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.06</duration>
          <className>org.apache.spark.scheduler.SparkListenerSuite</className>
          <testName>basic creation of StageInfo</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.133</duration>
          <className>org.apache.spark.scheduler.SparkListenerSuite</className>
          <testName>basic creation of StageInfo with shuffle</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.077</duration>
          <className>org.apache.spark.scheduler.SparkListenerSuite</className>
          <testName>StageInfo with fewer tasks than partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.169</duration>
          <className>org.apache.spark.scheduler.SparkListenerSuite</className>
          <testName>local metrics</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.255</duration>
          <className>org.apache.spark.scheduler.SparkListenerSuite</className>
          <testName>onTaskGettingResult() called when result fetched remotely</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.054</duration>
          <className>org.apache.spark.scheduler.SparkListenerSuite</className>
          <testName>onTaskGettingResult() not called when result sent directly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.05</duration>
          <className>org.apache.spark.scheduler.SparkListenerSuite</className>
          <testName>onTaskEnd() should be called for all started tasks, even after job has been killed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.057</duration>
          <className>org.apache.spark.scheduler.SparkListenerSuite</className>
          <testName>SparkListener moves on if a listener throws an exception</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.062</duration>
          <className>org.apache.spark.scheduler.SparkListenerSuite</className>
          <testName>extraListeners</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.scheduler.SparkListenerWithClusterSuite.xml</file>
      <name>org.apache.spark.scheduler.SparkListenerWithClusterSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>3.008</duration>
      <cases>
        <case>
          <duration>3.008</duration>
          <className>org.apache.spark.scheduler.SparkListenerWithClusterSuite</className>
          <testName>SparkListener sends executor added message</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.scheduler.TaskContextSuite.xml</file>
      <name>org.apache.spark.scheduler.TaskContextSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.529</duration>
      <cases>
        <case>
          <duration>0.08</duration>
          <className>org.apache.spark.scheduler.TaskContextSuite</className>
          <testName>provide metrics sources</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.058</duration>
          <className>org.apache.spark.scheduler.TaskContextSuite</className>
          <testName>calls TaskCompletionListener after failure</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.039</duration>
          <className>org.apache.spark.scheduler.TaskContextSuite</className>
          <testName>calls TaskFailureListeners after failure</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.scheduler.TaskContextSuite</className>
          <testName>all TaskCompletionListeners should be called even if some fail</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.scheduler.TaskContextSuite</className>
          <testName>all TaskFailureListeners should be called even if some fail</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.068</duration>
          <className>org.apache.spark.scheduler.TaskContextSuite</className>
          <testName>attemptNumber should return attempt number, not task id (SPARK-4014)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.116</duration>
          <className>org.apache.spark.scheduler.TaskContextSuite</className>
          <testName>accumulators are updated on exception failures</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.scheduler.TaskContextSuite</className>
          <testName>failed tasks collect only accumulators whose values count during failures</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.scheduler.TaskContextSuite</className>
          <testName>only updated internal accumulators will be sent back to driver</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.078</duration>
          <className>org.apache.spark.scheduler.TaskContextSuite</className>
          <testName>localProperties are propagated to executors correctly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.scheduler.TaskResultGetterSuite.xml</file>
      <name>org.apache.spark.scheduler.TaskResultGetterSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.942</duration>
      <cases>
        <case>
          <duration>0.057</duration>
          <className>org.apache.spark.scheduler.TaskResultGetterSuite</className>
          <testName>handling results smaller than max RPC message size</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.245</duration>
          <className>org.apache.spark.scheduler.TaskResultGetterSuite</className>
          <testName>handling results larger than max RPC message size</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.309</duration>
          <className>org.apache.spark.scheduler.TaskResultGetterSuite</className>
          <testName>task retried if result missing from block manager</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.269</duration>
          <className>org.apache.spark.scheduler.TaskResultGetterSuite</className>
          <testName>failed task deserialized with the correct classloader (SPARK-11195)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.062</duration>
          <className>org.apache.spark.scheduler.TaskResultGetterSuite</className>
          <testName>task result size is set on the driver, not the executors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.scheduler.TaskSchedulerImplSuite.xml</file>
      <name>org.apache.spark.scheduler.TaskSchedulerImplSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.5550003</duration>
      <cases>
        <case>
          <duration>2.135</duration>
          <className>org.apache.spark.scheduler.TaskSchedulerImplSuite</className>
          <testName>Scheduler does not always schedule tasks on the same workers</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.053</duration>
          <className>org.apache.spark.scheduler.TaskSchedulerImplSuite</className>
          <testName>Scheduler correctly accounts for multiple CPUs per task</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.044</duration>
          <className>org.apache.spark.scheduler.TaskSchedulerImplSuite</className>
          <testName>Scheduler does not crash when tasks are not serializable</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.038</duration>
          <className>org.apache.spark.scheduler.TaskSchedulerImplSuite</className>
          <testName>refuse to schedule concurrent attempts for the same stage (SPARK-8103)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.scheduler.TaskSchedulerImplSuite</className>
          <testName>don&apos;t schedule more tasks after a taskset is zombie</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.049</duration>
          <className>org.apache.spark.scheduler.TaskSchedulerImplSuite</className>
          <testName>if a zombie attempt finishes, continue scheduling tasks for non-zombie attempts</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.039</duration>
          <className>org.apache.spark.scheduler.TaskSchedulerImplSuite</className>
          <testName>tasks are not re-scheduled while executor loss reason is pending</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.scheduler.TaskSchedulerImplSuite</className>
          <testName>abort stage if executor loss results in unschedulability from previously failed tasks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.scheduler.TaskSchedulerImplSuite</className>
          <testName>don&apos;t abort if there is an executor available, though it hasn&apos;t had scheduled tasks yet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.058</duration>
          <className>org.apache.spark.scheduler.TaskSchedulerImplSuite</className>
          <testName>SPARK-16106 locality levels updated if executor added to existing host</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.scheduler.TaskSetBlacklistSuite.xml</file>
      <name>org.apache.spark.scheduler.TaskSetBlacklistSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.008</duration>
      <cases>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.scheduler.TaskSetBlacklistSuite</className>
          <testName>Blacklisting tasks, executors, and nodes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.scheduler.TaskSetBlacklistSuite</className>
          <testName>multiple attempts for the same task count once</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.scheduler.TaskSetBlacklistSuite</className>
          <testName>only blacklist nodes for the task set when all the blacklisted executors are all on same host</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.scheduler.TaskSetManagerSuite.xml</file>
      <name>org.apache.spark.scheduler.TaskSetManagerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>3.4040003</duration>
      <cases>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.scheduler.TaskSetManagerSuite</className>
          <testName>TaskSet with no preferences</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.scheduler.TaskSetManagerSuite</className>
          <testName>multiple offers with no preferences</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.scheduler.TaskSetManagerSuite</className>
          <testName>skip unsatisfiable locality levels</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.049</duration>
          <className>org.apache.spark.scheduler.TaskSetManagerSuite</className>
          <testName>basic delay scheduling</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.04</duration>
          <className>org.apache.spark.scheduler.TaskSetManagerSuite</className>
          <testName>we do not need to delay scheduling when we only have noPref tasks in the queue</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.045</duration>
          <className>org.apache.spark.scheduler.TaskSetManagerSuite</className>
          <testName>delay scheduling with fallback</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.044</duration>
          <className>org.apache.spark.scheduler.TaskSetManagerSuite</className>
          <testName>delay scheduling with failed hosts</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.scheduler.TaskSetManagerSuite</className>
          <testName>task result lost</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.scheduler.TaskSetManagerSuite</className>
          <testName>repeated failures lead to task set abortion</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.044</duration>
          <className>org.apache.spark.scheduler.TaskSetManagerSuite</className>
          <testName>executors should be blacklisted after task failure, in spite of locality preferences</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.041</duration>
          <className>org.apache.spark.scheduler.TaskSetManagerSuite</className>
          <testName>new executors get added and lost</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.scheduler.TaskSetManagerSuite</className>
          <testName>Executors exit for reason unrelated to currently running tasks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.scheduler.TaskSetManagerSuite</className>
          <testName>test RACK_LOCAL tasks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.scheduler.TaskSetManagerSuite</className>
          <testName>do not emit warning when serialized task is small</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.074</duration>
          <className>org.apache.spark.scheduler.TaskSetManagerSuite</className>
          <testName>emit warning when serialized task is large</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.scheduler.TaskSetManagerSuite</className>
          <testName>Not serializable exception thrown if the task cannot be serialized</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.119</duration>
          <className>org.apache.spark.scheduler.TaskSetManagerSuite</className>
          <testName>abort the job if total size of results is too large</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.scheduler.TaskSetManagerSuite</className>
          <testName>speculative and noPref task should be scheduled after node-local</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.scheduler.TaskSetManagerSuite</className>
          <testName>node-local tasks should be scheduled right away when there are only node-local and no-preference tasks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.scheduler.TaskSetManagerSuite</className>
          <testName>SPARK-4939: node-local tasks should be scheduled right after process-local tasks finished</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.22</duration>
          <className>org.apache.spark.scheduler.TaskSetManagerSuite</className>
          <testName>SPARK-4939: no-pref tasks should be scheduled after process-local tasks finished</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.039</duration>
          <className>org.apache.spark.scheduler.TaskSetManagerSuite</className>
          <testName>Ensure TaskSetManager is usable after addition of levels</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.038</duration>
          <className>org.apache.spark.scheduler.TaskSetManagerSuite</className>
          <testName>Test that locations with HDFSCacheTaskLocation are treated as PROCESS_LOCAL</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.scheduler.TaskSetManagerSuite</className>
          <testName>Test TaskLocation for different host type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.051</duration>
          <className>org.apache.spark.scheduler.TaskSetManagerSuite</className>
          <testName>Kill other task attempts when one attempt belonging to the same task succeeds</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.051</duration>
          <className>org.apache.spark.scheduler.TaskSetManagerSuite</className>
          <testName>Killing speculative tasks does not count towards aborting the taskset</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.041</duration>
          <className>org.apache.spark.scheduler.TaskSetManagerSuite</className>
          <testName>SPARK-17894: Verify TaskSetManagers for different stage attempts have unique names</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.security.CryptoStreamUtilsSuite.xml</file>
      <name>org.apache.spark.security.CryptoStreamUtilsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.010000001</duration>
      <cases>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.security.CryptoStreamUtilsSuite</className>
          <testName>Crypto configuration conversion</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.security.CryptoStreamUtilsSuite</className>
          <testName>Shuffle encryption is disabled by default</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.security.CryptoStreamUtilsSuite</className>
          <testName>Shuffle encryption key length should be 128 by default</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.security.CryptoStreamUtilsSuite</className>
          <testName>Initial credentials with key length in 256</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.security.CryptoStreamUtilsSuite</className>
          <testName>Initial credentials with invalid key length</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.serializer.GenericAvroSerializerSuite.xml</file>
      <name>org.apache.spark.serializer.GenericAvroSerializerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.073</duration>
      <cases>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.serializer.GenericAvroSerializerSuite</className>
          <testName>schema compression and decompression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.serializer.GenericAvroSerializerSuite</className>
          <testName>record serialization and deserialization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.serializer.GenericAvroSerializerSuite</className>
          <testName>uses schema fingerprint to decrease message size</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.serializer.GenericAvroSerializerSuite</className>
          <testName>caches previously seen schemas</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.serializer.JavaSerializerSuite.xml</file>
      <name>org.apache.spark.serializer.JavaSerializerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.006</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.serializer.JavaSerializerSuite</className>
          <testName>JavaSerializer instances are serializable</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.serializer.JavaSerializerSuite</className>
          <testName>Deserialize object containing a primitive Class as attribute</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.serializer.KryoBenchmark.xml</file>
      <name>org.apache.spark.serializer.KryoBenchmark</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>-0.001</duration>
      <cases>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.serializer.KryoBenchmark</className>
          <testName>Benchmark Kryo Unsafe vs safe Serialization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.serializer.KryoSerializerAutoResetDisabledSuite.xml</file>
      <name>org.apache.spark.serializer.KryoSerializerAutoResetDisabledSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.052</duration>
      <cases>
        <case>
          <duration>0.05</duration>
          <className>org.apache.spark.serializer.KryoSerializerAutoResetDisabledSuite</className>
          <testName>sort-shuffle with bypassMergeSort (SPARK-7873)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.serializer.KryoSerializerAutoResetDisabledSuite</className>
          <testName>calling deserialize() after deserializeStream()</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.serializer.KryoSerializerDistributedSuite.xml</file>
      <name>org.apache.spark.serializer.KryoSerializerDistributedSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>4.004</duration>
      <cases>
        <case>
          <duration>4.004</duration>
          <className>org.apache.spark.serializer.KryoSerializerDistributedSuite</className>
          <testName>kryo objects are serialised consistently in different processes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.serializer.KryoSerializerResizableOutputSuite.xml</file>
      <name>org.apache.spark.serializer.KryoSerializerResizableOutputSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.43400002</duration>
      <cases>
        <case>
          <duration>0.208</duration>
          <className>org.apache.spark.serializer.KryoSerializerResizableOutputSuite</className>
          <testName>kryo without resizable output buffer should fail on large array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.226</duration>
          <className>org.apache.spark.serializer.KryoSerializerResizableOutputSuite</className>
          <testName>kryo with resizable output buffer should succeed on large array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.serializer.KryoSerializerSuite.xml</file>
      <name>org.apache.spark.serializer.KryoSerializerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.29399997</duration>
      <cases>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.serializer.KryoSerializerSuite</className>
          <testName>SPARK-7392 configuration limits</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.serializer.KryoSerializerSuite</className>
          <testName>basic types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.serializer.KryoSerializerSuite</className>
          <testName>pairs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.serializer.KryoSerializerSuite</className>
          <testName>Scala data structures</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.serializer.KryoSerializerSuite</className>
          <testName>Bug: SPARK-10251</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.serializer.KryoSerializerSuite</className>
          <testName>ranges</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.serializer.KryoSerializerSuite</className>
          <testName>asJavaIterable</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.serializer.KryoSerializerSuite</className>
          <testName>custom registrator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.serializer.KryoSerializerSuite</className>
          <testName>kryo with collect</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.serializer.KryoSerializerSuite</className>
          <testName>kryo with parallelize</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.serializer.KryoSerializerSuite</className>
          <testName>kryo with parallelize for specialized tuples</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.04</duration>
          <className>org.apache.spark.serializer.KryoSerializerSuite</className>
          <testName>kryo with parallelize for primitive arrays</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.serializer.KryoSerializerSuite</className>
          <testName>kryo with collect for specialized tuples</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.serializer.KryoSerializerSuite</className>
          <testName>kryo with SerializableHyperLogLog</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.serializer.KryoSerializerSuite</className>
          <testName>kryo with reduce</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.018</duration>
          <className>org.apache.spark.serializer.KryoSerializerSuite</className>
          <testName>kryo with fold</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.serializer.KryoSerializerSuite</className>
          <testName>kryo with nonexistent custom registrator should fail</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.serializer.KryoSerializerSuite</className>
          <testName>default class loader can be set by a different thread</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.serializer.KryoSerializerSuite</className>
          <testName>registration of HighlyCompressedMapStatus</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.serializer.KryoSerializerSuite</className>
          <testName>serialization buffer overflow reporting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.serializer.KryoSerializerSuite</className>
          <testName>SPARK-12222: deserialize RoaringBitmap throw Buffer underflow exception</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.serializer.KryoSerializerSuite</className>
          <testName>readObject</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.serializer.KryoSerializerSuite</className>
          <testName>getAutoReset</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.serializer.KryoSerializerSuite</className>
          <testName>instance reuse with autoReset = true, referenceTracking = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.serializer.KryoSerializerSuite</className>
          <testName>instance reuse with autoReset = false, referenceTracking = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.serializer.KryoSerializerSuite</className>
          <testName>instance reuse with autoReset = true, referenceTracking = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.serializer.KryoSerializerSuite</className>
          <testName>instance reuse with autoReset = false, referenceTracking = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.serializer.ProactiveClosureSerializationSuite.xml</file>
      <name>org.apache.spark.serializer.ProactiveClosureSerializationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.027000003</duration>
      <cases>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.serializer.ProactiveClosureSerializationSuite</className>
          <testName>throws expected serialization exceptions on actions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.serializer.ProactiveClosureSerializationSuite</className>
          <testName>mapPartitions transformations throw proactive serialization exceptions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.serializer.ProactiveClosureSerializationSuite</className>
          <testName>map transformations throw proactive serialization exceptions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.serializer.ProactiveClosureSerializationSuite</className>
          <testName>filter transformations throw proactive serialization exceptions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.serializer.ProactiveClosureSerializationSuite</className>
          <testName>flatMap transformations throw proactive serialization exceptions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.serializer.ProactiveClosureSerializationSuite</className>
          <testName>mapPartitionsWithIndex transformations throw proactive serialization exceptions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.serializer.SerializationDebuggerSuite.xml</file>
      <name>org.apache.spark.serializer.SerializationDebuggerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.023000002</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.serializer.SerializationDebuggerSuite</className>
          <testName>primitives, strings, and nulls</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.serializer.SerializationDebuggerSuite</className>
          <testName>primitive arrays</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.serializer.SerializationDebuggerSuite</className>
          <testName>non-primitive arrays</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.serializer.SerializationDebuggerSuite</className>
          <testName>serializable object</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.serializer.SerializationDebuggerSuite</className>
          <testName>nested arrays</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.serializer.SerializationDebuggerSuite</className>
          <testName>nested objects</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.serializer.SerializationDebuggerSuite</className>
          <testName>cycles (should not loop forever)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.serializer.SerializationDebuggerSuite</className>
          <testName>root object not serializable</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.serializer.SerializationDebuggerSuite</className>
          <testName>array containing not serializable element</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.serializer.SerializationDebuggerSuite</className>
          <testName>object containing not serializable field</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.serializer.SerializationDebuggerSuite</className>
          <testName>externalizable class writing out not serializable object</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.serializer.SerializationDebuggerSuite</className>
          <testName>externalizable class writing out serializable objects</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.serializer.SerializationDebuggerSuite</className>
          <testName>object containing writeReplace() which returns not serializable object</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.serializer.SerializationDebuggerSuite</className>
          <testName>object containing writeReplace() which returns serializable object</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.serializer.SerializationDebuggerSuite</className>
          <testName>no infinite loop with writeReplace() which returns class of its own type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.serializer.SerializationDebuggerSuite</className>
          <testName>object containing writeObject() and not serializable field</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.serializer.SerializationDebuggerSuite</className>
          <testName>object containing writeObject() and serializable field</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.serializer.SerializationDebuggerSuite</className>
          <testName>object of serializable subclass with more fields than superclass (SPARK-7180)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.serializer.SerializationDebuggerSuite</className>
          <testName>crazy nested objects</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.serializer.SerializationDebuggerSuite</className>
          <testName>improveException</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.serializer.SerializationDebuggerSuite</className>
          <testName>improveException with error in debugger</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.serializer.SerializerPropertiesSuite.xml</file>
      <name>org.apache.spark.serializer.SerializerPropertiesSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.028</duration>
      <cases>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.serializer.SerializerPropertiesSuite</className>
          <testName>JavaSerializer does not support relocation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.serializer.SerializerPropertiesSuite</className>
          <testName>KryoSerializer supports relocation when auto-reset is enabled</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.serializer.SerializerPropertiesSuite</className>
          <testName>KryoSerializer does not support relocation when auto-reset is disabled</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.serializer.UnsafeKryoSerializerSuite.xml</file>
      <name>org.apache.spark.serializer.UnsafeKryoSerializerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.45300004</duration>
      <cases>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.serializer.UnsafeKryoSerializerSuite</className>
          <testName>SPARK-7392 configuration limits</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.serializer.UnsafeKryoSerializerSuite</className>
          <testName>basic types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.serializer.UnsafeKryoSerializerSuite</className>
          <testName>pairs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.serializer.UnsafeKryoSerializerSuite</className>
          <testName>Scala data structures</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.serializer.UnsafeKryoSerializerSuite</className>
          <testName>Bug: SPARK-10251</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.serializer.UnsafeKryoSerializerSuite</className>
          <testName>ranges</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.serializer.UnsafeKryoSerializerSuite</className>
          <testName>asJavaIterable</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.serializer.UnsafeKryoSerializerSuite</className>
          <testName>custom registrator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.039</duration>
          <className>org.apache.spark.serializer.UnsafeKryoSerializerSuite</className>
          <testName>kryo with collect</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.033</duration>
          <className>org.apache.spark.serializer.UnsafeKryoSerializerSuite</className>
          <testName>kryo with parallelize</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.serializer.UnsafeKryoSerializerSuite</className>
          <testName>kryo with parallelize for specialized tuples</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.serializer.UnsafeKryoSerializerSuite</className>
          <testName>kryo with parallelize for primitive arrays</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.serializer.UnsafeKryoSerializerSuite</className>
          <testName>kryo with collect for specialized tuples</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.081</duration>
          <className>org.apache.spark.serializer.UnsafeKryoSerializerSuite</className>
          <testName>kryo with SerializableHyperLogLog</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.serializer.UnsafeKryoSerializerSuite</className>
          <testName>kryo with reduce</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.serializer.UnsafeKryoSerializerSuite</className>
          <testName>kryo with fold</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.serializer.UnsafeKryoSerializerSuite</className>
          <testName>kryo with nonexistent custom registrator should fail</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.serializer.UnsafeKryoSerializerSuite</className>
          <testName>default class loader can be set by a different thread</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.serializer.UnsafeKryoSerializerSuite</className>
          <testName>registration of HighlyCompressedMapStatus</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.055</duration>
          <className>org.apache.spark.serializer.UnsafeKryoSerializerSuite</className>
          <testName>serialization buffer overflow reporting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.serializer.UnsafeKryoSerializerSuite</className>
          <testName>SPARK-12222: deserialize RoaringBitmap throw Buffer underflow exception</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.serializer.UnsafeKryoSerializerSuite</className>
          <testName>readObject</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.serializer.UnsafeKryoSerializerSuite</className>
          <testName>getAutoReset</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.serializer.UnsafeKryoSerializerSuite</className>
          <testName>instance reuse with autoReset = true, referenceTracking = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.serializer.UnsafeKryoSerializerSuite</className>
          <testName>instance reuse with autoReset = false, referenceTracking = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.serializer.UnsafeKryoSerializerSuite</className>
          <testName>instance reuse with autoReset = true, referenceTracking = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.serializer.UnsafeKryoSerializerSuite</className>
          <testName>instance reuse with autoReset = false, referenceTracking = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.shuffle.BlockStoreShuffleReaderSuite.xml</file>
      <name>org.apache.spark.shuffle.BlockStoreShuffleReaderSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.067</duration>
      <cases>
        <case>
          <duration>0.067</duration>
          <className>org.apache.spark.shuffle.BlockStoreShuffleReaderSuite</className>
          <testName>read() releases resources on completion</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.shuffle.ShuffleDependencySuite.xml</file>
      <name>org.apache.spark.shuffle.ShuffleDependencySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.218</duration>
      <cases>
        <case>
          <duration>0.063</duration>
          <className>org.apache.spark.shuffle.ShuffleDependencySuite</className>
          <testName>key, value, and combiner classes correct in shuffle dependency without aggregation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.063</duration>
          <className>org.apache.spark.shuffle.ShuffleDependencySuite</className>
          <testName>key, value, and combiner classes available in shuffle dependency with aggregation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.092</duration>
          <className>org.apache.spark.shuffle.ShuffleDependencySuite</className>
          <testName>combineByKey null combiner class tag handled correctly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriterSuite.xml</file>
      <name>org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriterSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.355</duration>
      <cases>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriterSuite</className>
          <testName>write empty iterator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.207</duration>
          <className>org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriterSuite</className>
          <testName>write with some empty partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.033</duration>
          <className>org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriterSuite</className>
          <testName>only generate temp shuffle file for non-empty partition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.107</duration>
          <className>org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriterSuite</className>
          <testName>cleanup of intermediate files after errors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.shuffle.sort.IndexShuffleBlockResolverSuite.xml</file>
      <name>org.apache.spark.shuffle.sort.IndexShuffleBlockResolverSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.007</duration>
      <cases>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.shuffle.sort.IndexShuffleBlockResolverSuite</className>
          <testName>commit shuffle files multiple times</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.shuffle.sort.PackedRecordPointerSuite.xml</file>
      <name>org.apache.spark.shuffle.sort.PackedRecordPointerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.004</duration>
      <cases>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.shuffle.sort.PackedRecordPointerSuite</className>
          <testName>offHeap</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.shuffle.sort.PackedRecordPointerSuite</className>
          <testName>maximumPartitionIdCanBeEncoded</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.shuffle.sort.PackedRecordPointerSuite</className>
          <testName>partitionIdsGreaterThanMaximumPartitionIdWillOverflowOrTriggerError</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.shuffle.sort.PackedRecordPointerSuite</className>
          <testName>maximumOffsetInPageCanBeEncoded</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.shuffle.sort.PackedRecordPointerSuite</className>
          <testName>heap</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.shuffle.sort.PackedRecordPointerSuite</className>
          <testName>offsetsPastMaxOffsetInPageWillOverflow</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.shuffle.sort.ShuffleInMemoryRadixSorterSuite.xml</file>
      <name>org.apache.spark.shuffle.sort.ShuffleInMemoryRadixSorterSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.04</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.shuffle.sort.ShuffleInMemoryRadixSorterSuite</className>
          <testName>testSortingEmptyInput</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.shuffle.sort.ShuffleInMemoryRadixSorterSuite</className>
          <testName>testBasicSorting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.038</duration>
          <className>org.apache.spark.shuffle.sort.ShuffleInMemoryRadixSorterSuite</className>
          <testName>testSortingManyNumbers</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.shuffle.sort.ShuffleInMemorySorterSuite.xml</file>
      <name>org.apache.spark.shuffle.sort.ShuffleInMemorySorterSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.109000005</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.shuffle.sort.ShuffleInMemorySorterSuite</className>
          <testName>testSortingEmptyInput</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.shuffle.sort.ShuffleInMemorySorterSuite</className>
          <testName>testBasicSorting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.108</duration>
          <className>org.apache.spark.shuffle.sort.ShuffleInMemorySorterSuite</className>
          <testName>testSortingManyNumbers</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.shuffle.sort.SortShuffleManagerSuite.xml</file>
      <name>org.apache.spark.shuffle.sort.SortShuffleManagerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.026999999</duration>
      <cases>
        <case>
          <duration>0.018</duration>
          <className>org.apache.spark.shuffle.sort.SortShuffleManagerSuite</className>
          <testName>supported shuffle dependencies for serialized shuffle</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.shuffle.sort.SortShuffleManagerSuite</className>
          <testName>unsupported shuffle dependencies for serialized shuffle</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite.xml</file>
      <name>org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.861</duration>
      <cases>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite</className>
          <testName>mergeSpillsWithTransferToAndNoCompression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite</className>
          <testName>mergeSpillsWithFileStreamAndNoCompression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite</className>
          <testName>spillFilesAreDeletedWhenStoppingAfterError</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite</className>
          <testName>doNotNeedToCallWriteBeforeUnsuccessfulStop</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite</className>
          <testName>testPeakMemoryUsed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite</className>
          <testName>writeWithoutSpilling</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite</className>
          <testName>mergeSpillsWithFileStreamAndSnappy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.033</duration>
          <className>org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite</className>
          <testName>writeEnoughRecordsToTriggerSortBufferExpansionAndSpillRadixOff</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.063</duration>
          <className>org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite</className>
          <testName>writeRecordsThatAreBiggerThanMaxRecordSize</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite</className>
          <testName>writeEnoughRecordsToTriggerSortBufferExpansionAndSpillRadixOn</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite</className>
          <testName>mergeSpillsWithTransferToAndSnappy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite</className>
          <testName>writeFailurePropagates</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite</className>
          <testName>mergeSpillsWithFileStreamAndLZ4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite</className>
          <testName>mergeSpillsWithFileStreamAndLZF</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.084</duration>
          <className>org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite</className>
          <testName>writeRecordsThatAreBiggerThanDiskWriteBufferSize</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite</className>
          <testName>mergeSpillsWithTransferToAndLZ4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite</className>
          <testName>mergeSpillsWithTransferToAndLZF</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.506</duration>
          <className>org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite</className>
          <testName>writeEnoughDataToTriggerSpill</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite</className>
          <testName>writeEmptyIterator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite</className>
          <testName>mustCallWriteBeforeSuccessfulStop</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.status.api.v1.AllStagesResourceSuite.xml</file>
      <name>org.apache.spark.status.api.v1.AllStagesResourceSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.006</duration>
      <cases>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.status.api.v1.AllStagesResourceSuite</className>
          <testName>firstTaskLaunchedTime when there are no tasks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.status.api.v1.AllStagesResourceSuite</className>
          <testName>firstTaskLaunchedTime when there are tasks but none launched</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.status.api.v1.AllStagesResourceSuite</className>
          <testName>firstTaskLaunchedTime when there are tasks and some launched</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.status.api.v1.SimpleDateParamSuite.xml</file>
      <name>org.apache.spark.status.api.v1.SimpleDateParamSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.001</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.status.api.v1.SimpleDateParamSuite</className>
          <testName>date parsing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.storage.BlockIdSuite.xml</file>
      <name>org.apache.spark.storage.BlockIdSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.003</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.storage.BlockIdSuite</className>
          <testName>test-bad-deserialization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.storage.BlockIdSuite</className>
          <testName>rdd</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.storage.BlockIdSuite</className>
          <testName>shuffle</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.storage.BlockIdSuite</className>
          <testName>broadcast</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.storage.BlockIdSuite</className>
          <testName>taskresult</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.storage.BlockIdSuite</className>
          <testName>stream</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.storage.BlockIdSuite</className>
          <testName>test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.storage.BlockInfoManagerSuite.xml</file>
      <name>org.apache.spark.storage.BlockInfoManagerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.538</duration>
      <cases>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.storage.BlockInfoManagerSuite</className>
          <testName>initial memory usage</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.storage.BlockInfoManagerSuite</className>
          <testName>get non-existent block</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.storage.BlockInfoManagerSuite</className>
          <testName>basic lockNewBlockForWriting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.303</duration>
          <className>org.apache.spark.storage.BlockInfoManagerSuite</className>
          <testName>lockNewBlockForWriting blocks while write lock is held, then returns false after release</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.306</duration>
          <className>org.apache.spark.storage.BlockInfoManagerSuite</className>
          <testName>lockNewBlockForWriting blocks while write lock is held, then returns true after removal</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.storage.BlockInfoManagerSuite</className>
          <testName>read locks are reentrant</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.storage.BlockInfoManagerSuite</className>
          <testName>multiple tasks can hold read locks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.storage.BlockInfoManagerSuite</className>
          <testName>single task can hold write lock</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.storage.BlockInfoManagerSuite</className>
          <testName>cannot grab a writer lock while already holding a write lock</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.storage.BlockInfoManagerSuite</className>
          <testName>assertBlockIsLockedForWriting throws exception if block is not locked</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.storage.BlockInfoManagerSuite</className>
          <testName>downgrade lock</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.303</duration>
          <className>org.apache.spark.storage.BlockInfoManagerSuite</className>
          <testName>write lock will block readers</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.304</duration>
          <className>org.apache.spark.storage.BlockInfoManagerSuite</className>
          <testName>read locks will block writer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.storage.BlockInfoManagerSuite</className>
          <testName>removing a non-existent block throws IllegalArgumentException</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.storage.BlockInfoManagerSuite</className>
          <testName>removing a block without holding any locks throws IllegalStateException</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.storage.BlockInfoManagerSuite</className>
          <testName>removing a block while holding only a read lock throws IllegalStateException</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.303</duration>
          <className>org.apache.spark.storage.BlockInfoManagerSuite</className>
          <testName>removing a block causes blocked callers to receive None</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.storage.BlockInfoManagerSuite</className>
          <testName>releaseAllLocksForTask releases write locks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.storage.BlockManagerReplicationSuite.xml</file>
      <name>org.apache.spark.storage.BlockManagerReplicationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.8040001</duration>
      <cases>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.storage.BlockManagerReplicationSuite</className>
          <testName>get peers with addition and removal of block managers</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.205</duration>
          <className>org.apache.spark.storage.BlockManagerReplicationSuite</className>
          <testName>block replication - 2x replication</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.433</duration>
          <className>org.apache.spark.storage.BlockManagerReplicationSuite</className>
          <testName>block replication - 3x replication</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.613</duration>
          <className>org.apache.spark.storage.BlockManagerReplicationSuite</className>
          <testName>block replication - mixed between 1x to 5x</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.108</duration>
          <className>org.apache.spark.storage.BlockManagerReplicationSuite</className>
          <testName>block replication - off-heap</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.storage.BlockManagerReplicationSuite</className>
          <testName>block replication - 2x replication without peers</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.177</duration>
          <className>org.apache.spark.storage.BlockManagerReplicationSuite</className>
          <testName>block replication - deterministic node selection</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.079</duration>
          <className>org.apache.spark.storage.BlockManagerReplicationSuite</className>
          <testName>block replication - replication failures</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.164</duration>
          <className>org.apache.spark.storage.BlockManagerReplicationSuite</className>
          <testName>block replication - addition and deletion of block managers</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.storage.BlockManagerSuite.xml</file>
      <name>org.apache.spark.storage.BlockManagerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.0450002</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>StorageLevel object caching</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>BlockManagerId object caching</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>isDriver() backwards-compatibility with legacy driver ids (SPARK-6716)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>master + 1 manager interaction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.067</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>master + 2 managers interaction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>removing block</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>removing rdd</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.045</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>removing broadcast</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>reregistration on heart beat</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>reregistration on block update</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.401</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>reregistration doesn&apos;t dead lock</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>correct BlockResult returned from get() calls</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>optimize a location order of blocks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.041</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>SPARK-9591: getRemoteBytes from another location when Exception throw</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.05</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>SPARK-14252: getOrElseUpdate should still read from remote storage</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>in-memory LRU storage</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>in-memory LRU storage with serialization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.018</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>in-memory LRU storage with off-heap</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>in-memory LRU for partitions of same RDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>in-memory LRU for partitions of multiple RDDs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>on-disk storage</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>disk and memory storage</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>disk and memory storage with getLocalBytes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>disk and memory storage with serialization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>disk and memory storage with serialization and getLocalBytes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>disk and off-heap memory storage</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>disk and off-heap memory storage with getLocalBytes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>LRU with mixed storage levels</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>in-memory LRU with streams</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>LRU with mixed storage levels and streams</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>negative byte values in ByteBufferInputStream</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>overly large block</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>block compression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>block store put failure</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>updated block statuses</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>query block statuses</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>get matching blocks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>SPARK-1194 regression: fix the same-RDD rule for cache replacement</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>safely unroll blocks through putIterator (disk)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>read-locked blocks cannot be evicted from memory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>remove block if a read fails due to missing DiskStore files (SPARK-15736)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>SPARK-13328: refresh block locations (fetch should fail after hitting a threshold)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>SPARK-13328: refresh block locations (fetch should succeed after location refresh)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>SPARK-17484: block status is properly updated following an exception in put()</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.storage.BlockManagerSuite</className>
          <testName>SPARK-17484: master block locations are updated following an invalid remote block fetch</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.storage.BlockReplicationPolicySuite.xml</file>
      <name>org.apache.spark.storage.BlockReplicationPolicySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.004</duration>
      <cases>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.storage.BlockReplicationPolicySuite</className>
          <testName>block replication - random block replication policy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.storage.BlockStatusListenerSuite.xml</file>
      <name>org.apache.spark.storage.BlockStatusListenerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.002</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.storage.BlockStatusListenerSuite</className>
          <testName>basic functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.storage.DiskBlockManagerSuite.xml</file>
      <name>org.apache.spark.storage.DiskBlockManagerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.012</duration>
      <cases>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.storage.DiskBlockManagerSuite</className>
          <testName>basic block creation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.storage.DiskBlockManagerSuite</className>
          <testName>enumerating blocks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.storage.DiskBlockObjectWriterSuite.xml</file>
      <name>org.apache.spark.storage.DiskBlockObjectWriterSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.141</duration>
      <cases>
        <case>
          <duration>0.065</duration>
          <className>org.apache.spark.storage.DiskBlockObjectWriterSuite</className>
          <testName>verify write metrics</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.storage.DiskBlockObjectWriterSuite</className>
          <testName>verify write metrics on revert</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.storage.DiskBlockObjectWriterSuite</className>
          <testName>Reopening a closed block writer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.storage.DiskBlockObjectWriterSuite</className>
          <testName>calling revertPartialWritesAndClose() on a partial write should truncate up to commit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.storage.DiskBlockObjectWriterSuite</className>
          <testName>calling revertPartialWritesAndClose() after commit() should have no effect</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.storage.DiskBlockObjectWriterSuite</className>
          <testName>calling revertPartialWritesAndClose() on a closed block writer should have no effect</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.storage.DiskBlockObjectWriterSuite</className>
          <testName>commit() and close() should be idempotent</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.storage.DiskBlockObjectWriterSuite</className>
          <testName>revertPartialWritesAndClose() should be idempotent</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.storage.DiskBlockObjectWriterSuite</className>
          <testName>commit() and close() without ever opening or writing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.storage.DiskStoreSuite.xml</file>
      <name>org.apache.spark.storage.DiskStoreSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.006</duration>
      <cases>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.storage.DiskStoreSuite</className>
          <testName>reads of memory-mapped and non memory-mapped files are equivalent</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.storage.FlatmapIteratorSuite.xml</file>
      <name>org.apache.spark.storage.FlatmapIteratorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.194</duration>
      <cases>
        <case>
          <duration>0.067</duration>
          <className>org.apache.spark.storage.FlatmapIteratorSuite</className>
          <testName>Flatmap Iterator to Disk</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.061</duration>
          <className>org.apache.spark.storage.FlatmapIteratorSuite</className>
          <testName>Flatmap Iterator to Memory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.066</duration>
          <className>org.apache.spark.storage.FlatmapIteratorSuite</className>
          <testName>Serializer Reset</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.storage.LocalDirsSuite.xml</file>
      <name>org.apache.spark.storage.LocalDirsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.001</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.storage.LocalDirsSuite</className>
          <testName>getLocalDir() returns a valid directory, even if some local dirs are missing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.storage.LocalDirsSuite</className>
          <testName>SPARK_LOCAL_DIRS override also affects driver</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.storage.MemoryStoreSuite.xml</file>
      <name>org.apache.spark.storage.MemoryStoreSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.145</duration>
      <cases>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.storage.MemoryStoreSuite</className>
          <testName>reserve/release unroll memory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.039</duration>
          <className>org.apache.spark.storage.MemoryStoreSuite</className>
          <testName>safely unroll blocks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.storage.MemoryStoreSuite</className>
          <testName>safely unroll blocks through putIteratorAsValues</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.038</duration>
          <className>org.apache.spark.storage.MemoryStoreSuite</className>
          <testName>safely unroll blocks through putIteratorAsBytes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.storage.MemoryStoreSuite</className>
          <testName>valuesIterator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.storage.MemoryStoreSuite</className>
          <testName>finishWritingToStream</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.storage.MemoryStoreSuite</className>
          <testName>multiple unrolls by the same thread</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.storage.MemoryStoreSuite</className>
          <testName>lazily create a big ByteBuffer to avoid OOM if it cannot be put into MemoryStore</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.storage.MemoryStoreSuite</className>
          <testName>put a small ByteBuffer to MemoryStore</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.storage.PartiallySerializedBlockSuite.xml</file>
      <name>org.apache.spark.storage.PartiallySerializedBlockSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.691</duration>
      <cases>
        <case>
          <duration>0.032</duration>
          <className>org.apache.spark.storage.PartiallySerializedBlockSuite</className>
          <testName>valuesIterator() and finishWritingToStream() cannot be called after discard() is called</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.storage.PartiallySerializedBlockSuite</className>
          <testName>discard() can be called more than once</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.storage.PartiallySerializedBlockSuite</className>
          <testName>cannot call valuesIterator() more than once</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.storage.PartiallySerializedBlockSuite</className>
          <testName>cannot call finishWritingToStream() more than once</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.storage.PartiallySerializedBlockSuite</className>
          <testName>cannot call finishWritingToStream() after valuesIterator()</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.storage.PartiallySerializedBlockSuite</className>
          <testName>cannot call valuesIterator() after finishWritingToStream()</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.storage.PartiallySerializedBlockSuite</className>
          <testName>buffers are deallocated in a TaskCompletionListener</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.storage.PartiallySerializedBlockSuite</className>
          <testName>basic numbers with discard() and numBuffered = 50</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.storage.PartiallySerializedBlockSuite</className>
          <testName>basic numbers with finishWritingToStream() and numBuffered = 50</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.storage.PartiallySerializedBlockSuite</className>
          <testName>basic numbers with valuesIterator() and numBuffered = 50</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.storage.PartiallySerializedBlockSuite</className>
          <testName>basic numbers with discard() and numBuffered = 0</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.storage.PartiallySerializedBlockSuite</className>
          <testName>basic numbers with finishWritingToStream() and numBuffered = 0</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.storage.PartiallySerializedBlockSuite</className>
          <testName>basic numbers with valuesIterator() and numBuffered = 0</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.storage.PartiallySerializedBlockSuite</className>
          <testName>basic numbers with discard() and numBuffered = 1000</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.storage.PartiallySerializedBlockSuite</className>
          <testName>basic numbers with finishWritingToStream() and numBuffered = 1000</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.storage.PartiallySerializedBlockSuite</className>
          <testName>basic numbers with valuesIterator() and numBuffered = 1000</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.049</duration>
          <className>org.apache.spark.storage.PartiallySerializedBlockSuite</className>
          <testName>case classes with discard() and numBuffered = 50</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.115</duration>
          <className>org.apache.spark.storage.PartiallySerializedBlockSuite</className>
          <testName>case classes with finishWritingToStream() and numBuffered = 50</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.storage.PartiallySerializedBlockSuite</className>
          <testName>case classes with valuesIterator() and numBuffered = 50</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.storage.PartiallySerializedBlockSuite</className>
          <testName>case classes with discard() and numBuffered = 0</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.072</duration>
          <className>org.apache.spark.storage.PartiallySerializedBlockSuite</className>
          <testName>case classes with finishWritingToStream() and numBuffered = 0</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.storage.PartiallySerializedBlockSuite</className>
          <testName>case classes with valuesIterator() and numBuffered = 0</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.101</duration>
          <className>org.apache.spark.storage.PartiallySerializedBlockSuite</className>
          <testName>case classes with discard() and numBuffered = 1000</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.083</duration>
          <className>org.apache.spark.storage.PartiallySerializedBlockSuite</className>
          <testName>case classes with finishWritingToStream() and numBuffered = 1000</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.071</duration>
          <className>org.apache.spark.storage.PartiallySerializedBlockSuite</className>
          <testName>case classes with valuesIterator() and numBuffered = 1000</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.storage.PartiallySerializedBlockSuite</className>
          <testName>empty iterator with discard() and numBuffered = 0</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.storage.PartiallySerializedBlockSuite</className>
          <testName>empty iterator with finishWritingToStream() and numBuffered = 0</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.storage.PartiallySerializedBlockSuite</className>
          <testName>empty iterator with valuesIterator() and numBuffered = 0</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.storage.PartiallyUnrolledIteratorSuite.xml</file>
      <name>org.apache.spark.storage.PartiallyUnrolledIteratorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.003</duration>
      <cases>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.storage.PartiallyUnrolledIteratorSuite</className>
          <testName>join two iterators</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.storage.ShuffleBlockFetcherIteratorSuite.xml</file>
      <name>org.apache.spark.storage.ShuffleBlockFetcherIteratorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.027999997</duration>
      <cases>
        <case>
          <duration>0.018</duration>
          <className>org.apache.spark.storage.ShuffleBlockFetcherIteratorSuite</className>
          <testName>successful 3 local reads + 2 remote reads</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.storage.ShuffleBlockFetcherIteratorSuite</className>
          <testName>release current unexhausted buffer in case the task completes early</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.storage.ShuffleBlockFetcherIteratorSuite</className>
          <testName>fail all blocks if any of the remote request fails</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.storage.StorageStatusListenerSuite.xml</file>
      <name>org.apache.spark.storage.StorageStatusListenerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.0050000004</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.storage.StorageStatusListenerSuite</className>
          <testName>block manager added/removed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.storage.StorageStatusListenerSuite</className>
          <testName>task end without updated blocks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.storage.StorageStatusListenerSuite</className>
          <testName>updated blocks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.storage.StorageStatusListenerSuite</className>
          <testName>unpersist RDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.storage.StorageSuite.xml</file>
      <name>org.apache.spark.storage.StorageSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.009000001</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.storage.StorageSuite</className>
          <testName>storage status add non-RDD blocks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.storage.StorageSuite</className>
          <testName>storage status update non-RDD blocks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.storage.StorageSuite</className>
          <testName>storage status remove non-RDD blocks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.storage.StorageSuite</className>
          <testName>storage status add RDD blocks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.storage.StorageSuite</className>
          <testName>storage status update RDD blocks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.storage.StorageSuite</className>
          <testName>storage status remove RDD blocks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.storage.StorageSuite</className>
          <testName>storage status containsBlock</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.storage.StorageSuite</className>
          <testName>storage status getBlock</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.storage.StorageSuite</className>
          <testName>storage status num[Rdd]Blocks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.storage.StorageSuite</className>
          <testName>storage status memUsed, diskUsed, externalBlockStoreUsed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.storage.StorageSuite</className>
          <testName>updateRddInfo</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.storage.StorageSuite</className>
          <testName>getRddBlockLocations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.storage.StorageSuite</className>
          <testName>getRddBlockLocations with multiple locations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.storage.TopologyMapperSuite.xml</file>
      <name>org.apache.spark.storage.TopologyMapperSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.006</duration>
      <cases>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.storage.TopologyMapperSuite</className>
          <testName>File based Topology Mapper</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.ui.PagedDataSourceSuite.xml</file>
      <name>org.apache.spark.ui.PagedDataSourceSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.001</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ui.PagedDataSourceSuite</className>
          <testName>basic</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.ui.PagedTableSuite.xml</file>
      <name>org.apache.spark.ui.PagedTableSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.009</duration>
      <cases>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.ui.PagedTableSuite</className>
          <testName>pageNavigation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.ui.StagePageSuite.xml</file>
      <name>org.apache.spark.ui.StagePageSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.088</duration>
      <cases>
        <case>
          <duration>0.08</duration>
          <className>org.apache.spark.ui.StagePageSuite</className>
          <testName>peak execution memory only displayed if unsafe is enabled</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.ui.StagePageSuite</className>
          <testName>SPARK-10543: peak execution memory should be per-task rather than cumulative</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.ui.UISeleniumSuite.xml</file>
      <name>org.apache.spark.ui.UISeleniumSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>10.225</duration>
      <cases>
        <case>
          <duration>0.9</duration>
          <className>org.apache.spark.ui.UISeleniumSuite</className>
          <testName>effects of unpersist() / persist() should be reflected</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.29</duration>
          <className>org.apache.spark.ui.UISeleniumSuite</className>
          <testName>failed stages should not appear to be active</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.79</duration>
          <className>org.apache.spark.ui.UISeleniumSuite</className>
          <testName>killEnabled should properly control kill button display</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.78</duration>
          <className>org.apache.spark.ui.UISeleniumSuite</className>
          <testName>jobs page should not display job group name unless some job was submitted in a job group</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.012</duration>
          <className>org.apache.spark.ui.UISeleniumSuite</className>
          <testName>job progress bars should handle stage / task failures</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.585</duration>
          <className>org.apache.spark.ui.UISeleniumSuite</className>
          <testName>job details page should display useful information for stages that haven&apos;t started</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.568</duration>
          <className>org.apache.spark.ui.UISeleniumSuite</className>
          <testName>job progress bars / cells reflect skipped stages / tasks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.065</duration>
          <className>org.apache.spark.ui.UISeleniumSuite</className>
          <testName>stages that aren&apos;t run appear as &apos;skipped stages&apos; after a job finishes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.426</duration>
          <className>org.apache.spark.ui.UISeleniumSuite</className>
          <testName>jobs with stages that are skipped should show correct link descriptions on all jobs page</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.535</duration>
          <className>org.apache.spark.ui.UISeleniumSuite</className>
          <testName>attaching and detaching a new tab</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.237</duration>
          <className>org.apache.spark.ui.UISeleniumSuite</className>
          <testName>kill stage POST/GET response is correct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.199</duration>
          <className>org.apache.spark.ui.UISeleniumSuite</className>
          <testName>kill job POST/GET response is correct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.18</duration>
          <className>org.apache.spark.ui.UISeleniumSuite</className>
          <testName>stage &amp; job retention</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.438</duration>
          <className>org.apache.spark.ui.UISeleniumSuite</className>
          <testName>live UI json application list</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.22</duration>
          <className>org.apache.spark.ui.UISeleniumSuite</className>
          <testName>job stages should have expected dotfile under DAG visualization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.ui.UISuite.xml</file>
      <name>org.apache.spark.ui.UISuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.265</duration>
      <cases>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.ui.UISuite</className>
          <testName>basic ui visibility</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.ui.UISuite</className>
          <testName>visibility at localhost:4040</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.ui.UISuite</className>
          <testName>jetty selects different port under contention</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.073</duration>
          <className>org.apache.spark.ui.UISuite</className>
          <testName>jetty with https selects different port under contention</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.ui.UISuite</className>
          <testName>jetty binds to port 0 correctly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.ui.UISuite</className>
          <testName>jetty with https binds to port 0 correctly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.056</duration>
          <className>org.apache.spark.ui.UISuite</className>
          <testName>verify appUIAddress contains the scheme</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.089</duration>
          <className>org.apache.spark.ui.UISuite</className>
          <testName>verify appUIAddress contains the port</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ui.UISuite</className>
          <testName>verify proxy rewrittenURI</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ui.UISuite</className>
          <testName>verify rewriting location header for reverse proxy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.ui.UIUtilsSuite.xml</file>
      <name>org.apache.spark.ui.UIUtilsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.021000002</duration>
      <cases>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.ui.UIUtilsSuite</className>
          <testName>makeDescription(plainText = false)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.ui.UIUtilsSuite</className>
          <testName>makeDescription(plainText = true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ui.UIUtilsSuite</className>
          <testName>SPARK-11906: Progress bar should not overflow because of speculative tasks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ui.UIUtilsSuite</className>
          <testName>)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.ui.agglogs.AggregatedLogsPageSuite.xml</file>
      <name>org.apache.spark.ui.agglogs.AggregatedLogsPageSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.005</duration>
      <cases>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.ui.agglogs.AggregatedLogsPageSuite</className>
          <testName>logsTable</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.ui.agglogs.AggregatedLogsTabSuite.xml</file>
      <name>org.apache.spark.ui.agglogs.AggregatedLogsTabSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.004</duration>
      <cases>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.ui.agglogs.AggregatedLogsTabSuite</className>
          <testName>post to aggregatedLogsListener</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.ui.jobs.JobProgressListenerSuite.xml</file>
      <name>org.apache.spark.ui.jobs.JobProgressListenerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.28</duration>
      <cases>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.ui.jobs.JobProgressListenerSuite</className>
          <testName>test LRU eviction of stages</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.ui.jobs.JobProgressListenerSuite</className>
          <testName>test clearing of stageIdToActiveJobs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.ui.jobs.JobProgressListenerSuite</className>
          <testName>test clearing of jobGroupToJobIds</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.25</duration>
          <className>org.apache.spark.ui.jobs.JobProgressListenerSuite</className>
          <testName>test LRU eviction of jobs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.ui.jobs.JobProgressListenerSuite</className>
          <testName>test executor id to summary</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ui.jobs.JobProgressListenerSuite</className>
          <testName>test task success vs failure counting for different task end reasons</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ui.jobs.JobProgressListenerSuite</className>
          <testName>test update metrics</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ui.jobs.JobProgressListenerSuite</className>
          <testName>drop internal and sql accumulators</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.ui.scope.RDDOperationGraphListenerSuite.xml</file>
      <name>org.apache.spark.ui.scope.RDDOperationGraphListenerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.48399997</duration>
      <cases>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.ui.scope.RDDOperationGraphListenerSuite</className>
          <testName>run normal jobs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ui.scope.RDDOperationGraphListenerSuite</className>
          <testName>run jobs with skipped stages</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.478</duration>
          <className>org.apache.spark.ui.scope.RDDOperationGraphListenerSuite</className>
          <testName>clean up metadata</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ui.scope.RDDOperationGraphListenerSuite</className>
          <testName>fate sharing between jobs and stages</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.ui.scope.RDDOperationGraphSuite.xml</file>
      <name>org.apache.spark.ui.scope.RDDOperationGraphSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.0</duration>
      <cases>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ui.scope.RDDOperationGraphSuite</className>
          <testName>Test simple cluster equals</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.ui.storage.StoragePageSuite.xml</file>
      <name>org.apache.spark.ui.storage.StoragePageSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.049000002</duration>
      <cases>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.ui.storage.StoragePageSuite</className>
          <testName>rddTable</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ui.storage.StoragePageSuite</className>
          <testName>empty rddTable</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ui.storage.StoragePageSuite</className>
          <testName>streamBlockStorageLevelDescriptionAndSize</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.ui.storage.StoragePageSuite</className>
          <testName>receiverBlockTables</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ui.storage.StoragePageSuite</className>
          <testName>empty receiverBlockTables</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.ui.storage.StorageTabSuite.xml</file>
      <name>org.apache.spark.ui.storage.StorageTabSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.0060000005</duration>
      <cases>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ui.storage.StorageTabSuite</className>
          <testName>stage submitted / completed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ui.storage.StorageTabSuite</className>
          <testName>unpersist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ui.storage.StorageTabSuite</className>
          <testName>block update</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ui.storage.StorageTabSuite</className>
          <testName>verify StorageTab contains all cached rdds</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ui.storage.StorageTabSuite</className>
          <testName>verify StorageTab still contains a renamed RDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.unsafe.map.BytesToBytesMapOffHeapSuite.xml</file>
      <name>org.apache.spark.unsafe.map.BytesToBytesMapOffHeapSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.246</duration>
      <cases>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.unsafe.map.BytesToBytesMapOffHeapSuite</className>
          <testName>failureToGrow</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.039</duration>
          <className>org.apache.spark.unsafe.map.BytesToBytesMapOffHeapSuite</className>
          <testName>setAndRetrieveAKey</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.unsafe.map.BytesToBytesMapOffHeapSuite</className>
          <testName>multipleValuesForSameKey</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.unsafe.map.BytesToBytesMapOffHeapSuite</className>
          <testName>failureToAllocateFirstPage</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.unsafe.map.BytesToBytesMapOffHeapSuite</className>
          <testName>testPeakMemoryUsed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.053</duration>
          <className>org.apache.spark.unsafe.map.BytesToBytesMapOffHeapSuite</className>
          <testName>randomizedTestWithRecordsLargerThanPageSize</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.unsafe.map.BytesToBytesMapOffHeapSuite</className>
          <testName>spillInIterator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.058</duration>
          <className>org.apache.spark.unsafe.map.BytesToBytesMapOffHeapSuite</className>
          <testName>destructiveIteratorTest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.57</duration>
          <className>org.apache.spark.unsafe.map.BytesToBytesMapOffHeapSuite</className>
          <testName>iteratingOverDataPagesWithWastedSpace</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.421</duration>
          <className>org.apache.spark.unsafe.map.BytesToBytesMapOffHeapSuite</className>
          <testName>randomizedStressTest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.041</duration>
          <className>org.apache.spark.unsafe.map.BytesToBytesMapOffHeapSuite</className>
          <testName>iteratorTest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.unsafe.map.BytesToBytesMapOffHeapSuite</className>
          <testName>emptyMap</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.unsafe.map.BytesToBytesMapOffHeapSuite</className>
          <testName>initialCapacityBoundsChecking</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.unsafe.map.BytesToBytesMapOnHeapSuite.xml</file>
      <name>org.apache.spark.unsafe.map.BytesToBytesMapOnHeapSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.021</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.unsafe.map.BytesToBytesMapOnHeapSuite</className>
          <testName>failureToGrow</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.038</duration>
          <className>org.apache.spark.unsafe.map.BytesToBytesMapOnHeapSuite</className>
          <testName>setAndRetrieveAKey</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.unsafe.map.BytesToBytesMapOnHeapSuite</className>
          <testName>multipleValuesForSameKey</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.unsafe.map.BytesToBytesMapOnHeapSuite</className>
          <testName>failureToAllocateFirstPage</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.unsafe.map.BytesToBytesMapOnHeapSuite</className>
          <testName>testPeakMemoryUsed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.unsafe.map.BytesToBytesMapOnHeapSuite</className>
          <testName>randomizedTestWithRecordsLargerThanPageSize</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.unsafe.map.BytesToBytesMapOnHeapSuite</className>
          <testName>spillInIterator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.unsafe.map.BytesToBytesMapOnHeapSuite</className>
          <testName>destructiveIteratorTest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.529</duration>
          <className>org.apache.spark.unsafe.map.BytesToBytesMapOnHeapSuite</className>
          <testName>iteratingOverDataPagesWithWastedSpace</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.357</duration>
          <className>org.apache.spark.unsafe.map.BytesToBytesMapOnHeapSuite</className>
          <testName>randomizedStressTest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.unsafe.map.BytesToBytesMapOnHeapSuite</className>
          <testName>iteratorTest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.unsafe.map.BytesToBytesMapOnHeapSuite</className>
          <testName>emptyMap</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.unsafe.map.BytesToBytesMapOnHeapSuite</className>
          <testName>initialCapacityBoundsChecking</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.AccumulatorV2Suite.xml</file>
      <name>org.apache.spark.util.AccumulatorV2Suite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.001</duration>
      <cases>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.util.AccumulatorV2Suite</className>
          <testName>LongAccumulator add/avg/sum/count/isZero</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.util.AccumulatorV2Suite</className>
          <testName>DoubleAccumulator add/avg/sum/count/isZero</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.util.AccumulatorV2Suite</className>
          <testName>ListAccumulator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.AccumulatorV2Suite</className>
          <testName>LegacyAccumulatorWrapper</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.CausedBySuite.xml</file>
      <name>org.apache.spark.util.CausedBySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.001</duration>
      <cases>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.util.CausedBySuite</className>
          <testName>For an error without a cause, should return the error</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.CausedBySuite</className>
          <testName>For an error with a cause, should return the cause of the error</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.util.CausedBySuite</className>
          <testName>For an error with a cause that itself has a cause, return the root cause</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.ClosureCleanerSuite.xml</file>
      <name>org.apache.spark.util.ClosureCleanerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.67399997</duration>
      <cases>
        <case>
          <duration>0.058</duration>
          <className>org.apache.spark.util.ClosureCleanerSuite</className>
          <testName>closures inside an object</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.054</duration>
          <className>org.apache.spark.util.ClosureCleanerSuite</className>
          <testName>closures inside a class</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.073</duration>
          <className>org.apache.spark.util.ClosureCleanerSuite</className>
          <testName>closures inside a class with no default constructor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.058</duration>
          <className>org.apache.spark.util.ClosureCleanerSuite</className>
          <testName>closures that don&apos;t use fields of the outer class</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.105</duration>
          <className>org.apache.spark.util.ClosureCleanerSuite</className>
          <testName>nested closures inside an object</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.111</duration>
          <className>org.apache.spark.util.ClosureCleanerSuite</className>
          <testName>nested closures inside a class</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.052</duration>
          <className>org.apache.spark.util.ClosureCleanerSuite</className>
          <testName>toplevel return statements in closures are identified at cleaning time</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.052</duration>
          <className>org.apache.spark.util.ClosureCleanerSuite</className>
          <testName>return statements from named functions nested in closures don&apos;t raise exceptions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.106</duration>
          <className>org.apache.spark.util.ClosureCleanerSuite</className>
          <testName>user provided closures are actually cleaned</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.util.ClosureCleanerSuite</className>
          <testName>createNullValue</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.ClosureCleanerSuite2.xml</file>
      <name>org.apache.spark.util.ClosureCleanerSuite2</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.208</duration>
      <cases>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.util.ClosureCleanerSuite2</className>
          <testName>get inner closure classes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.ClosureCleanerSuite2</className>
          <testName>get outer classes and objects</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.util.ClosureCleanerSuite2</className>
          <testName>get outer classes and objects with nesting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.util.ClosureCleanerSuite2</className>
          <testName>find accessed fields</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.util.ClosureCleanerSuite2</className>
          <testName>find accessed fields with nesting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.util.ClosureCleanerSuite2</className>
          <testName>clean basic serializable closures</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.util.ClosureCleanerSuite2</className>
          <testName>clean basic non-serializable closures</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.util.ClosureCleanerSuite2</className>
          <testName>clean basic nested serializable closures</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.052</duration>
          <className>org.apache.spark.util.ClosureCleanerSuite2</className>
          <testName>clean basic nested non-serializable closures</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.util.ClosureCleanerSuite2</className>
          <testName>clean complicated nested serializable closures</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.063</duration>
          <className>org.apache.spark.util.ClosureCleanerSuite2</className>
          <testName>clean complicated nested non-serializable closures</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.CompletionIteratorSuite.xml</file>
      <name>org.apache.spark.util.CompletionIteratorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.001</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.CompletionIteratorSuite</className>
          <testName>basic test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.DistributionSuite.xml</file>
      <name>org.apache.spark.util.DistributionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.001</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.DistributionSuite</className>
          <testName>summary</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.EventLoopSuite.xml</file>
      <name>org.apache.spark.util.EventLoopSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.031000001</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.util.EventLoopSuite</className>
          <testName>EventLoop</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.EventLoopSuite</className>
          <testName>EventLoop: start and stop</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.util.EventLoopSuite</className>
          <testName>EventLoop: onError</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.util.EventLoopSuite</className>
          <testName>EventLoop: error thrown from onError should not crash the event thread</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.EventLoopSuite</className>
          <testName>EventLoop: calling stop multiple times should only call onStop once</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.util.EventLoopSuite</className>
          <testName>EventLoop: post event in multiple threads</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.util.EventLoopSuite</className>
          <testName>EventLoop: onReceive swallows InterruptException</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.EventLoopSuite</className>
          <testName>EventLoop: stop in eventThread</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.EventLoopSuite</className>
          <testName>EventLoop: stop() in onStart should call onStop</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.EventLoopSuite</className>
          <testName>EventLoop: stop() in onReceive should call onStop</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.EventLoopSuite</className>
          <testName>EventLoop: stop() in onError should call onStop</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.FileAppenderSuite.xml</file>
      <name>org.apache.spark.util.FileAppenderSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.1739998</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.util.FileAppenderSuite</className>
          <testName>basic file appender</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.012</duration>
          <className>org.apache.spark.util.FileAppenderSuite</className>
          <testName>rolling file appender - time-based rolling</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.006</duration>
          <className>org.apache.spark.util.FileAppenderSuite</className>
          <testName>rolling file appender - time-based rolling (compressed)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.util.FileAppenderSuite</className>
          <testName>rolling file appender - size-based rolling</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.util.FileAppenderSuite</className>
          <testName>rolling file appender - size-based rolling (compressed)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.116</duration>
          <className>org.apache.spark.util.FileAppenderSuite</className>
          <testName>rolling file appender - cleaning</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.util.FileAppenderSuite</className>
          <testName>file appender selection</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.util.FileAppenderSuite</className>
          <testName>file appender async close stream abruptly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.util.FileAppenderSuite</className>
          <testName>file appender async close stream gracefully</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.JsonProtocolSuite.xml</file>
      <name>org.apache.spark.util.JsonProtocolSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.08599997</duration>
      <cases>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.util.JsonProtocolSuite</className>
          <testName>SparkListenerEvent</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.util.JsonProtocolSuite</className>
          <testName>Dependent Classes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.util.JsonProtocolSuite</className>
          <testName>ExceptionFailure backward compatibility: full stack trace</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.util.JsonProtocolSuite</className>
          <testName>StageInfo backward compatibility (details, accumulables)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.JsonProtocolSuite</className>
          <testName>InputMetrics backward compatibility</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.JsonProtocolSuite</className>
          <testName>Input/Output records backwards compatibility</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.JsonProtocolSuite</className>
          <testName>Shuffle Read/Write records backwards compatibility</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.JsonProtocolSuite</className>
          <testName>OutputMetrics backward compatibility</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.JsonProtocolSuite</className>
          <testName>BlockManager events backward compatibility</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.JsonProtocolSuite</className>
          <testName>FetchFailed backwards compatibility</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.JsonProtocolSuite</className>
          <testName>ShuffleReadMetrics: Local bytes read backwards compatibility</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.util.JsonProtocolSuite</className>
          <testName>SparkListenerApplicationStart backwards compatibility</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.JsonProtocolSuite</className>
          <testName>ExecutorLostFailure backward compatibility</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.util.JsonProtocolSuite</className>
          <testName>SparkListenerJobStart backward compatibility</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.util.JsonProtocolSuite</className>
          <testName>SparkListenerJobStart and SparkListenerJobEnd backward compatibility</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.util.JsonProtocolSuite</className>
          <testName>RDDInfo backward compatibility (scope, parent IDs, callsite)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.JsonProtocolSuite</className>
          <testName>StageInfo backward compatibility (parent IDs)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.util.JsonProtocolSuite</className>
          <testName>TaskCommitDenied backward compatibility</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.JsonProtocolSuite</className>
          <testName>AccumulableInfo backward compatibility</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.util.JsonProtocolSuite</className>
          <testName>ExceptionFailure backward compatibility: accumulator updates</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.util.JsonProtocolSuite</className>
          <testName>AccumulableInfo value de/serialization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.MutableURLClassLoaderSuite.xml</file>
      <name>org.apache.spark.util.MutableURLClassLoaderSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.204</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.util.MutableURLClassLoaderSuite</className>
          <testName>child first</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.MutableURLClassLoaderSuite</className>
          <testName>parent first</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.MutableURLClassLoaderSuite</className>
          <testName>child first can fall back</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.MutableURLClassLoaderSuite</className>
          <testName>child first can fail</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.MutableURLClassLoaderSuite</className>
          <testName>default JDK classloader get resources</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.MutableURLClassLoaderSuite</className>
          <testName>parent first get resources</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.util.MutableURLClassLoaderSuite</className>
          <testName>child first get resources</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.195</duration>
          <className>org.apache.spark.util.MutableURLClassLoaderSuite</className>
          <testName>driver sets context class loader in local mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.NextIteratorSuite.xml</file>
      <name>org.apache.spark.util.NextIteratorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.001</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.NextIteratorSuite</className>
          <testName>one iteration</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.util.NextIteratorSuite</className>
          <testName>two iterations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.util.NextIteratorSuite</className>
          <testName>empty iteration</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.util.NextIteratorSuite</className>
          <testName>close is called once for empty iterations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.util.NextIteratorSuite</className>
          <testName>close is called once for non-empty iterations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.SizeEstimatorSuite.xml</file>
      <name>org.apache.spark.util.SizeEstimatorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.076000005</duration>
      <cases>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.util.SizeEstimatorSuite</className>
          <testName>simple classes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.SizeEstimatorSuite</className>
          <testName>primitive wrapper objects</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.util.SizeEstimatorSuite</className>
          <testName>class field blocks rounding</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.util.SizeEstimatorSuite</className>
          <testName>strings</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.SizeEstimatorSuite</className>
          <testName>primitive arrays</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.054</duration>
          <className>org.apache.spark.util.SizeEstimatorSuite</className>
          <testName>object arrays</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.util.SizeEstimatorSuite</className>
          <testName>32-bit arch</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.util.SizeEstimatorSuite</className>
          <testName>64-bit arch with no compressed oops</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.SizeEstimatorSuite</className>
          <testName>class field blocks rounding on 64-bit VM without useCompressedOops</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.SizeEstimatorSuite</className>
          <testName>check 64-bit detection for s390x arch</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.SizeEstimatorSuite</className>
          <testName>SizeEstimation can provide the estimated size</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.ThreadUtilsSuite.xml</file>
      <name>org.apache.spark.util.ThreadUtilsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.019</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.ThreadUtilsSuite</className>
          <testName>newDaemonSingleThreadExecutor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.ThreadUtilsSuite</className>
          <testName>newDaemonSingleThreadScheduledExecutor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.009</duration>
          <className>org.apache.spark.util.ThreadUtilsSuite</className>
          <testName>newDaemonCachedThreadPool</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.ThreadUtilsSuite</className>
          <testName>sameThread</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.util.ThreadUtilsSuite</className>
          <testName>runInNewThread</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.TimeStampedHashMapSuite.xml</file>
      <name>org.apache.spark.util.TimeStampedHashMapSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.229</duration>
      <cases>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.util.TimeStampedHashMapSuite</className>
          <testName>HashMap - basic test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.util.TimeStampedHashMapSuite</className>
          <testName>TimeStampedHashMap - basic test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.187</duration>
          <className>org.apache.spark.util.TimeStampedHashMapSuite</className>
          <testName>TimeStampedHashMap - threading safety test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.033</duration>
          <className>org.apache.spark.util.TimeStampedHashMapSuite</className>
          <testName>TimeStampedHashMap - clearing by timestamp</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.UninterruptibleThreadSuite.xml</file>
      <name>org.apache.spark.util.UninterruptibleThreadSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.7779999</duration>
      <cases>
        <case>
          <duration>1.002</duration>
          <className>org.apache.spark.util.UninterruptibleThreadSuite</className>
          <testName>interrupt when runUninterruptibly is running</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.util.UninterruptibleThreadSuite</className>
          <testName>interrupt before runUninterruptibly runs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.util.UninterruptibleThreadSuite</className>
          <testName>nested runUninterruptibly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.768</duration>
          <className>org.apache.spark.util.UninterruptibleThreadSuite</className>
          <testName>stress test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.UtilsSuite.xml</file>
      <name>org.apache.spark.util.UtilsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.238</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>truncatedString</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>timeConversion</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>Test byteString conversion</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>bytesToString</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>copyStream</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>memoryStringToMb</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>splitCommandString</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>string formatting of time durations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>reading offset bytes of a file</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>reading offset bytes of a file (compressed)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>reading offset bytes across multiple files</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>reading offset bytes across multiple files (compressed)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>deserialize long value</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>get iterator size</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>getIteratorZipWithIndex</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>doesDirectoryContainFilesNewerThan</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>resolveURI</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>resolveURIs with multiple paths</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>nonLocalPaths</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>isBindCollision</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>log4j log level change</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>deleteRecursively</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>loading properties from file</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.003</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>timeIt with prepare</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>fetch hcfs dir</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>shutdown hook manager</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>isInDirectory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>circular buffer: if nothing was written to the buffer, display nothing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>circular buffer: if the buffer isn&apos;t full, print only the contents written</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>circular buffer: data written == size of the buffer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>circular buffer: multiple overflow</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>nanSafeCompareDoubles</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>nanSafeCompareFloats</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>isDynamicAllocationEnabled</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>getDynamicAllocationInitialExecutors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>Set Spark CallerContext</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>encodeFileNameToURIRawPath</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>decodeFileNameInURI</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.131</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>Kill process</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.util.UtilsSuite</className>
          <testName>chi square test of randomizeInPlace</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.VersionUtilsSuite.xml</file>
      <name>org.apache.spark.util.VersionUtilsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.006</duration>
      <cases>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.util.VersionUtilsSuite</className>
          <testName>Parse Spark major version</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.util.VersionUtilsSuite</className>
          <testName>Parse Spark minor version</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.VersionUtilsSuite</className>
          <testName>Parse Spark major and minor versions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.collection.AppendOnlyMapSuite.xml</file>
      <name>org.apache.spark.util.collection.AppendOnlyMapSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.017</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.collection.AppendOnlyMapSuite</className>
          <testName>initialization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.util.collection.AppendOnlyMapSuite</className>
          <testName>object keys and values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.util.collection.AppendOnlyMapSuite</className>
          <testName>primitive keys and values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.collection.AppendOnlyMapSuite</className>
          <testName>null keys</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.collection.AppendOnlyMapSuite</className>
          <testName>null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.util.collection.AppendOnlyMapSuite</className>
          <testName>changeValue</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.collection.AppendOnlyMapSuite</className>
          <testName>inserting in capacity-1 map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.util.collection.AppendOnlyMapSuite</className>
          <testName>destructive sort</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.collection.BitSetSuite.xml</file>
      <name>org.apache.spark.util.collection.BitSetSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.024000002</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.util.collection.BitSetSuite</className>
          <testName>basic set and get</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.util.collection.BitSetSuite</className>
          <testName>100% full bit set</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.collection.BitSetSuite</className>
          <testName>nextSetBit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.collection.BitSetSuite</className>
          <testName>xor len(bitsetX) &lt; len(bitsetY)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.collection.BitSetSuite</className>
          <testName>xor len(bitsetX) &gt; len(bitsetY)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.collection.BitSetSuite</className>
          <testName>andNot len(bitsetX) &lt; len(bitsetY)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.collection.BitSetSuite</className>
          <testName>andNot len(bitsetX) &gt; len(bitsetY)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.util.collection.BitSetSuite</className>
          <testName>[gs]etUntil</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.collection.CompactBufferSuite.xml</file>
      <name>org.apache.spark.util.collection.CompactBufferSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.014</duration>
      <cases>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.util.collection.CompactBufferSuite</className>
          <testName>empty buffer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.util.collection.CompactBufferSuite</className>
          <testName>basic inserts</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.util.collection.CompactBufferSuite</className>
          <testName>adding sequences</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.util.collection.CompactBufferSuite</className>
          <testName>adding the same buffer to itself</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.collection.ExternalAppendOnlyMapSuite.xml</file>
      <name>org.apache.spark.util.collection.ExternalAppendOnlyMapSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>38.31</duration>
      <cases>
        <case>
          <duration>0.058</duration>
          <className>org.apache.spark.util.collection.ExternalAppendOnlyMapSuite</className>
          <testName>single insert insert</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.057</duration>
          <className>org.apache.spark.util.collection.ExternalAppendOnlyMapSuite</className>
          <testName>multiple insert</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.066</duration>
          <className>org.apache.spark.util.collection.ExternalAppendOnlyMapSuite</className>
          <testName>insert with collision</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.066</duration>
          <className>org.apache.spark.util.collection.ExternalAppendOnlyMapSuite</className>
          <testName>ordering</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.075</duration>
          <className>org.apache.spark.util.collection.ExternalAppendOnlyMapSuite</className>
          <testName>null keys and values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.164</duration>
          <className>org.apache.spark.util.collection.ExternalAppendOnlyMapSuite</className>
          <testName>simple aggregator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.115</duration>
          <className>org.apache.spark.util.collection.ExternalAppendOnlyMapSuite</className>
          <testName>simple cogroup</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>5.27</duration>
          <className>org.apache.spark.util.collection.ExternalAppendOnlyMapSuite</className>
          <testName>spilling</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>15.368</duration>
          <className>org.apache.spark.util.collection.ExternalAppendOnlyMapSuite</className>
          <testName>spilling with compression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.295</duration>
          <className>org.apache.spark.util.collection.ExternalAppendOnlyMapSuite</className>
          <testName>spilling with hash collisions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.746</duration>
          <className>org.apache.spark.util.collection.ExternalAppendOnlyMapSuite</className>
          <testName>spilling with many hash collisions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.232</duration>
          <className>org.apache.spark.util.collection.ExternalAppendOnlyMapSuite</className>
          <testName>MaxValue key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.29</duration>
          <className>org.apache.spark.util.collection.ExternalAppendOnlyMapSuite</className>
          <testName>spilling with null keys and values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.372</duration>
          <className>org.apache.spark.util.collection.ExternalAppendOnlyMapSuite</className>
          <testName>external aggregation updates peak execution memory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>15.136</duration>
          <className>org.apache.spark.util.collection.ExternalAppendOnlyMapSuite</className>
          <testName>force to spill for external aggregation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.collection.ExternalSorterSuite.xml</file>
      <name>org.apache.spark.util.collection.ExternalSorterSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>50.407013</duration>
      <cases>
        <case>
          <duration>0.056</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>empty data stream with kryo ser</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.07</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>empty data stream with java ser</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.054</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>few elements per partition with kryo ser</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>few elements per partition with java ser</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.121</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>empty partitions with spilling with kryo ser</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.108</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>empty partitions with spilling with java ser</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>7.262</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>spilling in local cluster with kryo ser</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>7.208</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>spilling in local cluster with java ser</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>11.559</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>spilling in local cluster with many reduce tasks with kryo ser</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>10.23</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>spilling in local cluster with many reduce tasks with java ser</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.124</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>cleanup of intermediate files in sorter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.112</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>cleanup of intermediate files in sorter with failures</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.227</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>cleanup of intermediate files in shuffle</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.123</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>cleanup of intermediate files in shuffle with failures</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.063</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>no sorting or partial aggregation with kryo ser</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.052</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>no sorting or partial aggregation with java ser</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.088</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>no sorting or partial aggregation with spilling with kryo ser</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.088</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>no sorting or partial aggregation with spilling with java ser</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.06</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>sorting, no partial aggregation with kryo ser</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.053</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>sorting, no partial aggregation with java ser</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.076</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>sorting, no partial aggregation with spilling with kryo ser</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.097</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>sorting, no partial aggregation with spilling with java ser</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.062</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>partial aggregation, no sorting with kryo ser</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.056</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>partial aggregation, no sorting with java ser</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.067</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>partial aggregation, no sorting with spilling with kryo ser</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.066</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>partial aggregation, no sorting with spilling with java ser</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.075</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>partial aggregation and sorting with kryo ser</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.06</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>partial aggregation and sorting with java ser</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.068</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>partial aggregation and sorting with spilling with kryo ser</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.063</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>partial aggregation and sorting with spilling with java ser</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.174</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>sort without breaking sorting contracts with kryo ser</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.949</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>sort without breaking sorting contracts with java ser</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>sort without breaking timsort contracts for large arrays</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.187</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>spilling with hash collisions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.587</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>spilling with many hash collisions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.197</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>MaxValue key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.242</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>spilling with null keys and values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.472</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>sorting updates peak execution memory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>5.204</duration>
          <className>org.apache.spark.util.collection.ExternalSorterSuite</className>
          <testName>force to spill for external sorter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.collection.OpenHashMapSuite.xml</file>
      <name>org.apache.spark.util.collection.OpenHashMapSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>4.926</duration>
      <cases>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.util.collection.OpenHashMapSuite</className>
          <testName>size for specialized, primitive value (int)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.collection.OpenHashMapSuite</className>
          <testName>initialization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.util.collection.OpenHashMapSuite</className>
          <testName>primitive value</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.018</duration>
          <className>org.apache.spark.util.collection.OpenHashMapSuite</className>
          <testName>non-primitive value</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.collection.OpenHashMapSuite</className>
          <testName>null keys</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.collection.OpenHashMapSuite</className>
          <testName>null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.util.collection.OpenHashMapSuite</className>
          <testName>changeValue</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.collection.OpenHashMapSuite</className>
          <testName>inserting in capacity-1 map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.util.collection.OpenHashMapSuite</className>
          <testName>contains</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.866</duration>
          <className>org.apache.spark.util.collection.OpenHashMapSuite</className>
          <testName>support for more than 12M items</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.collection.OpenHashSetSuite.xml</file>
      <name>org.apache.spark.util.collection.OpenHashSetSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.012</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.collection.OpenHashSetSuite</className>
          <testName>size for specialized, primitive int</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.collection.OpenHashSetSuite</className>
          <testName>primitive int</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.collection.OpenHashSetSuite</className>
          <testName>primitive long</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.util.collection.OpenHashSetSuite</className>
          <testName>non-primitive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.util.collection.OpenHashSetSuite</className>
          <testName>non-primitive set growth</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.util.collection.OpenHashSetSuite</className>
          <testName>primitive set growth</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.collection.OpenHashSetSuite</className>
          <testName>SPARK-18200 Support zero as an initial set size</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.collection.PrimitiveKeyOpenHashMapSuite.xml</file>
      <name>org.apache.spark.util.collection.PrimitiveKeyOpenHashMapSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.042</duration>
      <cases>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.util.collection.PrimitiveKeyOpenHashMapSuite</className>
          <testName>size for specialized, primitive key, value (int, int)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.collection.PrimitiveKeyOpenHashMapSuite</className>
          <testName>initialization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.util.collection.PrimitiveKeyOpenHashMapSuite</className>
          <testName>basic operations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.collection.PrimitiveKeyOpenHashMapSuite</className>
          <testName>null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.util.collection.PrimitiveKeyOpenHashMapSuite</className>
          <testName>changeValue</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.util.collection.PrimitiveKeyOpenHashMapSuite</className>
          <testName>inserting in capacity-1 map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.util.collection.PrimitiveKeyOpenHashMapSuite</className>
          <testName>contains</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.collection.PrimitiveVectorSuite.xml</file>
      <name>org.apache.spark.util.collection.PrimitiveVectorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.032</duration>
      <cases>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.util.collection.PrimitiveVectorSuite</className>
          <testName>primitive value</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.util.collection.PrimitiveVectorSuite</className>
          <testName>non-primitive value</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.util.collection.PrimitiveVectorSuite</className>
          <testName>ideal growth</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.util.collection.PrimitiveVectorSuite</className>
          <testName>ideal size</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.util.collection.PrimitiveVectorSuite</className>
          <testName>resizing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.collection.SizeTrackerSuite.xml</file>
      <name>org.apache.spark.util.collection.SizeTrackerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>6.148</duration>
      <cases>
        <case>
          <duration>1.063</duration>
          <className>org.apache.spark.util.collection.SizeTrackerSuite</className>
          <testName>vector fixed size insertions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.708</duration>
          <className>org.apache.spark.util.collection.SizeTrackerSuite</className>
          <testName>vector variable size insertions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.055</duration>
          <className>org.apache.spark.util.collection.SizeTrackerSuite</className>
          <testName>map fixed size insertions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.121</duration>
          <className>org.apache.spark.util.collection.SizeTrackerSuite</className>
          <testName>map variable size insertions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.201</duration>
          <className>org.apache.spark.util.collection.SizeTrackerSuite</className>
          <testName>map updates</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.collection.SorterSuite.xml</file>
      <name>org.apache.spark.util.collection.SorterSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.6540003</duration>
      <cases>
        <case>
          <duration>0.168</duration>
          <className>org.apache.spark.util.collection.SorterSuite</className>
          <testName>sort</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.107</duration>
          <className>org.apache.spark.util.collection.SorterSuite</className>
          <testName>KVArraySorter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.381</duration>
          <className>org.apache.spark.util.collection.SorterSuite</className>
          <testName>SPARK-5984 TimSort bug</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.util.collection.SorterSuite</className>
          <testName>Sorter benchmark for key-value pairs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.util.collection.SorterSuite</className>
          <testName>Sorter benchmark for primitive int array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.collection.unsafe.sort.PrefixComparatorsSuite.xml</file>
      <name>org.apache.spark.util.collection.unsafe.sort.PrefixComparatorsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.208</duration>
      <cases>
        <case>
          <duration>0.141</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.PrefixComparatorsSuite</className>
          <testName>String prefix comparator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.065</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.PrefixComparatorsSuite</className>
          <testName>Binary prefix comparator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.PrefixComparatorsSuite</className>
          <testName>double prefix comparator handles NaNs properly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.PrefixComparatorsSuite</className>
          <testName>double prefix comparator handles negative NaNs properly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.PrefixComparatorsSuite</className>
          <testName>double prefix comparator handles other special values properly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.collection.unsafe.sort.RadixSortSuite.xml</file>
      <name>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.5950003</duration>
      <cases>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>radix support for unsigned binary data asc nulls first</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.054</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>sort unsigned binary data asc nulls first</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.057</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>sort key prefix unsigned binary data asc nulls first</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.214</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>fuzz test unsigned binary data asc nulls first with random bitmasks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.092</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>fuzz test key prefix unsigned binary data asc nulls first with random bitmasks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>radix support for unsigned binary data asc nulls last</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>sort unsigned binary data asc nulls last</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>sort key prefix unsigned binary data asc nulls last</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.111</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>fuzz test unsigned binary data asc nulls last with random bitmasks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.075</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>fuzz test key prefix unsigned binary data asc nulls last with random bitmasks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>radix support for unsigned binary data desc nulls last</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.032</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>sort unsigned binary data desc nulls last</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>sort key prefix unsigned binary data desc nulls last</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.054</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>fuzz test unsigned binary data desc nulls last with random bitmasks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.075</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>fuzz test key prefix unsigned binary data desc nulls last with random bitmasks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>radix support for unsigned binary data desc nulls first</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>sort unsigned binary data desc nulls first</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>sort key prefix unsigned binary data desc nulls first</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.052</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>fuzz test unsigned binary data desc nulls first with random bitmasks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.072</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>fuzz test key prefix unsigned binary data desc nulls first with random bitmasks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>radix support for twos complement asc nulls first</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>sort twos complement asc nulls first</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>sort key prefix twos complement asc nulls first</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>fuzz test twos complement asc nulls first with random bitmasks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.097</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>fuzz test key prefix twos complement asc nulls first with random bitmasks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>radix support for twos complement asc nulls last</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>sort twos complement asc nulls last</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>sort key prefix twos complement asc nulls last</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>fuzz test twos complement asc nulls last with random bitmasks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.065</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>fuzz test key prefix twos complement asc nulls last with random bitmasks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>radix support for twos complement desc nulls last</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>sort twos complement desc nulls last</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>sort key prefix twos complement desc nulls last</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.046</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>fuzz test twos complement desc nulls last with random bitmasks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.069</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>fuzz test key prefix twos complement desc nulls last with random bitmasks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>radix support for twos complement desc nulls first</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>sort twos complement desc nulls first</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>sort key prefix twos complement desc nulls first</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.039</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>fuzz test twos complement desc nulls first with random bitmasks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.067</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>fuzz test key prefix twos complement desc nulls first with random bitmasks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>radix support for binary data partial</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>sort binary data partial</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>sort key prefix binary data partial</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.035</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>fuzz test binary data partial with random bitmasks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.058</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.RadixSortSuite</className>
          <testName>fuzz test key prefix binary data partial with random bitmasks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorterRadixSortSuite.xml</file>
      <name>org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorterRadixSortSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.745</duration>
      <cases>
        <case>
          <duration>0.126</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorterRadixSortSuite</className>
          <testName>forcedSpillingWithoutComparator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorterRadixSortSuite</className>
          <testName>testPeakMemoryUsed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorterRadixSortSuite</className>
          <testName>testSortingEmptyArrays</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.111</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorterRadixSortSuite</className>
          <testName>sortingRecordsThatExceedPageSize</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorterRadixSortSuite</className>
          <testName>testSortingOnlyByPrefix</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorterRadixSortSuite</className>
          <testName>testSortTimeMetric</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorterRadixSortSuite</className>
          <testName>testFillingPage</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.06</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorterRadixSortSuite</className>
          <testName>forcedSpillingWithNotReadIterator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.363</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorterRadixSortSuite</className>
          <testName>spillingOccursInResponseToMemoryPressure</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.045</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorterRadixSortSuite</className>
          <testName>forcedSpillingWithReadIterator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorterSuite.xml</file>
      <name>org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorterSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.514</duration>
      <cases>
        <case>
          <duration>0.045</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorterSuite</className>
          <testName>forcedSpillingWithoutComparator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorterSuite</className>
          <testName>testPeakMemoryUsed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorterSuite</className>
          <testName>testSortingEmptyArrays</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.072</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorterSuite</className>
          <testName>sortingRecordsThatExceedPageSize</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorterSuite</className>
          <testName>testSortingOnlyByPrefix</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorterSuite</className>
          <testName>testSortTimeMetric</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorterSuite</className>
          <testName>testFillingPage</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.053</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorterSuite</className>
          <testName>forcedSpillingWithNotReadIterator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.275</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorterSuite</className>
          <testName>spillingOccursInResponseToMemoryPressure</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorterSuite</className>
          <testName>forcedSpillingWithReadIterator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorterRadixSortSuite.xml</file>
      <name>org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorterRadixSortSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.001</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorterRadixSortSuite</className>
          <testName>testSortingOnlyByIntegerPrefix</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorterRadixSortSuite</className>
          <testName>testSortingEmptyInput</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorterSuite.xml</file>
      <name>org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorterSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.007</duration>
      <cases>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorterSuite</className>
          <testName>testSortingOnlyByIntegerPrefix</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorterSuite</className>
          <testName>testSortingEmptyInput</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.io.ChunkedByteBufferOutputStreamSuite.xml</file>
      <name>org.apache.spark.util.io.ChunkedByteBufferOutputStreamSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.0070000007</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.io.ChunkedByteBufferOutputStreamSuite</className>
          <testName>empty output</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.io.ChunkedByteBufferOutputStreamSuite</className>
          <testName>write a single byte</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.util.io.ChunkedByteBufferOutputStreamSuite</className>
          <testName>write a single near boundary</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.io.ChunkedByteBufferOutputStreamSuite</className>
          <testName>write a single at boundary</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.io.ChunkedByteBufferOutputStreamSuite</className>
          <testName>single chunk output</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.io.ChunkedByteBufferOutputStreamSuite</className>
          <testName>single chunk output at boundary size</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.io.ChunkedByteBufferOutputStreamSuite</className>
          <testName>multiple chunk output</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.io.ChunkedByteBufferOutputStreamSuite</className>
          <testName>multiple chunk output at boundary size</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.random.RandomSamplerSuite.xml</file>
      <name>org.apache.spark.util.random.RandomSamplerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.1269999</duration>
      <cases>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.util.random.RandomSamplerSuite</className>
          <testName>utilities</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.201</duration>
          <className>org.apache.spark.util.random.RandomSamplerSuite</className>
          <testName>sanity check medianKSD against references</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.033</duration>
          <className>org.apache.spark.util.random.RandomSamplerSuite</className>
          <testName>bernoulli sampling</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.util.random.RandomSamplerSuite</className>
          <testName>bernoulli sampling without iterator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.096</duration>
          <className>org.apache.spark.util.random.RandomSamplerSuite</className>
          <testName>bernoulli sampling with gap sampling optimization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.097</duration>
          <className>org.apache.spark.util.random.RandomSamplerSuite</className>
          <testName>bernoulli sampling (without iterator) with gap sampling optimization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.util.random.RandomSamplerSuite</className>
          <testName>bernoulli boundary cases</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.util.random.RandomSamplerSuite</className>
          <testName>bernoulli (without iterator) boundary cases</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.051</duration>
          <className>org.apache.spark.util.random.RandomSamplerSuite</className>
          <testName>bernoulli data types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.util.random.RandomSamplerSuite</className>
          <testName>bernoulli clone</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.util.random.RandomSamplerSuite</className>
          <testName>bernoulli set seed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.053</duration>
          <className>org.apache.spark.util.random.RandomSamplerSuite</className>
          <testName>replacement sampling</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.053</duration>
          <className>org.apache.spark.util.random.RandomSamplerSuite</className>
          <testName>replacement sampling without iterator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.126</duration>
          <className>org.apache.spark.util.random.RandomSamplerSuite</className>
          <testName>replacement sampling with gap sampling</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.117</duration>
          <className>org.apache.spark.util.random.RandomSamplerSuite</className>
          <testName>replacement sampling (without iterator) with gap sampling</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.random.RandomSamplerSuite</className>
          <testName>replacement boundary cases</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.random.RandomSamplerSuite</className>
          <testName>replacement (without) boundary cases</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.068</duration>
          <className>org.apache.spark.util.random.RandomSamplerSuite</className>
          <testName>replacement data types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.util.random.RandomSamplerSuite</className>
          <testName>replacement clone</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.039</duration>
          <className>org.apache.spark.util.random.RandomSamplerSuite</className>
          <testName>replacement set seed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.util.random.RandomSamplerSuite</className>
          <testName>bernoulli partitioning sampling</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.04</duration>
          <className>org.apache.spark.util.random.RandomSamplerSuite</className>
          <testName>bernoulli partitioning sampling without iterator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.random.RandomSamplerSuite</className>
          <testName>bernoulli partitioning boundary cases</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.util.random.RandomSamplerSuite</className>
          <testName>bernoulli partitioning (without iterator) boundary cases</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.util.random.RandomSamplerSuite</className>
          <testName>bernoulli partitioning data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.util.random.RandomSamplerSuite</className>
          <testName>bernoulli partitioning clone</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.random.SamplingUtilsSuite.xml</file>
      <name>org.apache.spark.util.random.SamplingUtilsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.009000001</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.util.random.SamplingUtilsSuite</className>
          <testName>reservoirSampleAndCount</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.util.random.SamplingUtilsSuite</className>
          <testName>computeFraction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/core/target/test-reports/org.apache.spark.util.random.XORShiftRandomSuite.xml</file>
      <name>org.apache.spark.util.random.XORShiftRandomSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>11.474</duration>
      <cases>
        <case>
          <duration>11.472</duration>
          <className>org.apache.spark.util.random.XORShiftRandomSuite</className>
          <testName>XORShift generates valid random numbers</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.util.random.XORShiftRandomSuite</className>
          <testName>XORShift with zero seed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.util.random.XORShiftRandomSuite</className>
          <testName>hashSeed has random bits throughout</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/external/flume-sink/target/test-reports/org.apache.spark.streaming.flume.sink.SparkSinkSuite.xml</file>
      <name>org.apache.spark.streaming.flume.sink.SparkSinkSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>6.537</duration>
      <cases>
        <case>
          <duration>1.949</duration>
          <className>org.apache.spark.streaming.flume.sink.SparkSinkSuite</className>
          <testName>Success with ack</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.097</duration>
          <className>org.apache.spark.streaming.flume.sink.SparkSinkSuite</className>
          <testName>Failure with nack</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.071</duration>
          <className>org.apache.spark.streaming.flume.sink.SparkSinkSuite</className>
          <testName>Failure with timeout</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.238</duration>
          <className>org.apache.spark.streaming.flume.sink.SparkSinkSuite</className>
          <testName>Multiple consumers</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.182</duration>
          <className>org.apache.spark.streaming.flume.sink.SparkSinkSuite</className>
          <testName>Multiple consumers with some failures</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/external/flume/target/test-reports/org.apache.spark.streaming.flume.FlumePollingStreamSuite.xml</file>
      <name>org.apache.spark.streaming.flume.FlumePollingStreamSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>27.33</duration>
      <cases>
        <case>
          <duration>13.047</duration>
          <className>org.apache.spark.streaming.flume.FlumePollingStreamSuite</className>
          <testName>flume polling test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>14.283</duration>
          <className>org.apache.spark.streaming.flume.FlumePollingStreamSuite</className>
          <testName>flume polling test multiple hosts</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/external/flume/target/test-reports/org.apache.spark.streaming.flume.FlumeStreamSuite.xml</file>
      <name>org.apache.spark.streaming.flume.FlumeStreamSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>4.802</duration>
      <cases>
        <case>
          <duration>3.652</duration>
          <className>org.apache.spark.streaming.flume.FlumeStreamSuite</className>
          <testName>flume input stream</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.15</duration>
          <className>org.apache.spark.streaming.flume.FlumeStreamSuite</className>
          <testName>flume input compressed stream</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/external/flume/target/test-reports/org.apache.spark.streaming.flume.JavaFlumePollingStreamSuite.xml</file>
      <name>org.apache.spark.streaming.flume.JavaFlumePollingStreamSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.094</duration>
      <cases>
        <case>
          <duration>0.094</duration>
          <className>org.apache.spark.streaming.flume.JavaFlumePollingStreamSuite</className>
          <testName>testFlumeStream</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/external/flume/target/test-reports/org.apache.spark.streaming.flume.JavaFlumeStreamSuite.xml</file>
      <name>org.apache.spark.streaming.flume.JavaFlumeStreamSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.18</duration>
      <cases>
        <case>
          <duration>0.18</duration>
          <className>org.apache.spark.streaming.flume.JavaFlumeStreamSuite</className>
          <testName>testFlumeStream</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/external/kafka-0-10-sql/target/test-reports/org.apache.spark.sql.kafka010.JsonUtilsSuite.xml</file>
      <name>org.apache.spark.sql.kafka010.JsonUtilsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.436</duration>
      <cases>
        <case>
          <duration>0.434</duration>
          <className>org.apache.spark.sql.kafka010.JsonUtilsSuite</className>
          <testName>parsing partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.kafka010.JsonUtilsSuite</className>
          <testName>parsing partitionOffsets</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/external/kafka-0-10-sql/target/test-reports/org.apache.spark.sql.kafka010.KafkaSourceOffsetSuite.xml</file>
      <name>org.apache.spark.sql.kafka010.KafkaSourceOffsetSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.285</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.kafka010.KafkaSourceOffsetSuite</className>
          <testName>comparison {&quot;t&quot;:{&quot;0&quot;:1}} &lt;=&gt; {&quot;t&quot;:{&quot;0&quot;:2}}</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.kafka010.KafkaSourceOffsetSuite</className>
          <testName>comparison {&quot;t&quot;:{&quot;1&quot;:0,&quot;0&quot;:1}} &lt;=&gt; {&quot;t&quot;:{&quot;1&quot;:1,&quot;0&quot;:2}}</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.kafka010.KafkaSourceOffsetSuite</className>
          <testName>comparison {&quot;t&quot;:{&quot;0&quot;:1},&quot;T&quot;:{&quot;0&quot;:0}} &lt;=&gt; {&quot;t&quot;:{&quot;0&quot;:2},&quot;T&quot;:{&quot;0&quot;:1}}</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.kafka010.KafkaSourceOffsetSuite</className>
          <testName>comparison {&quot;t&quot;:{&quot;0&quot;:1}} &lt;=&gt; {&quot;t&quot;:{&quot;1&quot;:1,&quot;0&quot;:2}}</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.kafka010.KafkaSourceOffsetSuite</className>
          <testName>comparison {&quot;t&quot;:{&quot;0&quot;:1}} &lt;=&gt; {&quot;t&quot;:{&quot;1&quot;:3,&quot;0&quot;:2}}</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.kafka010.KafkaSourceOffsetSuite</className>
          <testName>basic serialization - deserialization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.281</duration>
          <className>org.apache.spark.sql.kafka010.KafkaSourceOffsetSuite</className>
          <testName>OffsetSeqLog serialization - deserialization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/external/kafka-0-10-sql/target/test-reports/org.apache.spark.sql.kafka010.KafkaSourceStressForDontFailOnDataLossSuite.xml</file>
      <name>org.apache.spark.sql.kafka010.KafkaSourceStressForDontFailOnDataLossSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>60.407</duration>
      <cases>
        <case>
          <duration>60.407</duration>
          <className>org.apache.spark.sql.kafka010.KafkaSourceStressForDontFailOnDataLossSuite</className>
          <testName>stress test for failOnDataLoss=false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/external/kafka-0-10-sql/target/test-reports/org.apache.spark.sql.kafka010.KafkaSourceStressSuite.xml</file>
      <name>org.apache.spark.sql.kafka010.KafkaSourceStressSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>46.414</duration>
      <cases>
        <case>
          <duration>46.414</duration>
          <className>org.apache.spark.sql.kafka010.KafkaSourceStressSuite</className>
          <testName>stress test with multiple topics and partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/external/kafka-0-10-sql/target/test-reports/org.apache.spark.sql.kafka010.KafkaSourceSuite.xml</file>
      <name>org.apache.spark.sql.kafka010.KafkaSourceSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>98.18</duration>
      <cases>
        <case>
          <duration>9.17</duration>
          <className>org.apache.spark.sql.kafka010.KafkaSourceSuite</className>
          <testName>maxOffsetsPerTrigger</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.901</duration>
          <className>org.apache.spark.sql.kafka010.KafkaSourceSuite</className>
          <testName>cannot stop Kafka stream</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>5.496</duration>
          <className>org.apache.spark.sql.kafka010.KafkaSourceSuite</className>
          <testName>assign from latest offsets (failOnDataLoss: true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.545</duration>
          <className>org.apache.spark.sql.kafka010.KafkaSourceSuite</className>
          <testName>assign from earliest offsets (failOnDataLoss: true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.9</duration>
          <className>org.apache.spark.sql.kafka010.KafkaSourceSuite</className>
          <testName>assign from specific offsets (failOnDataLoss: true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>5.068</duration>
          <className>org.apache.spark.sql.kafka010.KafkaSourceSuite</className>
          <testName>subscribing topic by name from latest offsets (failOnDataLoss: true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.401</duration>
          <className>org.apache.spark.sql.kafka010.KafkaSourceSuite</className>
          <testName>subscribing topic by name from earliest offsets (failOnDataLoss: true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.826</duration>
          <className>org.apache.spark.sql.kafka010.KafkaSourceSuite</className>
          <testName>subscribing topic by name from specific offsets (failOnDataLoss: true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>5.571</duration>
          <className>org.apache.spark.sql.kafka010.KafkaSourceSuite</className>
          <testName>subscribing topic by pattern from latest offsets (failOnDataLoss: true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.669</duration>
          <className>org.apache.spark.sql.kafka010.KafkaSourceSuite</className>
          <testName>subscribing topic by pattern from earliest offsets (failOnDataLoss: true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.013</duration>
          <className>org.apache.spark.sql.kafka010.KafkaSourceSuite</className>
          <testName>subscribing topic by pattern from specific offsets (failOnDataLoss: true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.975</duration>
          <className>org.apache.spark.sql.kafka010.KafkaSourceSuite</className>
          <testName>assign from latest offsets (failOnDataLoss: false)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.226</duration>
          <className>org.apache.spark.sql.kafka010.KafkaSourceSuite</className>
          <testName>assign from earliest offsets (failOnDataLoss: false)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.764</duration>
          <className>org.apache.spark.sql.kafka010.KafkaSourceSuite</className>
          <testName>assign from specific offsets (failOnDataLoss: false)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>5.077</duration>
          <className>org.apache.spark.sql.kafka010.KafkaSourceSuite</className>
          <testName>subscribing topic by name from latest offsets (failOnDataLoss: false)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.432</duration>
          <className>org.apache.spark.sql.kafka010.KafkaSourceSuite</className>
          <testName>subscribing topic by name from earliest offsets (failOnDataLoss: false)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.805</duration>
          <className>org.apache.spark.sql.kafka010.KafkaSourceSuite</className>
          <testName>subscribing topic by name from specific offsets (failOnDataLoss: false)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>5.361</duration>
          <className>org.apache.spark.sql.kafka010.KafkaSourceSuite</className>
          <testName>subscribing topic by pattern from latest offsets (failOnDataLoss: false)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.618</duration>
          <className>org.apache.spark.sql.kafka010.KafkaSourceSuite</className>
          <testName>subscribing topic by pattern from earliest offsets (failOnDataLoss: false)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.961</duration>
          <className>org.apache.spark.sql.kafka010.KafkaSourceSuite</className>
          <testName>subscribing topic by pattern from specific offsets (failOnDataLoss: false)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.799</duration>
          <className>org.apache.spark.sql.kafka010.KafkaSourceSuite</className>
          <testName>subscribing topic by pattern with topic deletions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.907</duration>
          <className>org.apache.spark.sql.kafka010.KafkaSourceSuite</className>
          <testName>starting offset is latest by default</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.kafka010.KafkaSourceSuite</className>
          <testName>bad source options</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.sql.kafka010.KafkaSourceSuite</className>
          <testName>unsupported kafka configs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.927</duration>
          <className>org.apache.spark.sql.kafka010.KafkaSourceSuite</className>
          <testName>input row metrics</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.969</duration>
          <className>org.apache.spark.sql.kafka010.KafkaSourceSuite</className>
          <testName>delete a topic when a Spark job is running</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.745</duration>
          <className>org.apache.spark.sql.kafka010.KafkaSourceSuite</className>
          <testName>Kafka column types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.031</duration>
          <className>org.apache.spark.sql.kafka010.KafkaSourceSuite</className>
          <testName>KafkaSource with watermark</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/external/kafka-0-10/target/test-reports/org.apache.spark.streaming.kafka010.DirectKafkaStreamSuite.xml</file>
      <name>org.apache.spark.streaming.kafka010.DirectKafkaStreamSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>18.087002</duration>
      <cases>
        <case>
          <duration>5.632</duration>
          <className>org.apache.spark.streaming.kafka010.DirectKafkaStreamSuite</className>
          <testName>basic stream receiving with multiple topics and smallest starting offset</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.239</duration>
          <className>org.apache.spark.streaming.kafka010.DirectKafkaStreamSuite</className>
          <testName>pattern based subscription</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.688</duration>
          <className>org.apache.spark.streaming.kafka010.DirectKafkaStreamSuite</className>
          <testName>receiving from largest starting offset</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.677</duration>
          <className>org.apache.spark.streaming.kafka010.DirectKafkaStreamSuite</className>
          <testName>creating stream by offset</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.962</duration>
          <className>org.apache.spark.streaming.kafka010.DirectKafkaStreamSuite</className>
          <testName>offset recovery</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.175</duration>
          <className>org.apache.spark.streaming.kafka010.DirectKafkaStreamSuite</className>
          <testName>offset recovery from kafka</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.581</duration>
          <className>org.apache.spark.streaming.kafka010.DirectKafkaStreamSuite</className>
          <testName>Direct Kafka stream report input information</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.173</duration>
          <className>org.apache.spark.streaming.kafka010.DirectKafkaStreamSuite</className>
          <testName>maxMessagesPerPartition with backpressure disabled</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.163</duration>
          <className>org.apache.spark.streaming.kafka010.DirectKafkaStreamSuite</className>
          <testName>maxMessagesPerPartition with no lag</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.161</duration>
          <className>org.apache.spark.streaming.kafka010.DirectKafkaStreamSuite</className>
          <testName>maxMessagesPerPartition respects max rate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.636</duration>
          <className>org.apache.spark.streaming.kafka010.DirectKafkaStreamSuite</className>
          <testName>using rate controller</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/external/kafka-0-10/target/test-reports/org.apache.spark.streaming.kafka010.JavaConsumerStrategySuite.xml</file>
      <name>org.apache.spark.streaming.kafka010.JavaConsumerStrategySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.003</duration>
      <cases>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.streaming.kafka010.JavaConsumerStrategySuite</className>
          <testName>testConsumerStrategyConstructors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/external/kafka-0-10/target/test-reports/org.apache.spark.streaming.kafka010.JavaDirectKafkaStreamSuite.xml</file>
      <name>org.apache.spark.streaming.kafka010.JavaDirectKafkaStreamSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.596</duration>
      <cases>
        <case>
          <duration>2.596</duration>
          <className>org.apache.spark.streaming.kafka010.JavaDirectKafkaStreamSuite</className>
          <testName>testKafkaStream</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/external/kafka-0-10/target/test-reports/org.apache.spark.streaming.kafka010.JavaKafkaRDDSuite.xml</file>
      <name>org.apache.spark.streaming.kafka010.JavaKafkaRDDSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.48</duration>
      <cases>
        <case>
          <duration>2.48</duration>
          <className>org.apache.spark.streaming.kafka010.JavaKafkaRDDSuite</className>
          <testName>testKafkaRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/external/kafka-0-10/target/test-reports/org.apache.spark.streaming.kafka010.JavaLocationStrategySuite.xml</file>
      <name>org.apache.spark.streaming.kafka010.JavaLocationStrategySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.0</duration>
      <cases>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.kafka010.JavaLocationStrategySuite</className>
          <testName>testLocationStrategyConstructors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/external/kafka-0-10/target/test-reports/org.apache.spark.streaming.kafka010.KafkaRDDSuite.xml</file>
      <name>org.apache.spark.streaming.kafka010.KafkaRDDSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>125.314</duration>
      <cases>
        <case>
          <duration>124.677</duration>
          <className>org.apache.spark.streaming.kafka010.KafkaRDDSuite</className>
          <testName>basic usage</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.632</duration>
          <className>org.apache.spark.streaming.kafka010.KafkaRDDSuite</className>
          <testName>iterator boundary conditions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.streaming.kafka010.KafkaRDDSuite</className>
          <testName>executor sorting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/external/kafka-0-8/target/test-reports/org.apache.spark.streaming.kafka.DirectKafkaStreamSuite.xml</file>
      <name>org.apache.spark.streaming.kafka.DirectKafkaStreamSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>5.883</duration>
      <cases>
        <case>
          <duration>1.14</duration>
          <className>org.apache.spark.streaming.kafka.DirectKafkaStreamSuite</className>
          <testName>basic stream receiving with multiple topics and smallest starting offset</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.317</duration>
          <className>org.apache.spark.streaming.kafka.DirectKafkaStreamSuite</className>
          <testName>receiving from largest starting offset</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.28</duration>
          <className>org.apache.spark.streaming.kafka.DirectKafkaStreamSuite</className>
          <testName>creating stream by offset</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.697</duration>
          <className>org.apache.spark.streaming.kafka.DirectKafkaStreamSuite</className>
          <testName>offset recovery</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.369</duration>
          <className>org.apache.spark.streaming.kafka.DirectKafkaStreamSuite</className>
          <testName>Direct Kafka stream report input information</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.08</duration>
          <className>org.apache.spark.streaming.kafka.DirectKafkaStreamSuite</className>
          <testName>maxMessagesPerPartition with backpressure disabled</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.056</duration>
          <className>org.apache.spark.streaming.kafka.DirectKafkaStreamSuite</className>
          <testName>maxMessagesPerPartition with no lag</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.053</duration>
          <className>org.apache.spark.streaming.kafka.DirectKafkaStreamSuite</className>
          <testName>maxMessagesPerPartition respects max rate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.891</duration>
          <className>org.apache.spark.streaming.kafka.DirectKafkaStreamSuite</className>
          <testName>using rate controller</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/external/kafka-0-8/target/test-reports/org.apache.spark.streaming.kafka.JavaDirectKafkaStreamSuite.xml</file>
      <name>org.apache.spark.streaming.kafka.JavaDirectKafkaStreamSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.117</duration>
      <cases>
        <case>
          <duration>1.117</duration>
          <className>org.apache.spark.streaming.kafka.JavaDirectKafkaStreamSuite</className>
          <testName>testKafkaStream</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/external/kafka-0-8/target/test-reports/org.apache.spark.streaming.kafka.JavaKafkaRDDSuite.xml</file>
      <name>org.apache.spark.streaming.kafka.JavaKafkaRDDSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.005</duration>
      <cases>
        <case>
          <duration>1.005</duration>
          <className>org.apache.spark.streaming.kafka.JavaKafkaRDDSuite</className>
          <testName>testKafkaRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/external/kafka-0-8/target/test-reports/org.apache.spark.streaming.kafka.JavaKafkaStreamSuite.xml</file>
      <name>org.apache.spark.streaming.kafka.JavaKafkaStreamSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.032</duration>
      <cases>
        <case>
          <duration>2.032</duration>
          <className>org.apache.spark.streaming.kafka.JavaKafkaStreamSuite</className>
          <testName>testKafkaStream</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/external/kafka-0-8/target/test-reports/org.apache.spark.streaming.kafka.KafkaClusterSuite.xml</file>
      <name>org.apache.spark.streaming.kafka.KafkaClusterSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.062000003</duration>
      <cases>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.streaming.kafka.KafkaClusterSuite</className>
          <testName>metadata apis</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.streaming.kafka.KafkaClusterSuite</className>
          <testName>leader offset apis</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.032</duration>
          <className>org.apache.spark.streaming.kafka.KafkaClusterSuite</className>
          <testName>consumer offset apis</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/external/kafka-0-8/target/test-reports/org.apache.spark.streaming.kafka.KafkaRDDSuite.xml</file>
      <name>org.apache.spark.streaming.kafka.KafkaRDDSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.833</duration>
      <cases>
        <case>
          <duration>1.515</duration>
          <className>org.apache.spark.streaming.kafka.KafkaRDDSuite</className>
          <testName>basic usage</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.318</duration>
          <className>org.apache.spark.streaming.kafka.KafkaRDDSuite</className>
          <testName>iterator boundary conditions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/external/kafka-0-8/target/test-reports/org.apache.spark.streaming.kafka.KafkaStreamSuite.xml</file>
      <name>org.apache.spark.streaming.kafka.KafkaStreamSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.228</duration>
      <cases>
        <case>
          <duration>1.228</duration>
          <className>org.apache.spark.streaming.kafka.KafkaStreamSuite</className>
          <testName>Kafka input stream</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/external/kafka-0-8/target/test-reports/org.apache.spark.streaming.kafka.ReliableKafkaStreamSuite.xml</file>
      <name>org.apache.spark.streaming.kafka.ReliableKafkaStreamSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.322</duration>
      <cases>
        <case>
          <duration>1.37</duration>
          <className>org.apache.spark.streaming.kafka.ReliableKafkaStreamSuite</className>
          <testName>Reliable Kafka input stream with single topic</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.952</duration>
          <className>org.apache.spark.streaming.kafka.ReliableKafkaStreamSuite</className>
          <testName>Reliable Kafka input stream with multiple topics</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/external/kinesis-asl/target/test-reports/org.apache.spark.streaming.kinesis.JavaKinesisStreamSuite.xml</file>
      <name>org.apache.spark.streaming.kinesis.JavaKinesisStreamSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.412</duration>
      <cases>
        <case>
          <duration>0.295</duration>
          <className>org.apache.spark.streaming.kinesis.JavaKinesisStreamSuite</className>
          <testName>testCustomHandler</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.117</duration>
          <className>org.apache.spark.streaming.kinesis.JavaKinesisStreamSuite</className>
          <testName>testKinesisStream</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/external/kinesis-asl/target/test-reports/org.apache.spark.streaming.kinesis.KinesisCheckpointerSuite.xml</file>
      <name>org.apache.spark.streaming.kinesis.KinesisCheckpointerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.199</duration>
      <cases>
        <case>
          <duration>0.075</duration>
          <className>org.apache.spark.streaming.kinesis.KinesisCheckpointerSuite</className>
          <testName>checkpoint is not called twice for the same sequence number</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.streaming.kinesis.KinesisCheckpointerSuite</className>
          <testName>checkpoint is called after sequence number increases</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.streaming.kinesis.KinesisCheckpointerSuite</className>
          <testName>should checkpoint if we have exceeded the checkpoint interval</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.streaming.kinesis.KinesisCheckpointerSuite</className>
          <testName>shouldn&apos;t checkpoint if we have not exceeded the checkpoint interval</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.streaming.kinesis.KinesisCheckpointerSuite</className>
          <testName>should not checkpoint for the same sequence number</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.streaming.kinesis.KinesisCheckpointerSuite</className>
          <testName>removing checkpointer checkpoints one last time</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.104</duration>
          <className>org.apache.spark.streaming.kinesis.KinesisCheckpointerSuite</className>
          <testName>if checkpointing is going on, wait until finished before removing and checkpointing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/external/kinesis-asl/target/test-reports/org.apache.spark.streaming.kinesis.KinesisReceiverSuite.xml</file>
      <name>org.apache.spark.streaming.kinesis.KinesisReceiverSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.166</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.streaming.kinesis.KinesisReceiverSuite</className>
          <testName>check serializability of SerializableAWSCredentials</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.streaming.kinesis.KinesisReceiverSuite</className>
          <testName>process records including store and set checkpointer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.streaming.kinesis.KinesisReceiverSuite</className>
          <testName>shouldn&apos;t store and update checkpointer when receiver is stopped</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.streaming.kinesis.KinesisReceiverSuite</className>
          <testName>shouldn&apos;t update checkpointer when exception occurs during store</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.streaming.kinesis.KinesisReceiverSuite</className>
          <testName>shutdown should checkpoint if the reason is TERMINATE</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.streaming.kinesis.KinesisReceiverSuite</className>
          <testName>shutdown should not checkpoint if the reason is something other than TERMINATE</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.streaming.kinesis.KinesisReceiverSuite</className>
          <testName>retry success on first attempt</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.045</duration>
          <className>org.apache.spark.streaming.kinesis.KinesisReceiverSuite</className>
          <testName>retry success on second attempt after a Kinesis throttling exception</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.streaming.kinesis.KinesisReceiverSuite</className>
          <testName>retry success on second attempt after a Kinesis dependency exception</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.streaming.kinesis.KinesisReceiverSuite</className>
          <testName>retry failed after a shutdown exception</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.streaming.kinesis.KinesisReceiverSuite</className>
          <testName>retry failed after an invalid state exception</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.streaming.kinesis.KinesisReceiverSuite</className>
          <testName>retry failed after unexpected exception</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.069</duration>
          <className>org.apache.spark.streaming.kinesis.KinesisReceiverSuite</className>
          <testName>retry failed after exhausting all retries</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/external/kinesis-asl/target/test-reports/org.apache.spark.streaming.kinesis.WithAggregationKinesisBackedBlockRDDSuite.xml</file>
      <name>org.apache.spark.streaming.kinesis.WithAggregationKinesisBackedBlockRDDSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>-0.0070000007</duration>
      <cases>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.streaming.kinesis.WithAggregationKinesisBackedBlockRDDSuite</className>
          <testName>Basic reading from Kinesis [enable by setting env var ENABLE_KINESIS_TESTS=1]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.streaming.kinesis.WithAggregationKinesisBackedBlockRDDSuite</className>
          <testName>Read data available in both block manager and Kinesis [enable by setting env var ENABLE_KINESIS_TESTS=1]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.streaming.kinesis.WithAggregationKinesisBackedBlockRDDSuite</className>
          <testName>Read data available only in block manager, not in Kinesis [enable by setting env var ENABLE_KINESIS_TESTS=1]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.streaming.kinesis.WithAggregationKinesisBackedBlockRDDSuite</className>
          <testName>Read data available only in Kinesis, not in block manager [enable by setting env var ENABLE_KINESIS_TESTS=1]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.streaming.kinesis.WithAggregationKinesisBackedBlockRDDSuite</className>
          <testName>Read data available partially in block manager, rest in Kinesis [enable by setting env var ENABLE_KINESIS_TESTS=1]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.streaming.kinesis.WithAggregationKinesisBackedBlockRDDSuite</className>
          <testName>Test isBlockValid skips block fetching from block manager [enable by setting env var ENABLE_KINESIS_TESTS=1]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.streaming.kinesis.WithAggregationKinesisBackedBlockRDDSuite</className>
          <testName>Test whether RDD is valid after removing blocks from block manager [enable by setting env var ENABLE_KINESIS_TESTS=1]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/external/kinesis-asl/target/test-reports/org.apache.spark.streaming.kinesis.WithAggregationKinesisStreamSuite.xml</file>
      <name>org.apache.spark.streaming.kinesis.WithAggregationKinesisStreamSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.35300004</duration>
      <cases>
        <case>
          <duration>0.277</duration>
          <className>org.apache.spark.streaming.kinesis.WithAggregationKinesisStreamSuite</className>
          <testName>KinesisUtils API</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.08</duration>
          <className>org.apache.spark.streaming.kinesis.WithAggregationKinesisStreamSuite</className>
          <testName>RDD generation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.streaming.kinesis.WithAggregationKinesisStreamSuite</className>
          <testName>basic operation [enable by setting env var ENABLE_KINESIS_TESTS=1]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.streaming.kinesis.WithAggregationKinesisStreamSuite</className>
          <testName>custom message handling [enable by setting env var ENABLE_KINESIS_TESTS=1]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.streaming.kinesis.WithAggregationKinesisStreamSuite</className>
          <testName>failure recovery [enable by setting env var ENABLE_KINESIS_TESTS=1]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.streaming.kinesis.WithAggregationKinesisStreamSuite</className>
          <testName>Prepare KinesisTestUtils [enable by setting env var ENABLE_KINESIS_TESTS=1]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/external/kinesis-asl/target/test-reports/org.apache.spark.streaming.kinesis.WithoutAggregationKinesisBackedBlockRDDSuite.xml</file>
      <name>org.apache.spark.streaming.kinesis.WithoutAggregationKinesisBackedBlockRDDSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>-0.0070000007</duration>
      <cases>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.streaming.kinesis.WithoutAggregationKinesisBackedBlockRDDSuite</className>
          <testName>Basic reading from Kinesis [enable by setting env var ENABLE_KINESIS_TESTS=1]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.streaming.kinesis.WithoutAggregationKinesisBackedBlockRDDSuite</className>
          <testName>Read data available in both block manager and Kinesis [enable by setting env var ENABLE_KINESIS_TESTS=1]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.streaming.kinesis.WithoutAggregationKinesisBackedBlockRDDSuite</className>
          <testName>Read data available only in block manager, not in Kinesis [enable by setting env var ENABLE_KINESIS_TESTS=1]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.streaming.kinesis.WithoutAggregationKinesisBackedBlockRDDSuite</className>
          <testName>Read data available only in Kinesis, not in block manager [enable by setting env var ENABLE_KINESIS_TESTS=1]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.streaming.kinesis.WithoutAggregationKinesisBackedBlockRDDSuite</className>
          <testName>Read data available partially in block manager, rest in Kinesis [enable by setting env var ENABLE_KINESIS_TESTS=1]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.streaming.kinesis.WithoutAggregationKinesisBackedBlockRDDSuite</className>
          <testName>Test isBlockValid skips block fetching from block manager [enable by setting env var ENABLE_KINESIS_TESTS=1]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.streaming.kinesis.WithoutAggregationKinesisBackedBlockRDDSuite</className>
          <testName>Test whether RDD is valid after removing blocks from block manager [enable by setting env var ENABLE_KINESIS_TESTS=1]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/external/kinesis-asl/target/test-reports/org.apache.spark.streaming.kinesis.WithoutAggregationKinesisStreamSuite.xml</file>
      <name>org.apache.spark.streaming.kinesis.WithoutAggregationKinesisStreamSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.007999999</duration>
      <cases>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.streaming.kinesis.WithoutAggregationKinesisStreamSuite</className>
          <testName>KinesisUtils API</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.streaming.kinesis.WithoutAggregationKinesisStreamSuite</className>
          <testName>RDD generation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.streaming.kinesis.WithoutAggregationKinesisStreamSuite</className>
          <testName>basic operation [enable by setting env var ENABLE_KINESIS_TESTS=1]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.streaming.kinesis.WithoutAggregationKinesisStreamSuite</className>
          <testName>custom message handling [enable by setting env var ENABLE_KINESIS_TESTS=1]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.streaming.kinesis.WithoutAggregationKinesisStreamSuite</className>
          <testName>failure recovery [enable by setting env var ENABLE_KINESIS_TESTS=1]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.streaming.kinesis.WithoutAggregationKinesisStreamSuite</className>
          <testName>Prepare KinesisTestUtils [enable by setting env var ENABLE_KINESIS_TESTS=1]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/graphx/target/test-reports/org.apache.spark.graphx.EdgeRDDSuite.xml</file>
      <name>org.apache.spark.graphx.EdgeRDDSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.046</duration>
      <cases>
        <case>
          <duration>0.046</duration>
          <className>org.apache.spark.graphx.EdgeRDDSuite</className>
          <testName>cache, getStorageLevel</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/graphx/target/test-reports/org.apache.spark.graphx.EdgeSuite.xml</file>
      <name>org.apache.spark.graphx.EdgeSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.001</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.graphx.EdgeSuite</className>
          <testName>compare</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/graphx/target/test-reports/org.apache.spark.graphx.GraphLoaderSuite.xml</file>
      <name>org.apache.spark.graphx.GraphLoaderSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.398</duration>
      <cases>
        <case>
          <duration>0.398</duration>
          <className>org.apache.spark.graphx.GraphLoaderSuite</className>
          <testName>edgeListFile</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/graphx/target/test-reports/org.apache.spark.graphx.GraphOpsSuite.xml</file>
      <name>org.apache.spark.graphx.GraphOpsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.223</duration>
      <cases>
        <case>
          <duration>0.137</duration>
          <className>org.apache.spark.graphx.GraphOpsSuite</className>
          <testName>joinVertices</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.261</duration>
          <className>org.apache.spark.graphx.GraphOpsSuite</className>
          <testName>collectNeighborIds</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.134</duration>
          <className>org.apache.spark.graphx.GraphOpsSuite</className>
          <testName>removeSelfEdges</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.184</duration>
          <className>org.apache.spark.graphx.GraphOpsSuite</className>
          <testName>filter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.106</duration>
          <className>org.apache.spark.graphx.GraphOpsSuite</className>
          <testName>convertToCanonicalEdges</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.235</duration>
          <className>org.apache.spark.graphx.GraphOpsSuite</className>
          <testName>collectEdgesCycleDirectionOut</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.232</duration>
          <className>org.apache.spark.graphx.GraphOpsSuite</className>
          <testName>collectEdgesCycleDirectionIn</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.249</duration>
          <className>org.apache.spark.graphx.GraphOpsSuite</className>
          <testName>collectEdgesCycleDirectionEither</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.236</duration>
          <className>org.apache.spark.graphx.GraphOpsSuite</className>
          <testName>collectEdgesChainDirectionOut</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.225</duration>
          <className>org.apache.spark.graphx.GraphOpsSuite</className>
          <testName>collectEdgesChainDirectionIn</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.224</duration>
          <className>org.apache.spark.graphx.GraphOpsSuite</className>
          <testName>collectEdgesChainDirectionEither</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/graphx/target/test-reports/org.apache.spark.graphx.GraphSuite.xml</file>
      <name>org.apache.spark.graphx.GraphSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>8.267999</duration>
      <cases>
        <case>
          <duration>0.215</duration>
          <className>org.apache.spark.graphx.GraphSuite</className>
          <testName>fromEdgeTuples</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.084</duration>
          <className>org.apache.spark.graphx.GraphSuite</className>
          <testName>fromEdges</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.186</duration>
          <className>org.apache.spark.graphx.GraphSuite</className>
          <testName>apply</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.213</duration>
          <className>org.apache.spark.graphx.GraphSuite</className>
          <testName>triplets</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.883</duration>
          <className>org.apache.spark.graphx.GraphSuite</className>
          <testName>partitionBy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.184</duration>
          <className>org.apache.spark.graphx.GraphSuite</className>
          <testName>mapVertices</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.196</duration>
          <className>org.apache.spark.graphx.GraphSuite</className>
          <testName>mapVertices changing type with same erased type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.105</duration>
          <className>org.apache.spark.graphx.GraphSuite</className>
          <testName>mapEdges</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.213</duration>
          <className>org.apache.spark.graphx.GraphSuite</className>
          <testName>mapTriplets</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.197</duration>
          <className>org.apache.spark.graphx.GraphSuite</className>
          <testName>reverse</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.158</duration>
          <className>org.apache.spark.graphx.GraphSuite</className>
          <testName>reverse with join elimination</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.224</duration>
          <className>org.apache.spark.graphx.GraphSuite</className>
          <testName>subgraph</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.185</duration>
          <className>org.apache.spark.graphx.GraphSuite</className>
          <testName>mask</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.213</duration>
          <className>org.apache.spark.graphx.GraphSuite</className>
          <testName>groupEdges</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.226</duration>
          <className>org.apache.spark.graphx.GraphSuite</className>
          <testName>aggregateMessages</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.345</duration>
          <className>org.apache.spark.graphx.GraphSuite</className>
          <testName>outerJoinVertices</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.178</duration>
          <className>org.apache.spark.graphx.GraphSuite</className>
          <testName>more edge partitions than vertex partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.403</duration>
          <className>org.apache.spark.graphx.GraphSuite</className>
          <testName>checkpoint</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.064</duration>
          <className>org.apache.spark.graphx.GraphSuite</className>
          <testName>cache, getStorageLevel</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.245</duration>
          <className>org.apache.spark.graphx.GraphSuite</className>
          <testName>non-default number of edge partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.403</duration>
          <className>org.apache.spark.graphx.GraphSuite</className>
          <testName>unpersist graph RDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.148</duration>
          <className>org.apache.spark.graphx.GraphSuite</className>
          <testName>SPARK-14219: pickRandomVertex</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/graphx/target/test-reports/org.apache.spark.graphx.PregelSuite.xml</file>
      <name>org.apache.spark.graphx.PregelSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.401</duration>
      <cases>
        <case>
          <duration>0.332</duration>
          <className>org.apache.spark.graphx.PregelSuite</className>
          <testName>1 iteration</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.069</duration>
          <className>org.apache.spark.graphx.PregelSuite</className>
          <testName>chain propagation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/graphx/target/test-reports/org.apache.spark.graphx.VertexRDDSuite.xml</file>
      <name>org.apache.spark.graphx.VertexRDDSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>3.1260002</duration>
      <cases>
        <case>
          <duration>0.202</duration>
          <className>org.apache.spark.graphx.VertexRDDSuite</className>
          <testName>filter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.199</duration>
          <className>org.apache.spark.graphx.VertexRDDSuite</className>
          <testName>mapValues</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.168</duration>
          <className>org.apache.spark.graphx.VertexRDDSuite</className>
          <testName>minus</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.18</duration>
          <className>org.apache.spark.graphx.VertexRDDSuite</className>
          <testName>minus with RDD[(VertexId, VD)]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.278</duration>
          <className>org.apache.spark.graphx.VertexRDDSuite</className>
          <testName>minus with non-equal number of partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.234</duration>
          <className>org.apache.spark.graphx.VertexRDDSuite</className>
          <testName>diff</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.238</duration>
          <className>org.apache.spark.graphx.VertexRDDSuite</className>
          <testName>diff with RDD[(VertexId, VD)]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.187</duration>
          <className>org.apache.spark.graphx.VertexRDDSuite</className>
          <testName>diff vertices with non-equal number of partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.295</duration>
          <className>org.apache.spark.graphx.VertexRDDSuite</className>
          <testName>leftJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.209</duration>
          <className>org.apache.spark.graphx.VertexRDDSuite</className>
          <testName>leftJoin vertices with non-equal number of partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.327</duration>
          <className>org.apache.spark.graphx.VertexRDDSuite</className>
          <testName>innerJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.191</duration>
          <className>org.apache.spark.graphx.VertexRDDSuite</className>
          <testName>innerJoin vertices with the non-equal number of partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.229</duration>
          <className>org.apache.spark.graphx.VertexRDDSuite</className>
          <testName>aggregateUsingIndex</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.124</duration>
          <className>org.apache.spark.graphx.VertexRDDSuite</className>
          <testName>mergeFunc</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.065</duration>
          <className>org.apache.spark.graphx.VertexRDDSuite</className>
          <testName>cache, getStorageLevel</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/graphx/target/test-reports/org.apache.spark.graphx.impl.EdgePartitionSuite.xml</file>
      <name>org.apache.spark.graphx.impl.EdgePartitionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.033</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.graphx.impl.EdgePartitionSuite</className>
          <testName>reverse</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.graphx.impl.EdgePartitionSuite</className>
          <testName>map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.graphx.impl.EdgePartitionSuite</className>
          <testName>filter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.graphx.impl.EdgePartitionSuite</className>
          <testName>groupEdges</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.graphx.impl.EdgePartitionSuite</className>
          <testName>innerJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.graphx.impl.EdgePartitionSuite</className>
          <testName>isActive, numActives, replaceActives</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.graphx.impl.EdgePartitionSuite</className>
          <testName>tripletIterator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.graphx.impl.EdgePartitionSuite</className>
          <testName>serialization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/graphx/target/test-reports/org.apache.spark.graphx.impl.VertexPartitionSuite.xml</file>
      <name>org.apache.spark.graphx.impl.VertexPartitionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.016</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.graphx.impl.VertexPartitionSuite</className>
          <testName>isDefined, filter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.graphx.impl.VertexPartitionSuite</className>
          <testName>map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.graphx.impl.VertexPartitionSuite</className>
          <testName>diff</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.graphx.impl.VertexPartitionSuite</className>
          <testName>leftJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.graphx.impl.VertexPartitionSuite</className>
          <testName>innerJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.graphx.impl.VertexPartitionSuite</className>
          <testName>createUsingIndex</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.graphx.impl.VertexPartitionSuite</className>
          <testName>innerJoinKeepLeft</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.graphx.impl.VertexPartitionSuite</className>
          <testName>aggregateUsingIndex</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.graphx.impl.VertexPartitionSuite</className>
          <testName>reindex</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.graphx.impl.VertexPartitionSuite</className>
          <testName>serialization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/graphx/target/test-reports/org.apache.spark.graphx.lib.ConnectedComponentsSuite.xml</file>
      <name>org.apache.spark.graphx.lib.ConnectedComponentsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>7.533</duration>
      <cases>
        <case>
          <duration>1.626</duration>
          <className>org.apache.spark.graphx.lib.ConnectedComponentsSuite</className>
          <testName>Grid Connected Components</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.733</duration>
          <className>org.apache.spark.graphx.lib.ConnectedComponentsSuite</className>
          <testName>Reverse Grid Connected Components</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.929</duration>
          <className>org.apache.spark.graphx.lib.ConnectedComponentsSuite</className>
          <testName>Chain Connected Components</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.928</duration>
          <className>org.apache.spark.graphx.lib.ConnectedComponentsSuite</className>
          <testName>Reverse Chain Connected Components</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.317</duration>
          <className>org.apache.spark.graphx.lib.ConnectedComponentsSuite</className>
          <testName>Connected Components on a Toy Connected Graph</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/graphx/target/test-reports/org.apache.spark.graphx.lib.LabelPropagationSuite.xml</file>
      <name>org.apache.spark.graphx.lib.LabelPropagationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.983</duration>
      <cases>
        <case>
          <duration>1.983</duration>
          <className>org.apache.spark.graphx.lib.LabelPropagationSuite</className>
          <testName>Label Propagation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/graphx/target/test-reports/org.apache.spark.graphx.lib.PageRankSuite.xml</file>
      <name>org.apache.spark.graphx.lib.PageRankSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>21.23</duration>
      <cases>
        <case>
          <duration>5.183</duration>
          <className>org.apache.spark.graphx.lib.PageRankSuite</className>
          <testName>Star PageRank</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.956</duration>
          <className>org.apache.spark.graphx.lib.PageRankSuite</className>
          <testName>Star PersonalPageRank</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>8.168</duration>
          <className>org.apache.spark.graphx.lib.PageRankSuite</className>
          <testName>Grid PageRank</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.785</duration>
          <className>org.apache.spark.graphx.lib.PageRankSuite</className>
          <testName>Chain PageRank</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.138</duration>
          <className>org.apache.spark.graphx.lib.PageRankSuite</className>
          <testName>Chain PersonalizedPageRank</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/graphx/target/test-reports/org.apache.spark.graphx.lib.SVDPlusPlusSuite.xml</file>
      <name>org.apache.spark.graphx.lib.SVDPlusPlusSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.678</duration>
      <cases>
        <case>
          <duration>0.678</duration>
          <className>org.apache.spark.graphx.lib.SVDPlusPlusSuite</className>
          <testName>Test SVD++ with mean square error on training set</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/graphx/target/test-reports/org.apache.spark.graphx.lib.ShortestPathsSuite.xml</file>
      <name>org.apache.spark.graphx.lib.ShortestPathsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.466</duration>
      <cases>
        <case>
          <duration>0.466</duration>
          <className>org.apache.spark.graphx.lib.ShortestPathsSuite</className>
          <testName>Shortest Path Computations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/graphx/target/test-reports/org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite.xml</file>
      <name>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>3.913</duration>
      <cases>
        <case>
          <duration>0.648</duration>
          <className>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite</className>
          <testName>Island Strongly Connected Components</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.954</duration>
          <className>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite</className>
          <testName>Cycle Strongly Connected Components</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.311</duration>
          <className>org.apache.spark.graphx.lib.StronglyConnectedComponentsSuite</className>
          <testName>2 Cycle Strongly Connected Components</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/graphx/target/test-reports/org.apache.spark.graphx.lib.TriangleCountSuite.xml</file>
      <name>org.apache.spark.graphx.lib.TriangleCountSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.583</duration>
      <cases>
        <case>
          <duration>0.457</duration>
          <className>org.apache.spark.graphx.lib.TriangleCountSuite</className>
          <testName>Count a single triangle</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.389</duration>
          <className>org.apache.spark.graphx.lib.TriangleCountSuite</className>
          <testName>Count two triangles</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.341</duration>
          <className>org.apache.spark.graphx.lib.TriangleCountSuite</className>
          <testName>Count two triangles with bi-directed edges</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.396</duration>
          <className>org.apache.spark.graphx.lib.TriangleCountSuite</className>
          <testName>Count a single triangle with duplicate edges</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/graphx/target/test-reports/org.apache.spark.graphx.util.BytecodeUtilsSuite.xml</file>
      <name>org.apache.spark.graphx.util.BytecodeUtilsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.213</duration>
      <cases>
        <case>
          <duration>0.18</duration>
          <className>org.apache.spark.graphx.util.BytecodeUtilsSuite</className>
          <testName>closure invokes a method</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.graphx.util.BytecodeUtilsSuite</className>
          <testName>closure inside a closure invokes a method</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.graphx.util.BytecodeUtilsSuite</className>
          <testName>closure inside a closure inside a closure invokes a method</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.graphx.util.BytecodeUtilsSuite</className>
          <testName>closure calling a function that invokes a method</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.graphx.util.BytecodeUtilsSuite</className>
          <testName>closure calling a function that invokes a method which uses another closure</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.graphx.util.BytecodeUtilsSuite</className>
          <testName>nested closure</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/graphx/target/test-reports/org.apache.spark.graphx.util.GraphGeneratorsSuite.xml</file>
      <name>org.apache.spark.graphx.util.GraphGeneratorsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.601</duration>
      <cases>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.graphx.util.GraphGeneratorsSuite</className>
          <testName>generateRandomEdges</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.graphx.util.GraphGeneratorsSuite</className>
          <testName>sampleLogNormal</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.485</duration>
          <className>org.apache.spark.graphx.util.GraphGeneratorsSuite</className>
          <testName>logNormalGraph</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.104</duration>
          <className>org.apache.spark.graphx.util.GraphGeneratorsSuite</className>
          <testName>rmatGraph numEdges upper bound</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/launcher/target/test-reports/org.apache.spark.launcher.CommandBuilderUtilsSuite.xml</file>
      <name>org.apache.spark.launcher.CommandBuilderUtilsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.002</duration>
      <cases>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.launcher.CommandBuilderUtilsSuite</className>
          <testName>testAddPermGenSizeOpt</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.launcher.CommandBuilderUtilsSuite</className>
          <testName>testValidOptionStrings</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.launcher.CommandBuilderUtilsSuite</className>
          <testName>testJavaMajorVersion</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.launcher.CommandBuilderUtilsSuite</className>
          <testName>testPythonArgQuoting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.launcher.CommandBuilderUtilsSuite</className>
          <testName>testWindowsBatchQuoting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.launcher.CommandBuilderUtilsSuite</className>
          <testName>testInvalidOptionStrings</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/launcher/target/test-reports/org.apache.spark.launcher.LauncherServerSuite.xml</file>
      <name>org.apache.spark.launcher.LauncherServerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.379</duration>
      <cases>
        <case>
          <duration>0.219</duration>
          <className>org.apache.spark.launcher.LauncherServerSuite</className>
          <testName>testTimeout</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.054</duration>
          <className>org.apache.spark.launcher.LauncherServerSuite</className>
          <testName>testSparkSubmitVmShutsDown</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.018</duration>
          <className>org.apache.spark.launcher.LauncherServerSuite</className>
          <testName>testLauncherServerReuse</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.088</duration>
          <className>org.apache.spark.launcher.LauncherServerSuite</className>
          <testName>testCommunication</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/launcher/target/test-reports/org.apache.spark.launcher.SparkSubmitCommandBuilderSuite.xml</file>
      <name>org.apache.spark.launcher.SparkSubmitCommandBuilderSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.156</duration>
      <cases>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.launcher.SparkSubmitCommandBuilderSuite</className>
          <testName>testCliParser</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.launcher.SparkSubmitCommandBuilderSuite</className>
          <testName>testPySparkLauncher</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.launcher.SparkSubmitCommandBuilderSuite</className>
          <testName>testAlternateSyntaxParsing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.launcher.SparkSubmitCommandBuilderSuite</className>
          <testName>testExamplesRunner</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.launcher.SparkSubmitCommandBuilderSuite</className>
          <testName>testSparkRShell</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.launcher.SparkSubmitCommandBuilderSuite</className>
          <testName>testMissingAppResource</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.launcher.SparkSubmitCommandBuilderSuite</className>
          <testName>testShellCliParser</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.launcher.SparkSubmitCommandBuilderSuite</className>
          <testName>testClusterCmdBuilder</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.launcher.SparkSubmitCommandBuilderSuite</className>
          <testName>testDriverCmdBuilder</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.launcher.SparkSubmitCommandBuilderSuite</className>
          <testName>testCliKillAndStatus</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.launcher.SparkSubmitCommandBuilderSuite</className>
          <testName>testPySparkFallback</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.launcher.SparkSubmitCommandBuilderSuite</className>
          <testName>testCliHelpAndNoArg</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/launcher/target/test-reports/org.apache.spark.launcher.SparkSubmitOptionParserSuite.xml</file>
      <name>org.apache.spark.launcher.SparkSubmitOptionParserSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.365</duration>
      <cases>
        <case>
          <duration>0.225</duration>
          <className>org.apache.spark.launcher.SparkSubmitOptionParserSuite</className>
          <testName>testMissingArg</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.136</duration>
          <className>org.apache.spark.launcher.SparkSubmitOptionParserSuite</className>
          <testName>testAllOptions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.launcher.SparkSubmitOptionParserSuite</className>
          <testName>testEqualSeparatedOption</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.launcher.SparkSubmitOptionParserSuite</className>
          <testName>testExtraOptions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib-local/target/test-reports/org.apache.spark.ml.impl.UtilsSuite.xml</file>
      <name>org.apache.spark.ml.impl.UtilsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.0</duration>
      <cases>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.impl.UtilsSuite</className>
          <testName>EPSILON</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib-local/target/test-reports/org.apache.spark.ml.linalg.BLASSuite.xml</file>
      <name>org.apache.spark.ml.linalg.BLASSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.029000001</duration>
      <cases>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.ml.linalg.BLASSuite</className>
          <testName>copy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.linalg.BLASSuite</className>
          <testName>scal</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.linalg.BLASSuite</className>
          <testName>axpy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.linalg.BLASSuite</className>
          <testName>dot</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.linalg.BLASSuite</className>
          <testName>spr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.ml.linalg.BLASSuite</className>
          <testName>syr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.ml.linalg.BLASSuite</className>
          <testName>gemm</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.ml.linalg.BLASSuite</className>
          <testName>gemv</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.linalg.BLASSuite</className>
          <testName>spmv</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib-local/target/test-reports/org.apache.spark.ml.linalg.BreezeMatrixConversionSuite.xml</file>
      <name>org.apache.spark.ml.linalg.BreezeMatrixConversionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.002</duration>
      <cases>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.linalg.BreezeMatrixConversionSuite</className>
          <testName>dense matrix to breeze</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.linalg.BreezeMatrixConversionSuite</className>
          <testName>dense breeze matrix to matrix</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.linalg.BreezeMatrixConversionSuite</className>
          <testName>sparse matrix to breeze</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.linalg.BreezeMatrixConversionSuite</className>
          <testName>sparse breeze matrix to sparse matrix</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib-local/target/test-reports/org.apache.spark.ml.linalg.BreezeVectorConversionSuite.xml</file>
      <name>org.apache.spark.ml.linalg.BreezeVectorConversionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.49</duration>
      <cases>
        <case>
          <duration>0.347</duration>
          <className>org.apache.spark.ml.linalg.BreezeVectorConversionSuite</className>
          <testName>dense to breeze</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.142</duration>
          <className>org.apache.spark.ml.linalg.BreezeVectorConversionSuite</className>
          <testName>sparse to breeze</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.linalg.BreezeVectorConversionSuite</className>
          <testName>dense breeze to vector</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.linalg.BreezeVectorConversionSuite</className>
          <testName>sparse breeze to vector</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.linalg.BreezeVectorConversionSuite</className>
          <testName>sparse breeze with partially-used arrays to vector</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib-local/target/test-reports/org.apache.spark.ml.linalg.MatricesSuite.xml</file>
      <name>org.apache.spark.ml.linalg.MatricesSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.44799998</duration>
      <cases>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.ml.linalg.MatricesSuite</className>
          <testName>dense matrix construction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.linalg.MatricesSuite</className>
          <testName>dense matrix construction with wrong dimension</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.164</duration>
          <className>org.apache.spark.ml.linalg.MatricesSuite</className>
          <testName>sparse matrix construction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.linalg.MatricesSuite</className>
          <testName>sparse matrix construction with wrong number of elements</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.ml.linalg.MatricesSuite</className>
          <testName>index in matrices incorrect input</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.ml.linalg.MatricesSuite</className>
          <testName>equals</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.linalg.MatricesSuite</className>
          <testName>matrix copies are deep copies</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.linalg.MatricesSuite</className>
          <testName>matrix indexing and updating</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.linalg.MatricesSuite</className>
          <testName>toSparse, toDense</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.ml.linalg.MatricesSuite</className>
          <testName>map, update</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.linalg.MatricesSuite</className>
          <testName>transpose</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.linalg.MatricesSuite</className>
          <testName>foreachActive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.ml.linalg.MatricesSuite</className>
          <testName>horzcat, vertcat, eye, speye</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.linalg.MatricesSuite</className>
          <testName>zeros</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.linalg.MatricesSuite</className>
          <testName>ones</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.linalg.MatricesSuite</className>
          <testName>eye</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.152</duration>
          <className>org.apache.spark.ml.linalg.MatricesSuite</className>
          <testName>rand</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.linalg.MatricesSuite</className>
          <testName>randn</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.linalg.MatricesSuite</className>
          <testName>diag</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.ml.linalg.MatricesSuite</className>
          <testName>sprand</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.ml.linalg.MatricesSuite</className>
          <testName>sprandn</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.ml.linalg.MatricesSuite</className>
          <testName>toString</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.linalg.MatricesSuite</className>
          <testName>numNonzeros and numActives</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.ml.linalg.MatricesSuite</className>
          <testName>fromBreeze with sparse matrix</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.ml.linalg.MatricesSuite</className>
          <testName>row/col iterator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib-local/target/test-reports/org.apache.spark.ml.linalg.VectorsSuite.xml</file>
      <name>org.apache.spark.ml.linalg.VectorsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.15100002</duration>
      <cases>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.linalg.VectorsSuite</className>
          <testName>dense vector construction with varargs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.linalg.VectorsSuite</className>
          <testName>dense vector construction from a double array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.linalg.VectorsSuite</className>
          <testName>sparse vector construction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.linalg.VectorsSuite</className>
          <testName>sparse vector construction with unordered elements</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.linalg.VectorsSuite</className>
          <testName>sparse vector construction with mismatched indices/values array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.linalg.VectorsSuite</className>
          <testName>sparse vector construction with too many indices vs size</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.linalg.VectorsSuite</className>
          <testName>sparse vector construction with negative indices</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.linalg.VectorsSuite</className>
          <testName>dense to array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.linalg.VectorsSuite</className>
          <testName>dense argmax</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.linalg.VectorsSuite</className>
          <testName>sparse to array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.linalg.VectorsSuite</className>
          <testName>sparse argmax</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.ml.linalg.VectorsSuite</className>
          <testName>vector equals</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.linalg.VectorsSuite</className>
          <testName>vectors equals with explicit 0</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.linalg.VectorsSuite</className>
          <testName>indexing dense vectors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.linalg.VectorsSuite</className>
          <testName>indexing sparse vectors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.linalg.VectorsSuite</className>
          <testName>zeros</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.linalg.VectorsSuite</className>
          <testName>copy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.linalg.VectorsSuite</className>
          <testName>fromBreeze</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.123</duration>
          <className>org.apache.spark.ml.linalg.VectorsSuite</className>
          <testName>sqdist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.ml.linalg.VectorsSuite</className>
          <testName>foreachActive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.ml.linalg.VectorsSuite</className>
          <testName>vector p-norm</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.linalg.VectorsSuite</className>
          <testName>Vector numActive and numNonzeros</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.linalg.VectorsSuite</className>
          <testName>Vector toSparse and toDense</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.linalg.VectorsSuite</className>
          <testName>compressed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.linalg.VectorsSuite</className>
          <testName>slice</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib-local/target/test-reports/org.apache.spark.ml.stat.distribution.MultivariateGaussianSuite.xml</file>
      <name>org.apache.spark.ml.stat.distribution.MultivariateGaussianSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.15</duration>
      <cases>
        <case>
          <duration>0.126</duration>
          <className>org.apache.spark.ml.stat.distribution.MultivariateGaussianSuite</className>
          <testName>univariate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.ml.stat.distribution.MultivariateGaussianSuite</className>
          <testName>multivariate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.stat.distribution.MultivariateGaussianSuite</className>
          <testName>multivariate degenerate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.ml.stat.distribution.MultivariateGaussianSuite</className>
          <testName>SPARK-11302</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib-local/target/test-reports/org.apache.spark.ml.util.TestingUtilsSuite.xml</file>
      <name>org.apache.spark.ml.util.TestingUtilsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.408</duration>
      <cases>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.ml.util.TestingUtilsSuite</className>
          <testName>Comparing doubles using relative error</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.util.TestingUtilsSuite</className>
          <testName>Comparing doubles using absolute error</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.018</duration>
          <className>org.apache.spark.ml.util.TestingUtilsSuite</className>
          <testName>Comparing vectors using relative error</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.ml.util.TestingUtilsSuite</className>
          <testName>Comparing vectors using absolute error</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.347</duration>
          <className>org.apache.spark.ml.util.TestingUtilsSuite</className>
          <testName>Comparing Matrices using absolute error</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.ml.util.TestingUtilsSuite</className>
          <testName>Comparing Matrices using relative error</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.JavaPipelineSuite.xml</file>
      <name>org.apache.spark.ml.JavaPipelineSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.368</duration>
      <cases>
        <case>
          <duration>0.368</duration>
          <className>org.apache.spark.ml.JavaPipelineSuite</className>
          <testName>pipeline</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.PipelineSuite.xml</file>
      <name>org.apache.spark.ml.PipelineSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.919</duration>
      <cases>
        <case>
          <duration>0.11</duration>
          <className>org.apache.spark.ml.PipelineSuite</className>
          <testName>pipeline</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.PipelineSuite</className>
          <testName>pipeline with duplicate stages</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.PipelineSuite</className>
          <testName>copy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.PipelineSuite</className>
          <testName>copy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.PipelineSuite</className>
          <testName>pipeline model constructors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.373</duration>
          <className>org.apache.spark.ml.PipelineSuite</className>
          <testName>Pipeline read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.PipelineSuite</className>
          <testName>Pipeline read/write with non-Writable stage</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.398</duration>
          <className>org.apache.spark.ml.PipelineSuite</className>
          <testName>PipelineModel read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.PipelineSuite</className>
          <testName>PipelineModel read/write: getStagePath</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.PipelineSuite</className>
          <testName>PipelineModel read/write with non-Writable stage</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.032</duration>
          <className>org.apache.spark.ml.PipelineSuite</className>
          <testName>pipeline validateParams</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.PipelineSuite</className>
          <testName>setStages should handle Java Arrays being non-covariant</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.PredictorSuite.xml</file>
      <name>org.apache.spark.ml.PredictorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.037</duration>
      <cases>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.ml.PredictorSuite</className>
          <testName>should support all NumericType labels and not support other types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.ann.ANNSuite.xml</file>
      <name>org.apache.spark.ml.ann.ANNSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.526</duration>
      <cases>
        <case>
          <duration>0.289</duration>
          <className>org.apache.spark.ml.ann.ANNSuite</className>
          <testName>ANN with Sigmoid learns XOR function with LBFGS optimizer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.237</duration>
          <className>org.apache.spark.ml.ann.ANNSuite</className>
          <testName>ANN with SoftMax learns XOR function with 2-bit output and batch GD optimizer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.ann.GradientSuite.xml</file>
      <name>org.apache.spark.ml.ann.GradientSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.004</duration>
      <cases>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.ml.ann.GradientSuite</className>
          <testName>Gradient computation against numerical differentiation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.attribute.AttributeGroupSuite.xml</file>
      <name>org.apache.spark.ml.attribute.AttributeGroupSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.046</duration>
      <cases>
        <case>
          <duration>0.045</duration>
          <className>org.apache.spark.ml.attribute.AttributeGroupSuite</className>
          <testName>attribute group</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.attribute.AttributeGroupSuite</className>
          <testName>attribute group without attributes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.attribute.AttributeSuite.xml</file>
      <name>org.apache.spark.ml.attribute.AttributeSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.010000001</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.attribute.AttributeSuite</className>
          <testName>default numeric attribute</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.attribute.AttributeSuite</className>
          <testName>customized numeric attribute</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.attribute.AttributeSuite</className>
          <testName>bad numeric attributes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.attribute.AttributeSuite</className>
          <testName>default nominal attribute</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.attribute.AttributeSuite</className>
          <testName>customized nominal attribute</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.attribute.AttributeSuite</className>
          <testName>bad nominal attributes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.attribute.AttributeSuite</className>
          <testName>default binary attribute</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.attribute.AttributeSuite</className>
          <testName>customized binary attribute</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.attribute.AttributeSuite</className>
          <testName>bad binary attributes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.attribute.AttributeSuite</className>
          <testName>attribute from struct field</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.attribute.JavaAttributeGroupSuite.xml</file>
      <name>org.apache.spark.ml.attribute.JavaAttributeGroupSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.0</duration>
      <cases>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.attribute.JavaAttributeGroupSuite</className>
          <testName>testAttributeGroup</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.attribute.JavaAttributeSuite.xml</file>
      <name>org.apache.spark.ml.attribute.JavaAttributeSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.0</duration>
      <cases>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.attribute.JavaAttributeSuite</className>
          <testName>testBinaryAttribute</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.attribute.JavaAttributeSuite</className>
          <testName>testNominalAttribute</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.attribute.JavaAttributeSuite</className>
          <testName>testAttributeType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.attribute.JavaAttributeSuite</className>
          <testName>testNumericAttribute</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.classification.ClassifierSuite.xml</file>
      <name>org.apache.spark.ml.classification.ClassifierSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.315</duration>
      <cases>
        <case>
          <duration>0.145</duration>
          <className>org.apache.spark.ml.classification.ClassifierSuite</className>
          <testName>extractLabeledPoints</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.17</duration>
          <className>org.apache.spark.ml.classification.ClassifierSuite</className>
          <testName>getNumClasses</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.classification.DecisionTreeClassifierSuite.xml</file>
      <name>org.apache.spark.ml.classification.DecisionTreeClassifierSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>7.879999</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.classification.DecisionTreeClassifierSuite</className>
          <testName>params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.209</duration>
          <className>org.apache.spark.ml.classification.DecisionTreeClassifierSuite</className>
          <testName>Binary classification stump with ordered categorical features</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.868</duration>
          <className>org.apache.spark.ml.classification.DecisionTreeClassifierSuite</className>
          <testName>Binary classification stump with fixed labels 0,1 for Entropy,Gini</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.211</duration>
          <className>org.apache.spark.ml.classification.DecisionTreeClassifierSuite</className>
          <testName>Multiclass classification stump with 3-ary (unordered) categorical features</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.18</duration>
          <className>org.apache.spark.ml.classification.DecisionTreeClassifierSuite</className>
          <testName>Binary classification stump with 1 continuous feature, to check off-by-1 error</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.17</duration>
          <className>org.apache.spark.ml.classification.DecisionTreeClassifierSuite</className>
          <testName>Binary classification stump with 2 continuous features</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.256</duration>
          <className>org.apache.spark.ml.classification.DecisionTreeClassifierSuite</className>
          <testName>Multiclass classification stump with unordered categorical features, with just enough bins</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.413</duration>
          <className>org.apache.spark.ml.classification.DecisionTreeClassifierSuite</className>
          <testName>Multiclass classification stump with continuous features</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.397</duration>
          <className>org.apache.spark.ml.classification.DecisionTreeClassifierSuite</className>
          <testName>Multiclass classification stump with continuous + unordered categorical features</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.293</duration>
          <className>org.apache.spark.ml.classification.DecisionTreeClassifierSuite</className>
          <testName>Multiclass classification stump with 10-ary (ordered) categorical features</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.265</duration>
          <className>org.apache.spark.ml.classification.DecisionTreeClassifierSuite</className>
          <testName>Multiclass classification tree with 10-ary (ordered) categorical features, with just enough bins</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.197</duration>
          <className>org.apache.spark.ml.classification.DecisionTreeClassifierSuite</className>
          <testName>split must satisfy min instances per node requirements</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.198</duration>
          <className>org.apache.spark.ml.classification.DecisionTreeClassifierSuite</className>
          <testName>do not choose split that does not satisfy min instance per node requirements</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.206</duration>
          <className>org.apache.spark.ml.classification.DecisionTreeClassifierSuite</className>
          <testName>split must satisfy min info gain requirements</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.321</duration>
          <className>org.apache.spark.ml.classification.DecisionTreeClassifierSuite</className>
          <testName>predictRaw and predictProbability</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.12</duration>
          <className>org.apache.spark.ml.classification.DecisionTreeClassifierSuite</className>
          <testName>training with 1-category categorical feature</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.088</duration>
          <className>org.apache.spark.ml.classification.DecisionTreeClassifierSuite</className>
          <testName>Use soft prediction for binary classification with ordered categorical features</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.097</duration>
          <className>org.apache.spark.ml.classification.DecisionTreeClassifierSuite</className>
          <testName>Feature importance with toy data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.594</duration>
          <className>org.apache.spark.ml.classification.DecisionTreeClassifierSuite</className>
          <testName>should support all NumericType labels and not support other types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.14</duration>
          <className>org.apache.spark.ml.classification.DecisionTreeClassifierSuite</className>
          <testName>Fitting without numClasses in metadata</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.655</duration>
          <className>org.apache.spark.ml.classification.DecisionTreeClassifierSuite</className>
          <testName>read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.classification.GBTClassifierSuite.xml</file>
      <name>org.apache.spark.ml.classification.GBTClassifierSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>29.189001</duration>
      <cases>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.ml.classification.GBTClassifierSuite</className>
          <testName>params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>10.302</duration>
          <className>org.apache.spark.ml.classification.GBTClassifierSuite</className>
          <testName>Binary classification with continuous features: Log Loss</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.812</duration>
          <className>org.apache.spark.ml.classification.GBTClassifierSuite</className>
          <testName>Checkpointing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>12.833</duration>
          <className>org.apache.spark.ml.classification.GBTClassifierSuite</className>
          <testName>should support all NumericType labels and not support other types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.106</duration>
          <className>org.apache.spark.ml.classification.GBTClassifierSuite</className>
          <testName>Fitting without numClasses in metadata</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.16</duration>
          <className>org.apache.spark.ml.classification.GBTClassifierSuite</className>
          <testName>extractLabeledPoints with bad data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.537</duration>
          <className>org.apache.spark.ml.classification.GBTClassifierSuite</className>
          <testName>Feature importance with toy data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.428</duration>
          <className>org.apache.spark.ml.classification.GBTClassifierSuite</className>
          <testName>model save/load</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite.xml</file>
      <name>org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.244</duration>
      <cases>
        <case>
          <duration>0.244</duration>
          <className>org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite</className>
          <testName>runDT</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.classification.JavaGBTClassifierSuite.xml</file>
      <name>org.apache.spark.ml.classification.JavaGBTClassifierSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.433</duration>
      <cases>
        <case>
          <duration>0.433</duration>
          <className>org.apache.spark.ml.classification.JavaGBTClassifierSuite</className>
          <testName>runDT</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.classification.JavaLogisticRegressionSuite.xml</file>
      <name>org.apache.spark.ml.classification.JavaLogisticRegressionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.295</duration>
      <cases>
        <case>
          <duration>0.488</duration>
          <className>org.apache.spark.ml.classification.JavaLogisticRegressionSuite</className>
          <testName>logisticRegressionWithSetters</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.208</duration>
          <className>org.apache.spark.ml.classification.JavaLogisticRegressionSuite</className>
          <testName>logisticRegressionTrainingSummary</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.311</duration>
          <className>org.apache.spark.ml.classification.JavaLogisticRegressionSuite</className>
          <testName>logisticRegressionPredictorClassifierMethods</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.288</duration>
          <className>org.apache.spark.ml.classification.JavaLogisticRegressionSuite</className>
          <testName>logisticRegressionDefaultParams</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite.xml</file>
      <name>org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.672</duration>
      <cases>
        <case>
          <duration>0.672</duration>
          <className>org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite</className>
          <testName>testMLPC</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.classification.JavaNaiveBayesSuite.xml</file>
      <name>org.apache.spark.ml.classification.JavaNaiveBayesSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.286</duration>
      <cases>
        <case>
          <duration>0.218</duration>
          <className>org.apache.spark.ml.classification.JavaNaiveBayesSuite</className>
          <testName>testNaiveBayes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.068</duration>
          <className>org.apache.spark.ml.classification.JavaNaiveBayesSuite</className>
          <testName>naiveBayesDefaultParams</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.classification.JavaOneVsRestSuite.xml</file>
      <name>org.apache.spark.ml.classification.JavaOneVsRestSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.968</duration>
      <cases>
        <case>
          <duration>0.968</duration>
          <className>org.apache.spark.ml.classification.JavaOneVsRestSuite</className>
          <testName>oneVsRestDefaultParams</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.classification.JavaRandomForestClassifierSuite.xml</file>
      <name>org.apache.spark.ml.classification.JavaRandomForestClassifierSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.233</duration>
      <cases>
        <case>
          <duration>0.233</duration>
          <className>org.apache.spark.ml.classification.JavaRandomForestClassifierSuite</className>
          <testName>runDT</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.classification.LogisticRegressionSuite.xml</file>
      <name>org.apache.spark.ml.classification.LogisticRegressionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>219.56902</duration>
      <cases>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>export test data into CSV format</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.229</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>logistic regression: default params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.168</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>empty probabilityCol</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.156</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>setThreshold, getThreshold</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.321</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>thresholds prediction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.499</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>logistic regression doesn&apos;t fit intercept when fitIntercept is off</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.35</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>logistic regression with setters</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.129</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>multinomial logistic regression: Predictor, Classifier methods</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.237</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>binary logistic regression: Predictor, Classifier methods</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.167</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>coefficients and intercept methods</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>overflow prediction for multiclass</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>MultiClassSummarizer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>MultiClassSummarizer with weighted samples</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.125</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>binary logistic regression with intercept without regularization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.958</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>binary logistic regression without intercept without regularization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.725</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>binary logistic regression with intercept with L1 regularization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.285</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>binary logistic regression without intercept with L1 regularization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.697</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>binary logistic regression with intercept with L2 regularization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.154</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>binary logistic regression without intercept with L2 regularization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>25.766</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>binary logistic regression with intercept with ElasticNet regularization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>7.303</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>binary logistic regression without intercept with ElasticNet regularization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.487</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>binary logistic regression with intercept with strong L1 regularization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.93</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>multinomial logistic regression with intercept with strong L1 regularization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>5.444</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>multinomial logistic regression with intercept without regularization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.145</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>multinomial logistic regression without intercept without regularization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>54.271</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>multinomial logistic regression with intercept with L1 regularization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>11.332</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>multinomial logistic regression without intercept with L1 regularization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.849</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>multinomial logistic regression with intercept with L2 regularization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.675</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>multinomial logistic regression without intercept with L2 regularization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>48.927</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>multinomial logistic regression with intercept with elasticnet regularization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>26.912</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>multinomial logistic regression without intercept with elasticnet regularization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.73</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>evaluate on test set</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.349</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>evaluate with labels that are not doubles</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.159</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>statistics on training data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.691</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>binary logistic regression with weighted data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.607</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>multinomial logistic regression with weighted data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.225</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>set family</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.154</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>set initial model</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.376</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>binary logistic regression with all labels the same</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.221</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>multiclass logistic regression with all labels the same</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.119</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>compressed storage</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.194</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>numClasses specified in metadata/inferred</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.821</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.649</duration>
          <className>org.apache.spark.ml.classification.LogisticRegressionSuite</className>
          <testName>should support all NumericType labels and not support other types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.classification.MultilayerPerceptronClassifierSuite.xml</file>
      <name>org.apache.spark.ml.classification.MultilayerPerceptronClassifierSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>6.18</duration>
      <cases>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.ml.classification.MultilayerPerceptronClassifierSuite</className>
          <testName>Input Validation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.676</duration>
          <className>org.apache.spark.ml.classification.MultilayerPerceptronClassifierSuite</className>
          <testName>XOR function learning as binary classification problem with two outputs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.19</duration>
          <className>org.apache.spark.ml.classification.MultilayerPerceptronClassifierSuite</className>
          <testName>Test setWeights by training restart</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.152</duration>
          <className>org.apache.spark.ml.classification.MultilayerPerceptronClassifierSuite</className>
          <testName>3 class classification with 2 hidden layers</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.153</duration>
          <className>org.apache.spark.ml.classification.MultilayerPerceptronClassifierSuite</className>
          <testName>read/write: MultilayerPerceptronClassifier</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.653</duration>
          <className>org.apache.spark.ml.classification.MultilayerPerceptronClassifierSuite</className>
          <testName>read/write: MultilayerPerceptronClassificationModel</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.35</duration>
          <className>org.apache.spark.ml.classification.MultilayerPerceptronClassifierSuite</className>
          <testName>should support all NumericType labels and not support other types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.classification.NaiveBayesSuite.xml</file>
      <name>org.apache.spark.ml.classification.NaiveBayesSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>6.5010004</duration>
      <cases>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.classification.NaiveBayesSuite</className>
          <testName>model types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.ml.classification.NaiveBayesSuite</className>
          <testName>params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.classification.NaiveBayesSuite</className>
          <testName>naive bayes: default params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.777</duration>
          <className>org.apache.spark.ml.classification.NaiveBayesSuite</className>
          <testName>Naive Bayes Multinomial</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.783</duration>
          <className>org.apache.spark.ml.classification.NaiveBayesSuite</className>
          <testName>Naive Bayes Multinomial with weighted samples</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.715</duration>
          <className>org.apache.spark.ml.classification.NaiveBayesSuite</className>
          <testName>Naive Bayes Bernoulli with weighted samples</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.65</duration>
          <className>org.apache.spark.ml.classification.NaiveBayesSuite</className>
          <testName>Naive Bayes Bernoulli</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.258</duration>
          <className>org.apache.spark.ml.classification.NaiveBayesSuite</className>
          <testName>detect negative values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.184</duration>
          <className>org.apache.spark.ml.classification.NaiveBayesSuite</className>
          <testName>detect non zero or one values in Bernoulli</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.774</duration>
          <className>org.apache.spark.ml.classification.NaiveBayesSuite</className>
          <testName>read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.354</duration>
          <className>org.apache.spark.ml.classification.NaiveBayesSuite</className>
          <testName>should support all NumericType labels and not support other types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.classification.OneVsRestSuite.xml</file>
      <name>org.apache.spark.ml.classification.OneVsRestSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>17.48</duration>
      <cases>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.ml.classification.OneVsRestSuite</className>
          <testName>params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>6.44</duration>
          <className>org.apache.spark.ml.classification.OneVsRestSuite</className>
          <testName>one-vs-rest: default params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.535</duration>
          <className>org.apache.spark.ml.classification.OneVsRestSuite</className>
          <testName>one-vs-rest: pass label metadata correctly during train</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.085</duration>
          <className>org.apache.spark.ml.classification.OneVsRestSuite</className>
          <testName>SPARK-8092: ensure label features and prediction cols are configurable</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.702</duration>
          <className>org.apache.spark.ml.classification.OneVsRestSuite</className>
          <testName>SPARK-8049: OneVsRest shouldn&apos;t output temp columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.682</duration>
          <className>org.apache.spark.ml.classification.OneVsRestSuite</className>
          <testName>copy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.638</duration>
          <className>org.apache.spark.ml.classification.OneVsRestSuite</className>
          <testName>read/write: OneVsRest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.184</duration>
          <className>org.apache.spark.ml.classification.OneVsRestSuite</className>
          <testName>read/write: OneVsRestModel</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.186</duration>
          <className>org.apache.spark.ml.classification.OneVsRestSuite</className>
          <testName>should support all NumericType labels and not support other types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.classification.ProbabilisticClassifierSuite.xml</file>
      <name>org.apache.spark.ml.classification.ProbabilisticClassifierSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.002</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.classification.ProbabilisticClassifierSuite</className>
          <testName>test thresholding</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.classification.ProbabilisticClassifierSuite</className>
          <testName>test thresholding not required</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.classification.ProbabilisticClassifierSuite</className>
          <testName>test tiebreak</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.classification.ProbabilisticClassifierSuite</className>
          <testName>test one zero threshold</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.classification.ProbabilisticClassifierSuite</className>
          <testName>bad thresholds</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.classification.RandomForestClassifierSuite.xml</file>
      <name>org.apache.spark.ml.classification.RandomForestClassifierSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>4.6660004</duration>
      <cases>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.ml.classification.RandomForestClassifierSuite</className>
          <testName>params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.521</duration>
          <className>org.apache.spark.ml.classification.RandomForestClassifierSuite</className>
          <testName> RandomForest(numTrees = 1)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.475</duration>
          <className>org.apache.spark.ml.classification.RandomForestClassifierSuite</className>
          <testName> RandomForest(numTrees = 1)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.248</duration>
          <className>org.apache.spark.ml.classification.RandomForestClassifierSuite</className>
          <testName>alternating categorical and continuous features with multiclass labels to test indexing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.501</duration>
          <className>org.apache.spark.ml.classification.RandomForestClassifierSuite</className>
          <testName>subsampling rate in RandomForest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.233</duration>
          <className>org.apache.spark.ml.classification.RandomForestClassifierSuite</className>
          <testName>predictRaw and predictProbability</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.163</duration>
          <className>org.apache.spark.ml.classification.RandomForestClassifierSuite</className>
          <testName>Fitting without numClasses in metadata</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.134</duration>
          <className>org.apache.spark.ml.classification.RandomForestClassifierSuite</className>
          <testName>Feature importance with toy data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.685</duration>
          <className>org.apache.spark.ml.classification.RandomForestClassifierSuite</className>
          <testName>should support all NumericType labels and not support other types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.702</duration>
          <className>org.apache.spark.ml.classification.RandomForestClassifierSuite</className>
          <testName>read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.clustering.BisectingKMeansSuite.xml</file>
      <name>org.apache.spark.ml.clustering.BisectingKMeansSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>3.3460002</duration>
      <cases>
        <case>
          <duration>0.117</duration>
          <className>org.apache.spark.ml.clustering.BisectingKMeansSuite</className>
          <testName>default parameters</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.clustering.BisectingKMeansSuite</className>
          <testName>setter/getter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.148</duration>
          <className>org.apache.spark.ml.clustering.BisectingKMeansSuite</className>
          <testName>fit, transform and summary</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.08</duration>
          <className>org.apache.spark.ml.clustering.BisectingKMeansSuite</className>
          <testName>read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.clustering.GaussianMixtureSuite.xml</file>
      <name>org.apache.spark.ml.clustering.GaussianMixtureSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.512</duration>
      <cases>
        <case>
          <duration>0.384</duration>
          <className>org.apache.spark.ml.clustering.GaussianMixtureSuite</className>
          <testName>default parameters</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.clustering.GaussianMixtureSuite</className>
          <testName>set parameters</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.clustering.GaussianMixtureSuite</className>
          <testName>parameters validation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.02</duration>
          <className>org.apache.spark.ml.clustering.GaussianMixtureSuite</className>
          <testName>fit, transform and summary</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.108</duration>
          <className>org.apache.spark.ml.clustering.GaussianMixtureSuite</className>
          <testName>read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.clustering.JavaKMeansSuite.xml</file>
      <name>org.apache.spark.ml.clustering.JavaKMeansSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.208</duration>
      <cases>
        <case>
          <duration>0.208</duration>
          <className>org.apache.spark.ml.clustering.JavaKMeansSuite</className>
          <testName>fitAndTransform</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.clustering.KMeansSuite.xml</file>
      <name>org.apache.spark.ml.clustering.KMeansSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.9059999</duration>
      <cases>
        <case>
          <duration>0.167</duration>
          <className>org.apache.spark.ml.clustering.KMeansSuite</className>
          <testName>default parameters</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.clustering.KMeansSuite</className>
          <testName>set parameters</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.clustering.KMeansSuite</className>
          <testName>parameters validation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.788</duration>
          <className>org.apache.spark.ml.clustering.KMeansSuite</className>
          <testName>fit, transform and summary</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.12</duration>
          <className>org.apache.spark.ml.clustering.KMeansSuite</className>
          <testName>KMeansModel transform with non-default feature and prediction cols</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.828</duration>
          <className>org.apache.spark.ml.clustering.KMeansSuite</className>
          <testName>read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.clustering.LDASuite.xml</file>
      <name>org.apache.spark.ml.clustering.LDASuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>4.8089995</duration>
      <cases>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.ml.clustering.LDASuite</className>
          <testName>default parameters</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.clustering.LDASuite</className>
          <testName>set parameters</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.ml.clustering.LDASuite</className>
          <testName>parameters validation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.554</duration>
          <className>org.apache.spark.ml.clustering.LDASuite</className>
          <testName>fit &amp; transform with Online LDA</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.323</duration>
          <className>org.apache.spark.ml.clustering.LDASuite</className>
          <testName>fit &amp; transform with EM LDA</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.932</duration>
          <className>org.apache.spark.ml.clustering.LDASuite</className>
          <testName>read/write LocalLDAModel</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.823</duration>
          <className>org.apache.spark.ml.clustering.LDASuite</className>
          <testName>read/write DistributedLDAModel</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.459</duration>
          <className>org.apache.spark.ml.clustering.LDASuite</className>
          <testName>EM LDA checkpointing: save last checkpoint</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.424</duration>
          <className>org.apache.spark.ml.clustering.LDASuite</className>
          <testName>EM LDA checkpointing: remove last checkpoint</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.252</duration>
          <className>org.apache.spark.ml.clustering.LDASuite</className>
          <testName>EM LDA disable checkpointing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.evaluation.BinaryClassificationEvaluatorSuite.xml</file>
      <name>org.apache.spark.ml.evaluation.BinaryClassificationEvaluatorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.931</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.evaluation.BinaryClassificationEvaluatorSuite</className>
          <testName>params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.181</duration>
          <className>org.apache.spark.ml.evaluation.BinaryClassificationEvaluatorSuite</className>
          <testName>read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.215</duration>
          <className>org.apache.spark.ml.evaluation.BinaryClassificationEvaluatorSuite</className>
          <testName>should accept both vector and double raw prediction col</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.534</duration>
          <className>org.apache.spark.ml.evaluation.BinaryClassificationEvaluatorSuite</className>
          <testName>should support all NumericType labels and not support other types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.evaluation.MulticlassClassificationEvaluatorSuite.xml</file>
      <name>org.apache.spark.ml.evaluation.MulticlassClassificationEvaluatorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.728</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.evaluation.MulticlassClassificationEvaluatorSuite</className>
          <testName>params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.162</duration>
          <className>org.apache.spark.ml.evaluation.MulticlassClassificationEvaluatorSuite</className>
          <testName>read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.565</duration>
          <className>org.apache.spark.ml.evaluation.MulticlassClassificationEvaluatorSuite</className>
          <testName>should support all NumericType labels and not support other types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.evaluation.RegressionEvaluatorSuite.xml</file>
      <name>org.apache.spark.ml.evaluation.RegressionEvaluatorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.52</duration>
      <cases>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.evaluation.RegressionEvaluatorSuite</className>
          <testName>params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.203</duration>
          <className>org.apache.spark.ml.evaluation.RegressionEvaluatorSuite</className>
          <testName>Regression Evaluator: default params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.152</duration>
          <className>org.apache.spark.ml.evaluation.RegressionEvaluatorSuite</className>
          <testName>read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.165</duration>
          <className>org.apache.spark.ml.evaluation.RegressionEvaluatorSuite</className>
          <testName>should support all NumericType labels and not support other types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.BinarizerSuite.xml</file>
      <name>org.apache.spark.ml.feature.BinarizerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.333</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.feature.BinarizerSuite</className>
          <testName>params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.ml.feature.BinarizerSuite</className>
          <testName>Binarize continuous features with default parameter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.ml.feature.BinarizerSuite</className>
          <testName>Binarize continuous features with setter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.044</duration>
          <className>org.apache.spark.ml.feature.BinarizerSuite</className>
          <testName>Binarize vector of continuous features with default parameter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.038</duration>
          <className>org.apache.spark.ml.feature.BinarizerSuite</className>
          <testName>Binarize vector of continuous features with setter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.183</duration>
          <className>org.apache.spark.ml.feature.BinarizerSuite</className>
          <testName>read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.BucketizerSuite.xml</file>
      <name>org.apache.spark.ml.feature.BucketizerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.444</duration>
      <cases>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.ml.feature.BucketizerSuite</className>
          <testName>params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.094</duration>
          <className>org.apache.spark.ml.feature.BucketizerSuite</className>
          <testName>Bucket continuous features, without -inf,inf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.ml.feature.BucketizerSuite</className>
          <testName>Bucket continuous features, with -inf,inf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.114</duration>
          <className>org.apache.spark.ml.feature.BucketizerSuite</className>
          <testName>Bucket continuous features, with NaN data but non-NaN splits</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.feature.BucketizerSuite</className>
          <testName>Bucket continuous features, with NaN splits</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.feature.BucketizerSuite</className>
          <testName>Binary search correctness on hand-picked examples</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.ml.feature.BucketizerSuite</className>
          <testName>Binary search correctness in contrast with linear search, on random data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.2</duration>
          <className>org.apache.spark.ml.feature.BucketizerSuite</className>
          <testName>read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.ChiSqSelectorSuite.xml</file>
      <name>org.apache.spark.ml.feature.ChiSqSelectorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.209</duration>
      <cases>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.ml.feature.ChiSqSelectorSuite</className>
          <testName>params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.094</duration>
          <className>org.apache.spark.ml.feature.ChiSqSelectorSuite</className>
          <testName>Test Chi-Square selector: numTopFeatures</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.ml.feature.ChiSqSelectorSuite</className>
          <testName>Test Chi-Square selector: percentile</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.ml.feature.ChiSqSelectorSuite</className>
          <testName>Test Chi-Square selector: fpr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.7</duration>
          <className>org.apache.spark.ml.feature.ChiSqSelectorSuite</className>
          <testName>read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.315</duration>
          <className>org.apache.spark.ml.feature.ChiSqSelectorSuite</className>
          <testName>should support all NumericType labels and not support other types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.CountVectorizerSuite.xml</file>
      <name>org.apache.spark.ml.feature.CountVectorizerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.5120001</duration>
      <cases>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.ml.feature.CountVectorizerSuite</className>
          <testName>params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.078</duration>
          <className>org.apache.spark.ml.feature.CountVectorizerSuite</className>
          <testName>CountVectorizerModel common cases</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.148</duration>
          <className>org.apache.spark.ml.feature.CountVectorizerSuite</className>
          <testName>CountVectorizer common cases</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.279</duration>
          <className>org.apache.spark.ml.feature.CountVectorizerSuite</className>
          <testName>CountVectorizer vocabSize and minDF</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.082</duration>
          <className>org.apache.spark.ml.feature.CountVectorizerSuite</className>
          <testName>CountVectorizer throws exception when vocab is empty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.063</duration>
          <className>org.apache.spark.ml.feature.CountVectorizerSuite</className>
          <testName>CountVectorizerModel with minTF count</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.062</duration>
          <className>org.apache.spark.ml.feature.CountVectorizerSuite</className>
          <testName>CountVectorizerModel with minTF freq</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.124</duration>
          <className>org.apache.spark.ml.feature.CountVectorizerSuite</className>
          <testName>CountVectorizerModel and CountVectorizer with binary</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.154</duration>
          <className>org.apache.spark.ml.feature.CountVectorizerSuite</className>
          <testName>CountVectorizer read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.513</duration>
          <className>org.apache.spark.ml.feature.CountVectorizerSuite</className>
          <testName>CountVectorizerModel read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.DCTSuite.xml</file>
      <name>org.apache.spark.ml.feature.DCTSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.255</duration>
      <cases>
        <case>
          <duration>0.079</duration>
          <className>org.apache.spark.ml.feature.DCTSuite</className>
          <testName>forward transform of discrete cosine matches jTransforms result</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.ml.feature.DCTSuite</className>
          <testName>inverse transform of discrete cosine matches jTransforms result</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.155</duration>
          <className>org.apache.spark.ml.feature.DCTSuite</className>
          <testName>read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.ElementwiseProductSuite.xml</file>
      <name>org.apache.spark.ml.feature.ElementwiseProductSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.176</duration>
      <cases>
        <case>
          <duration>0.176</duration>
          <className>org.apache.spark.ml.feature.ElementwiseProductSuite</className>
          <testName>read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.HashingTFSuite.xml</file>
      <name>org.apache.spark.ml.feature.HashingTFSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.258</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.feature.HashingTFSuite</className>
          <testName>params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.052</duration>
          <className>org.apache.spark.ml.feature.HashingTFSuite</className>
          <testName>hashingTF</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.046</duration>
          <className>org.apache.spark.ml.feature.HashingTFSuite</className>
          <testName>applying binary term freqs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.159</duration>
          <className>org.apache.spark.ml.feature.HashingTFSuite</className>
          <testName>read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.IDFSuite.xml</file>
      <name>org.apache.spark.ml.feature.IDFSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.803</duration>
      <cases>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.ml.feature.IDFSuite</className>
          <testName>params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.069</duration>
          <className>org.apache.spark.ml.feature.IDFSuite</className>
          <testName>compute IDF with default parameter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.038</duration>
          <className>org.apache.spark.ml.feature.IDFSuite</className>
          <testName>compute IDF with setter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.195</duration>
          <className>org.apache.spark.ml.feature.IDFSuite</className>
          <testName>IDF read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.495</duration>
          <className>org.apache.spark.ml.feature.IDFSuite</className>
          <testName>IDFModel read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.InteractionSuite.xml</file>
      <name>org.apache.spark.ml.feature.InteractionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.388</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.feature.InteractionSuite</className>
          <testName>params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.ml.feature.InteractionSuite</className>
          <testName>feature encoder</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.087</duration>
          <className>org.apache.spark.ml.feature.InteractionSuite</className>
          <testName>numeric interaction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.041</duration>
          <className>org.apache.spark.ml.feature.InteractionSuite</className>
          <testName>nominal interaction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.103</duration>
          <className>org.apache.spark.ml.feature.InteractionSuite</className>
          <testName>default attr names</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.151</duration>
          <className>org.apache.spark.ml.feature.InteractionSuite</className>
          <testName>read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.JavaBucketizerSuite.xml</file>
      <name>org.apache.spark.ml.feature.JavaBucketizerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.079</duration>
      <cases>
        <case>
          <duration>0.079</duration>
          <className>org.apache.spark.ml.feature.JavaBucketizerSuite</className>
          <testName>bucketizerTest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.JavaDCTSuite.xml</file>
      <name>org.apache.spark.ml.feature.JavaDCTSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.061</duration>
      <cases>
        <case>
          <duration>0.061</duration>
          <className>org.apache.spark.ml.feature.JavaDCTSuite</className>
          <testName>javaCompatibilityTest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.JavaHashingTFSuite.xml</file>
      <name>org.apache.spark.ml.feature.JavaHashingTFSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.128</duration>
      <cases>
        <case>
          <duration>0.128</duration>
          <className>org.apache.spark.ml.feature.JavaHashingTFSuite</className>
          <testName>hashingTF</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.JavaNormalizerSuite.xml</file>
      <name>org.apache.spark.ml.feature.JavaNormalizerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.132</duration>
      <cases>
        <case>
          <duration>0.132</duration>
          <className>org.apache.spark.ml.feature.JavaNormalizerSuite</className>
          <testName>normalizer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.JavaPCASuite.xml</file>
      <name>org.apache.spark.ml.feature.JavaPCASuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.206</duration>
      <cases>
        <case>
          <duration>0.206</duration>
          <className>org.apache.spark.ml.feature.JavaPCASuite</className>
          <testName>testPCA</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.JavaPolynomialExpansionSuite.xml</file>
      <name>org.apache.spark.ml.feature.JavaPolynomialExpansionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.083</duration>
      <cases>
        <case>
          <duration>0.083</duration>
          <className>org.apache.spark.ml.feature.JavaPolynomialExpansionSuite</className>
          <testName>polynomialExpansionTest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.JavaStandardScalerSuite.xml</file>
      <name>org.apache.spark.ml.feature.JavaStandardScalerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.11</duration>
      <cases>
        <case>
          <duration>0.11</duration>
          <className>org.apache.spark.ml.feature.JavaStandardScalerSuite</className>
          <testName>standardScaler</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.JavaStopWordsRemoverSuite.xml</file>
      <name>org.apache.spark.ml.feature.JavaStopWordsRemoverSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.085</duration>
      <cases>
        <case>
          <duration>0.085</duration>
          <className>org.apache.spark.ml.feature.JavaStopWordsRemoverSuite</className>
          <testName>javaCompatibilityTest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.JavaStringIndexerSuite.xml</file>
      <name>org.apache.spark.ml.feature.JavaStringIndexerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.26</duration>
      <cases>
        <case>
          <duration>0.26</duration>
          <className>org.apache.spark.ml.feature.JavaStringIndexerSuite</className>
          <testName>testStringIndexer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.JavaTokenizerSuite.xml</file>
      <name>org.apache.spark.ml.feature.JavaTokenizerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.138</duration>
      <cases>
        <case>
          <duration>0.138</duration>
          <className>org.apache.spark.ml.feature.JavaTokenizerSuite</className>
          <testName>regexTokenizer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.JavaVectorAssemblerSuite.xml</file>
      <name>org.apache.spark.ml.feature.JavaVectorAssemblerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.11</duration>
      <cases>
        <case>
          <duration>0.11</duration>
          <className>org.apache.spark.ml.feature.JavaVectorAssemblerSuite</className>
          <testName>testVectorAssembler</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.JavaVectorIndexerSuite.xml</file>
      <name>org.apache.spark.ml.feature.JavaVectorIndexerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.103</duration>
      <cases>
        <case>
          <duration>0.103</duration>
          <className>org.apache.spark.ml.feature.JavaVectorIndexerSuite</className>
          <testName>vectorIndexerAPI</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.JavaVectorSlicerSuite.xml</file>
      <name>org.apache.spark.ml.feature.JavaVectorSlicerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.11</duration>
      <cases>
        <case>
          <duration>0.11</duration>
          <className>org.apache.spark.ml.feature.JavaVectorSlicerSuite</className>
          <testName>vectorSlice</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.JavaWord2VecSuite.xml</file>
      <name>org.apache.spark.ml.feature.JavaWord2VecSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.198</duration>
      <cases>
        <case>
          <duration>0.198</duration>
          <className>org.apache.spark.ml.feature.JavaWord2VecSuite</className>
          <testName>testJavaWord2Vec</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.MaxAbsScalerSuite.xml</file>
      <name>org.apache.spark.ml.feature.MaxAbsScalerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.72800004</duration>
      <cases>
        <case>
          <duration>0.052</duration>
          <className>org.apache.spark.ml.feature.MaxAbsScalerSuite</className>
          <testName>MaxAbsScaler fit basic case</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.158</duration>
          <className>org.apache.spark.ml.feature.MaxAbsScalerSuite</className>
          <testName>MaxAbsScaler read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.518</duration>
          <className>org.apache.spark.ml.feature.MaxAbsScalerSuite</className>
          <testName>MaxAbsScalerModel read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.MinHashSuite.xml</file>
      <name>org.apache.spark.ml.feature.MinHashSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>3.906</duration>
      <cases>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.ml.feature.MinHashSuite</className>
          <testName>params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.feature.MinHashSuite</className>
          <testName>MinHash: default params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.717</duration>
          <className>org.apache.spark.ml.feature.MinHashSuite</className>
          <testName>read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.feature.MinHashSuite</className>
          <testName>hashFunction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.feature.MinHashSuite</className>
          <testName>keyDistance and hashDistance</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.573</duration>
          <className>org.apache.spark.ml.feature.MinHashSuite</className>
          <testName>MinHash: test of LSH property</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.379</duration>
          <className>org.apache.spark.ml.feature.MinHashSuite</className>
          <testName>approxNearestNeighbors for min hash</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.228</duration>
          <className>org.apache.spark.ml.feature.MinHashSuite</className>
          <testName>approxSimilarityJoin for minhash on different dataset</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.MinMaxScalerSuite.xml</file>
      <name>org.apache.spark.ml.feature.MinMaxScalerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.865</duration>
      <cases>
        <case>
          <duration>0.096</duration>
          <className>org.apache.spark.ml.feature.MinMaxScalerSuite</className>
          <testName>MinMaxScaler fit basic case</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.035</duration>
          <className>org.apache.spark.ml.feature.MinMaxScalerSuite</className>
          <testName>MinMaxScaler arguments max must be larger than min</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.161</duration>
          <className>org.apache.spark.ml.feature.MinMaxScalerSuite</className>
          <testName>MinMaxScaler read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.533</duration>
          <className>org.apache.spark.ml.feature.MinMaxScalerSuite</className>
          <testName>MinMaxScalerModel read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.04</duration>
          <className>org.apache.spark.ml.feature.MinMaxScalerSuite</className>
          <testName>MinMaxScaler should remain NaN value</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.NGramSuite.xml</file>
      <name>org.apache.spark.ml.feature.NGramSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.44599998</duration>
      <cases>
        <case>
          <duration>0.077</duration>
          <className>org.apache.spark.ml.feature.NGramSuite</className>
          <testName>default behavior yields bigram features</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.062</duration>
          <className>org.apache.spark.ml.feature.NGramSuite</className>
          <testName>NGramLength=4 yields length 4 n-grams</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.063</duration>
          <className>org.apache.spark.ml.feature.NGramSuite</className>
          <testName>empty input yields empty output</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.069</duration>
          <className>org.apache.spark.ml.feature.NGramSuite</className>
          <testName>input array &lt; n yields empty output</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.175</duration>
          <className>org.apache.spark.ml.feature.NGramSuite</className>
          <testName>read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.NormalizerSuite.xml</file>
      <name>org.apache.spark.ml.feature.NormalizerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.212</duration>
      <cases>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.ml.feature.NormalizerSuite</className>
          <testName>Normalization with default parameter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.ml.feature.NormalizerSuite</className>
          <testName>Normalization with setter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.192</duration>
          <className>org.apache.spark.ml.feature.NormalizerSuite</className>
          <testName>read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.OneHotEncoderSuite.xml</file>
      <name>org.apache.spark.ml.feature.OneHotEncoderSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.772</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.feature.OneHotEncoderSuite</className>
          <testName>params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.137</duration>
          <className>org.apache.spark.ml.feature.OneHotEncoderSuite</className>
          <testName>OneHotEncoder dropLast = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.065</duration>
          <className>org.apache.spark.ml.feature.OneHotEncoderSuite</className>
          <testName>OneHotEncoder dropLast = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.ml.feature.OneHotEncoderSuite</className>
          <testName>input column with ML attribute</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.ml.feature.OneHotEncoderSuite</className>
          <testName>input column without ML attribute</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.191</duration>
          <className>org.apache.spark.ml.feature.OneHotEncoderSuite</className>
          <testName>read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.31</duration>
          <className>org.apache.spark.ml.feature.OneHotEncoderSuite</className>
          <testName>OneHotEncoder with varying types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.PCASuite.xml</file>
      <name>org.apache.spark.ml.feature.PCASuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.933</duration>
      <cases>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.ml.feature.PCASuite</className>
          <testName>params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.169</duration>
          <className>org.apache.spark.ml.feature.PCASuite</className>
          <testName>pca</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.182</duration>
          <className>org.apache.spark.ml.feature.PCASuite</className>
          <testName>PCA read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.577</duration>
          <className>org.apache.spark.ml.feature.PCASuite</className>
          <testName>PCAModel read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.PolynomialExpansionSuite.xml</file>
      <name>org.apache.spark.ml.feature.PolynomialExpansionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.399</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.feature.PolynomialExpansionSuite</className>
          <testName>params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.059</duration>
          <className>org.apache.spark.ml.feature.PolynomialExpansionSuite</className>
          <testName>Polynomial expansion with default parameter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.ml.feature.PolynomialExpansionSuite</className>
          <testName>Polynomial expansion with setter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.ml.feature.PolynomialExpansionSuite</className>
          <testName>Polynomial expansion with degree 1 is identity on vectors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.174</duration>
          <className>org.apache.spark.ml.feature.PolynomialExpansionSuite</className>
          <testName>read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.118</duration>
          <className>org.apache.spark.ml.feature.PolynomialExpansionSuite</className>
          <testName>getPolySize</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.QuantileDiscretizerSuite.xml</file>
      <name>org.apache.spark.ml.feature.QuantileDiscretizerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.703</duration>
      <cases>
        <case>
          <duration>1.627</duration>
          <className>org.apache.spark.ml.feature.QuantileDiscretizerSuite</className>
          <testName>Test observed number of buckets and their sizes match expected values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.544</duration>
          <className>org.apache.spark.ml.feature.QuantileDiscretizerSuite</className>
          <testName>Test on data with high proportion of duplicated values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.186</duration>
          <className>org.apache.spark.ml.feature.QuantileDiscretizerSuite</className>
          <testName>Test transform on data with NaN value</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.148</duration>
          <className>org.apache.spark.ml.feature.QuantileDiscretizerSuite</className>
          <testName>Test transform method on unseen data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.158</duration>
          <className>org.apache.spark.ml.feature.QuantileDiscretizerSuite</className>
          <testName>read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.04</duration>
          <className>org.apache.spark.ml.feature.QuantileDiscretizerSuite</className>
          <testName>Verify resulting model has parent</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.RFormulaParserSuite.xml</file>
      <name>org.apache.spark.ml.feature.RFormulaParserSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.07499999</duration>
      <cases>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.ml.feature.RFormulaParserSuite</className>
          <testName>parse simple formulas</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.ml.feature.RFormulaParserSuite</className>
          <testName>parse dot</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.feature.RFormulaParserSuite</className>
          <testName>parse deletion</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.feature.RFormulaParserSuite</className>
          <testName>parse additions and deletions in order</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.ml.feature.RFormulaParserSuite</className>
          <testName>dot ignores complex column types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.feature.RFormulaParserSuite</className>
          <testName>parse intercept</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.ml.feature.RFormulaParserSuite</className>
          <testName>parse interactions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.ml.feature.RFormulaParserSuite</className>
          <testName>parse basic interactions with dot</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.feature.RFormulaParserSuite</className>
          <testName>parse all to all iris interactions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.ml.feature.RFormulaParserSuite</className>
          <testName>parse interaction negation with iris</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.RFormulaSuite.xml</file>
      <name>org.apache.spark.ml.feature.RFormulaSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>5.2460003</duration>
      <cases>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.ml.feature.RFormulaSuite</className>
          <testName>params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.128</duration>
          <className>org.apache.spark.ml.feature.RFormulaSuite</className>
          <testName>transform numeric data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.ml.feature.RFormulaSuite</className>
          <testName>features column already exists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.ml.feature.RFormulaSuite</className>
          <testName>label column already exists and forceIndexLabel was set with false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.ml.feature.RFormulaSuite</className>
          <testName>label column already exists but forceIndexLabel was set with true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.ml.feature.RFormulaSuite</className>
          <testName>label column already exists but is not numeric type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.ml.feature.RFormulaSuite</className>
          <testName>allow missing label column for test datasets</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.091</duration>
          <className>org.apache.spark.ml.feature.RFormulaSuite</className>
          <testName>allow empty label</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.187</duration>
          <className>org.apache.spark.ml.feature.RFormulaSuite</className>
          <testName>encodes string terms</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.25</duration>
          <className>org.apache.spark.ml.feature.RFormulaSuite</className>
          <testName>index string label</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.18</duration>
          <className>org.apache.spark.ml.feature.RFormulaSuite</className>
          <testName>force to index label even it is numeric type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.068</duration>
          <className>org.apache.spark.ml.feature.RFormulaSuite</className>
          <testName>attribute generation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.061</duration>
          <className>org.apache.spark.ml.feature.RFormulaSuite</className>
          <testName>vector attribute generation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.ml.feature.RFormulaSuite</className>
          <testName>vector attribute generation with unnamed input attrs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.14</duration>
          <className>org.apache.spark.ml.feature.RFormulaSuite</className>
          <testName>numeric interaction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.102</duration>
          <className>org.apache.spark.ml.feature.RFormulaSuite</className>
          <testName>factor numeric interaction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.194</duration>
          <className>org.apache.spark.ml.feature.RFormulaSuite</className>
          <testName>factor factor interaction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.172</duration>
          <className>org.apache.spark.ml.feature.RFormulaSuite</className>
          <testName>read/write: RFormula</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.507</duration>
          <className>org.apache.spark.ml.feature.RFormulaSuite</className>
          <testName>read/write: RFormulaModel</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.044</duration>
          <className>org.apache.spark.ml.feature.RFormulaSuite</className>
          <testName>should support all NumericType labels</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.RandomProjectionSuite.xml</file>
      <name>org.apache.spark.ml.feature.RandomProjectionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>10.461</duration>
      <cases>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.ml.feature.RandomProjectionSuite</className>
          <testName>params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.feature.RandomProjectionSuite</className>
          <testName>RandomProjection: default params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.799</duration>
          <className>org.apache.spark.ml.feature.RandomProjectionSuite</className>
          <testName>read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.feature.RandomProjectionSuite</className>
          <testName>hashFunction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.feature.RandomProjectionSuite</className>
          <testName>keyDistance and hashDistance</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.ml.feature.RandomProjectionSuite</className>
          <testName>RandomProjection: randUnitVectors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.004</duration>
          <className>org.apache.spark.ml.feature.RandomProjectionSuite</className>
          <testName>RandomProjection: test of LSH property</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.007</duration>
          <className>org.apache.spark.ml.feature.RandomProjectionSuite</className>
          <testName>RandomProjection with high dimension data: test of LSH property</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.585</duration>
          <className>org.apache.spark.ml.feature.RandomProjectionSuite</className>
          <testName>approxNearestNeighbors for random projection</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.611</duration>
          <className>org.apache.spark.ml.feature.RandomProjectionSuite</className>
          <testName>approxNearestNeighbors with multiple probing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.588</duration>
          <className>org.apache.spark.ml.feature.RandomProjectionSuite</className>
          <testName>approxSimilarityJoin for random projection on different dataset</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.841</duration>
          <className>org.apache.spark.ml.feature.RandomProjectionSuite</className>
          <testName>approxSimilarityJoin for self join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.RegexTokenizerSuite.xml</file>
      <name>org.apache.spark.ml.feature.RegexTokenizerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.45400003</duration>
      <cases>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.ml.feature.RegexTokenizerSuite</className>
          <testName>params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.194</duration>
          <className>org.apache.spark.ml.feature.RegexTokenizerSuite</className>
          <testName>RegexTokenizer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.07</duration>
          <className>org.apache.spark.ml.feature.RegexTokenizerSuite</className>
          <testName>RegexTokenizer with toLowercase false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.187</duration>
          <className>org.apache.spark.ml.feature.RegexTokenizerSuite</className>
          <testName>read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.SQLTransformerSuite.xml</file>
      <name>org.apache.spark.ml.feature.SQLTransformerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>7.063</duration>
      <cases>
        <case>
          <duration>0.046</duration>
          <className>org.apache.spark.ml.feature.SQLTransformerSuite</className>
          <testName>params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>6.034</duration>
          <className>org.apache.spark.ml.feature.SQLTransformerSuite</className>
          <testName>transform numeric data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.918</duration>
          <className>org.apache.spark.ml.feature.SQLTransformerSuite</className>
          <testName>read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.065</duration>
          <className>org.apache.spark.ml.feature.SQLTransformerSuite</className>
          <testName>transformSchema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.StandardScalerSuite.xml</file>
      <name>org.apache.spark.ml.feature.StandardScalerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.894</duration>
      <cases>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.ml.feature.StandardScalerSuite</className>
          <testName>params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.ml.feature.StandardScalerSuite</className>
          <testName>Standardization with default parameter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.112</duration>
          <className>org.apache.spark.ml.feature.StandardScalerSuite</className>
          <testName>Standardization with setter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.056</duration>
          <className>org.apache.spark.ml.feature.StandardScalerSuite</className>
          <testName>sparse data and withMean</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.154</duration>
          <className>org.apache.spark.ml.feature.StandardScalerSuite</className>
          <testName>StandardScaler read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.518</duration>
          <className>org.apache.spark.ml.feature.StandardScalerSuite</className>
          <testName>StandardScalerModel read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.StopWordsRemoverSuite.xml</file>
      <name>org.apache.spark.ml.feature.StopWordsRemoverSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.74499995</duration>
      <cases>
        <case>
          <duration>0.125</duration>
          <className>org.apache.spark.ml.feature.StopWordsRemoverSuite</className>
          <testName>StopWordsRemover default</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.073</duration>
          <className>org.apache.spark.ml.feature.StopWordsRemoverSuite</className>
          <testName>StopWordsRemover with particular stop words list</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.075</duration>
          <className>org.apache.spark.ml.feature.StopWordsRemoverSuite</className>
          <testName>StopWordsRemover case sensitive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.ml.feature.StopWordsRemoverSuite</className>
          <testName>default stop words of supported languages are not empty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.083</duration>
          <className>org.apache.spark.ml.feature.StopWordsRemoverSuite</className>
          <testName>StopWordsRemover with language selection</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.087</duration>
          <className>org.apache.spark.ml.feature.StopWordsRemoverSuite</className>
          <testName>StopWordsRemover with ignored words</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.072</duration>
          <className>org.apache.spark.ml.feature.StopWordsRemoverSuite</className>
          <testName>StopWordsRemover with additional words</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.181</duration>
          <className>org.apache.spark.ml.feature.StopWordsRemoverSuite</className>
          <testName>read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.ml.feature.StopWordsRemoverSuite</className>
          <testName>StopWordsRemover output column already exists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.StringIndexerSuite.xml</file>
      <name>org.apache.spark.ml.feature.StringIndexerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.656</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.feature.StringIndexerSuite</className>
          <testName>params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.132</duration>
          <className>org.apache.spark.ml.feature.StringIndexerSuite</className>
          <testName>StringIndexer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.17</duration>
          <className>org.apache.spark.ml.feature.StringIndexerSuite</className>
          <testName>StringIndexerUnseen</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.105</duration>
          <className>org.apache.spark.ml.feature.StringIndexerSuite</className>
          <testName>StringIndexer with a numeric input column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.05</duration>
          <className>org.apache.spark.ml.feature.StringIndexerSuite</className>
          <testName>StringIndexerModel should keep silent if the input column does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.049</duration>
          <className>org.apache.spark.ml.feature.StringIndexerSuite</className>
          <testName>StringIndexerModel can&apos;t overwrite output column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.178</duration>
          <className>org.apache.spark.ml.feature.StringIndexerSuite</className>
          <testName>StringIndexer read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.551</duration>
          <className>org.apache.spark.ml.feature.StringIndexerSuite</className>
          <testName>StringIndexerModel read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.ml.feature.StringIndexerSuite</className>
          <testName>IndexToString params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.074</duration>
          <className>org.apache.spark.ml.feature.StringIndexerSuite</className>
          <testName>transform</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.075</duration>
          <className>org.apache.spark.ml.feature.StringIndexerSuite</className>
          <testName>StringIndexer, IndexToString are inverses</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.feature.StringIndexerSuite</className>
          <testName>transformSchema (SPARK-10573)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.206</duration>
          <className>org.apache.spark.ml.feature.StringIndexerSuite</className>
          <testName>IndexToString read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.061</duration>
          <className>org.apache.spark.ml.feature.StringIndexerSuite</className>
          <testName>StringIndexer metadata</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.TokenizerSuite.xml</file>
      <name>org.apache.spark.ml.feature.TokenizerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.178</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.feature.TokenizerSuite</className>
          <testName>params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.177</duration>
          <className>org.apache.spark.ml.feature.TokenizerSuite</className>
          <testName>read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.VectorAssemblerSuite.xml</file>
      <name>org.apache.spark.ml.feature.VectorAssemblerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.404</duration>
      <cases>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.feature.VectorAssemblerSuite</className>
          <testName>params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.feature.VectorAssemblerSuite</className>
          <testName>assemble</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.feature.VectorAssemblerSuite</className>
          <testName>assemble should compress vectors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.125</duration>
          <className>org.apache.spark.ml.feature.VectorAssemblerSuite</className>
          <testName>VectorAssembler</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.ml.feature.VectorAssemblerSuite</className>
          <testName>transform should throw an exception in case of unsupported type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.088</duration>
          <className>org.apache.spark.ml.feature.VectorAssemblerSuite</className>
          <testName>ML attributes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.155</duration>
          <className>org.apache.spark.ml.feature.VectorAssemblerSuite</className>
          <testName>read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.VectorIndexerSuite.xml</file>
      <name>org.apache.spark.ml.feature.VectorIndexerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.3510001</duration>
      <cases>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.ml.feature.VectorIndexerSuite</className>
          <testName>params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.ml.feature.VectorIndexerSuite</className>
          <testName>Cannot fit an empty DataFrame</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.168</duration>
          <className>org.apache.spark.ml.feature.VectorIndexerSuite</className>
          <testName>Throws error when given RDDs with different size vectors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.084</duration>
          <className>org.apache.spark.ml.feature.VectorIndexerSuite</className>
          <testName>Same result with dense and sparse vectors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.175</duration>
          <className>org.apache.spark.ml.feature.VectorIndexerSuite</className>
          <testName>Builds valid categorical feature value index, transform correctly, check metadata</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.091</duration>
          <className>org.apache.spark.ml.feature.VectorIndexerSuite</className>
          <testName>Maintain sparsity for sparse vectors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.ml.feature.VectorIndexerSuite</className>
          <testName>Preserve metadata</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.159</duration>
          <className>org.apache.spark.ml.feature.VectorIndexerSuite</className>
          <testName>VectorIndexer read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.626</duration>
          <className>org.apache.spark.ml.feature.VectorIndexerSuite</className>
          <testName>VectorIndexerModel read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.VectorSlicerSuite.xml</file>
      <name>org.apache.spark.ml.feature.VectorSlicerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.32700002</duration>
      <cases>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.ml.feature.VectorSlicerSuite</className>
          <testName>params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.feature.VectorSlicerSuite</className>
          <testName>feature validity checks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.161</duration>
          <className>org.apache.spark.ml.feature.VectorSlicerSuite</className>
          <testName>Test vector slicer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.158</duration>
          <className>org.apache.spark.ml.feature.VectorSlicerSuite</className>
          <testName>read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.feature.Word2VecSuite.xml</file>
      <name>org.apache.spark.ml.feature.Word2VecSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.155</duration>
      <cases>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.ml.feature.Word2VecSuite</className>
          <testName>params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.238</duration>
          <className>org.apache.spark.ml.feature.Word2VecSuite</className>
          <testName>Word2Vec</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.313</duration>
          <className>org.apache.spark.ml.feature.Word2VecSuite</className>
          <testName>getVectors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.2</duration>
          <className>org.apache.spark.ml.feature.Word2VecSuite</className>
          <testName>findSynonyms</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.384</duration>
          <className>org.apache.spark.ml.feature.Word2VecSuite</className>
          <testName>window size</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.158</duration>
          <className>org.apache.spark.ml.feature.Word2VecSuite</className>
          <testName>Word2Vec read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.628</duration>
          <className>org.apache.spark.ml.feature.Word2VecSuite</className>
          <testName>Word2VecModel read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.228</duration>
          <className>org.apache.spark.ml.feature.Word2VecSuite</className>
          <testName>Word2Vec works with input that is non-nullable (NGram)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.linalg.JavaSQLDataTypesSuite.xml</file>
      <name>org.apache.spark.ml.linalg.JavaSQLDataTypesSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.001</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.linalg.JavaSQLDataTypesSuite</className>
          <testName>testSQLDataTypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.linalg.JsonVectorConverterSuite.xml</file>
      <name>org.apache.spark.ml.linalg.JsonVectorConverterSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.005</duration>
      <cases>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.ml.linalg.JsonVectorConverterSuite</className>
          <testName>toJson/fromJson</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.linalg.MatrixUDTSuite.xml</file>
      <name>org.apache.spark.ml.linalg.MatrixUDTSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.002</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.linalg.MatrixUDTSuite</className>
          <testName>preloaded MatrixUDT</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.linalg.SQLDataTypesSuite.xml</file>
      <name>org.apache.spark.ml.linalg.SQLDataTypesSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.001</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.linalg.SQLDataTypesSuite</className>
          <testName>sqlDataTypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.linalg.VectorUDTSuite.xml</file>
      <name>org.apache.spark.ml.linalg.VectorUDTSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.023</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.linalg.VectorUDTSuite</className>
          <testName>preloaded VectorUDT</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.ml.linalg.VectorUDTSuite</className>
          <testName>JavaTypeInference with VectorUDT</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.optim.IterativelyReweightedLeastSquaresSuite.xml</file>
      <name>org.apache.spark.ml.optim.IterativelyReweightedLeastSquaresSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.375</duration>
      <cases>
        <case>
          <duration>0.103</duration>
          <className>org.apache.spark.ml.optim.IterativelyReweightedLeastSquaresSuite</className>
          <testName>IRLS against GLM with Binomial errors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.13</duration>
          <className>org.apache.spark.ml.optim.IterativelyReweightedLeastSquaresSuite</className>
          <testName>IRLS against GLM with Poisson errors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.142</duration>
          <className>org.apache.spark.ml.optim.IterativelyReweightedLeastSquaresSuite</className>
          <testName>IRLS against L1Regression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.optim.WeightedLeastSquaresSuite.xml</file>
      <name>org.apache.spark.ml.optim.WeightedLeastSquaresSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.29</duration>
      <cases>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.ml.optim.WeightedLeastSquaresSuite</className>
          <testName>WLS with strong L1 regularization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.ml.optim.WeightedLeastSquaresSuite</className>
          <testName>diagonal inverse of AtWA</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.125</duration>
          <className>org.apache.spark.ml.optim.WeightedLeastSquaresSuite</className>
          <testName>two collinear features</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.098</duration>
          <className>org.apache.spark.ml.optim.WeightedLeastSquaresSuite</className>
          <testName>WLS against lm</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.104</duration>
          <className>org.apache.spark.ml.optim.WeightedLeastSquaresSuite</className>
          <testName>WLS against lm when label is constant and no regularization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.ml.optim.WeightedLeastSquaresSuite</className>
          <testName>WLS with regularization when label is constant</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.178</duration>
          <className>org.apache.spark.ml.optim.WeightedLeastSquaresSuite</className>
          <testName>WLS against glmnet with constant features</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.433</duration>
          <className>org.apache.spark.ml.optim.WeightedLeastSquaresSuite</className>
          <testName>WLS against glmnet with L1/ElasticNet regularization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.297</duration>
          <className>org.apache.spark.ml.optim.WeightedLeastSquaresSuite</className>
          <testName>WLS against glmnet with L2 regularization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.param.JavaParamsSuite.xml</file>
      <name>org.apache.spark.ml.param.JavaParamsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.003</duration>
      <cases>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.ml.param.JavaParamsSuite</className>
          <testName>testParams</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.param.JavaParamsSuite</className>
          <testName>testParamValidate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.param.ParamsSuite.xml</file>
      <name>org.apache.spark.ml.param.ParamsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.026000002</duration>
      <cases>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.ml.param.ParamsSuite</className>
          <testName>json encode/decode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.ml.param.ParamsSuite</className>
          <testName>param</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.param.ParamsSuite</className>
          <testName>param pair</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.param.ParamsSuite</className>
          <testName>param map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.ml.param.ParamsSuite</className>
          <testName>params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.param.ParamsSuite</className>
          <testName>ParamValidate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.param.ParamsSuite</className>
          <testName>copyValues</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.ml.param.ParamsSuite</className>
          <testName>Filtering ParamMap</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.param.shared.SharedParamsSuite.xml</file>
      <name>org.apache.spark.ml.param.shared.SharedParamsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.002</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.param.shared.SharedParamsSuite</className>
          <testName>outputCol</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.python.MLSerDeSuite.xml</file>
      <name>org.apache.spark.ml.python.MLSerDeSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.007</duration>
      <cases>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.ml.python.MLSerDeSuite</className>
          <testName>pickle vector</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.python.MLSerDeSuite</className>
          <testName>pickle double</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.python.MLSerDeSuite</className>
          <testName>pickle matrix</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.r.RWrapperUtilsSuite.xml</file>
      <name>org.apache.spark.ml.r.RWrapperUtilsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.088</duration>
      <cases>
        <case>
          <duration>0.088</duration>
          <className>org.apache.spark.ml.r.RWrapperUtilsSuite</className>
          <testName>avoid libsvm data column name conflicting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.recommendation.ALSCleanerSuite.xml</file>
      <name>org.apache.spark.ml.recommendation.ALSCleanerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.104</duration>
      <cases>
        <case>
          <duration>0.116</duration>
          <className>org.apache.spark.ml.recommendation.ALSCleanerSuite</className>
          <testName>ALS shuffle cleanup standalone</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.988</duration>
          <className>org.apache.spark.ml.recommendation.ALSCleanerSuite</className>
          <testName>ALS shuffle cleanup in algorithm</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.recommendation.ALSStorageSuite.xml</file>
      <name>org.apache.spark.ml.recommendation.ALSStorageSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.94699997</duration>
      <cases>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.ml.recommendation.ALSStorageSuite</className>
          <testName>invalid storage params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.936</duration>
          <className>org.apache.spark.ml.recommendation.ALSStorageSuite</className>
          <testName>default and non-default storage params set correct RDD StorageLevels</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.recommendation.ALSSuite.xml</file>
      <name>org.apache.spark.ml.recommendation.ALSSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>60.478004</duration>
      <cases>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.ml.recommendation.ALSSuite</className>
          <testName>LocalIndexEncoder</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.ml.recommendation.ALSSuite</className>
          <testName>normal equation construction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.recommendation.ALSSuite</className>
          <testName>CholeskySolver</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.ml.recommendation.ALSSuite</className>
          <testName>RatingBlockBuilder</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.ml.recommendation.ALSSuite</className>
          <testName>UncompressedInBlock</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>7.164</duration>
          <className>org.apache.spark.ml.recommendation.ALSSuite</className>
          <testName>exact rank-1 matrix</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>6.822</duration>
          <className>org.apache.spark.ml.recommendation.ALSSuite</className>
          <testName>approximate rank-1 matrix</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>6.016</duration>
          <className>org.apache.spark.ml.recommendation.ALSSuite</className>
          <testName>approximate rank-2 matrix</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>11.287</duration>
          <className>org.apache.spark.ml.recommendation.ALSSuite</className>
          <testName>different block settings</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.615</duration>
          <className>org.apache.spark.ml.recommendation.ALSSuite</className>
          <testName>more blocks than ratings</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.316</duration>
          <className>org.apache.spark.ml.recommendation.ALSSuite</className>
          <testName>implicit feedback</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.098</duration>
          <className>org.apache.spark.ml.recommendation.ALSSuite</className>
          <testName>using generic ID types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.995</duration>
          <className>org.apache.spark.ml.recommendation.ALSSuite</className>
          <testName>nonnegative constraint</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.ml.recommendation.ALSSuite</className>
          <testName>als partitioner is a projection</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.48</duration>
          <className>org.apache.spark.ml.recommendation.ALSSuite</className>
          <testName>partitioner in returned factors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>6.033</duration>
          <className>org.apache.spark.ml.recommendation.ALSSuite</className>
          <testName>als with large number of iterations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.256</duration>
          <className>org.apache.spark.ml.recommendation.ALSSuite</className>
          <testName>read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>12.368</duration>
          <className>org.apache.spark.ml.recommendation.ALSSuite</className>
          <testName>input type validation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.regression.AFTSurvivalRegressionSuite.xml</file>
      <name>org.apache.spark.ml.regression.AFTSurvivalRegressionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>7.209</duration>
      <cases>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.ml.regression.AFTSurvivalRegressionSuite</className>
          <testName>export test data into CSV format</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.ml.regression.AFTSurvivalRegressionSuite</className>
          <testName>params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.572</duration>
          <className>org.apache.spark.ml.regression.AFTSurvivalRegressionSuite</className>
          <testName>aft survival regression: default params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.503</duration>
          <className>org.apache.spark.ml.regression.AFTSurvivalRegressionSuite</className>
          <testName>aft survival regression with univariate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.313</duration>
          <className>org.apache.spark.ml.regression.AFTSurvivalRegressionSuite</className>
          <testName>aft survival regression with multivariate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.275</duration>
          <className>org.apache.spark.ml.regression.AFTSurvivalRegressionSuite</className>
          <testName>aft survival regression w/o intercept</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.476</duration>
          <className>org.apache.spark.ml.regression.AFTSurvivalRegressionSuite</className>
          <testName>aft survival regression w/o quantiles column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.03</duration>
          <className>org.apache.spark.ml.regression.AFTSurvivalRegressionSuite</className>
          <testName>should support all NumericType labels</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.872</duration>
          <className>org.apache.spark.ml.regression.AFTSurvivalRegressionSuite</className>
          <testName>numerical stability of standardization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.727</duration>
          <className>org.apache.spark.ml.regression.AFTSurvivalRegressionSuite</className>
          <testName>read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.435</duration>
          <className>org.apache.spark.ml.regression.AFTSurvivalRegressionSuite</className>
          <testName>SPARK-15892: Incorrectly merged AFTAggregator with zero total count</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.regression.DecisionTreeRegressorSuite.xml</file>
      <name>org.apache.spark.ml.regression.DecisionTreeRegressorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>4.134</duration>
      <cases>
        <case>
          <duration>0.181</duration>
          <className>org.apache.spark.ml.regression.DecisionTreeRegressorSuite</className>
          <testName>Regression stump with 3-ary (ordered) categorical features</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.176</duration>
          <className>org.apache.spark.ml.regression.DecisionTreeRegressorSuite</className>
          <testName>Regression stump with binary (ordered) categorical features</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.095</duration>
          <className>org.apache.spark.ml.regression.DecisionTreeRegressorSuite</className>
          <testName>copied model must have the same parent</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.283</duration>
          <className>org.apache.spark.ml.regression.DecisionTreeRegressorSuite</className>
          <testName>predictVariance</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.115</duration>
          <className>org.apache.spark.ml.regression.DecisionTreeRegressorSuite</className>
          <testName>Feature importance with toy data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.611</duration>
          <className>org.apache.spark.ml.regression.DecisionTreeRegressorSuite</className>
          <testName>should support all NumericType labels and not support other types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.673</duration>
          <className>org.apache.spark.ml.regression.DecisionTreeRegressorSuite</className>
          <testName>read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.regression.GBTRegressorSuite.xml</file>
      <name>org.apache.spark.ml.regression.GBTRegressorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>36.127</duration>
      <cases>
        <case>
          <duration>18.426</duration>
          <className>org.apache.spark.ml.regression.GBTRegressorSuite</className>
          <testName>Regression with continuous features</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.279</duration>
          <className>org.apache.spark.ml.regression.GBTRegressorSuite</className>
          <testName>GBTRegressor behaves reasonably on toy data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.697</duration>
          <className>org.apache.spark.ml.regression.GBTRegressorSuite</className>
          <testName>Checkpointing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>12.239</duration>
          <className>org.apache.spark.ml.regression.GBTRegressorSuite</className>
          <testName>should support all NumericType labels and not support other types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.423</duration>
          <className>org.apache.spark.ml.regression.GBTRegressorSuite</className>
          <testName>Feature importance with toy data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.063</duration>
          <className>org.apache.spark.ml.regression.GBTRegressorSuite</className>
          <testName>model save/load</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.regression.GeneralizedLinearRegressionSuite.xml</file>
      <name>org.apache.spark.ml.regression.GeneralizedLinearRegressionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>27.403</duration>
      <cases>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.ml.regression.GeneralizedLinearRegressionSuite</className>
          <testName>export test data into CSV format</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.ml.regression.GeneralizedLinearRegressionSuite</className>
          <testName>params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.367</duration>
          <className>org.apache.spark.ml.regression.GeneralizedLinearRegressionSuite</className>
          <testName>generalized linear regression: default params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.52</duration>
          <className>org.apache.spark.ml.regression.GeneralizedLinearRegressionSuite</className>
          <testName>generalized linear regression: gaussian family against glm</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.923</duration>
          <className>org.apache.spark.ml.regression.GeneralizedLinearRegressionSuite</className>
          <testName>generalized linear regression: gaussian family against glmnet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>6.679</duration>
          <className>org.apache.spark.ml.regression.GeneralizedLinearRegressionSuite</className>
          <testName>generalized linear regression: binomial family against glm</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.391</duration>
          <className>org.apache.spark.ml.regression.GeneralizedLinearRegressionSuite</className>
          <testName>generalized linear regression: poisson family against glm</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.392</duration>
          <className>org.apache.spark.ml.regression.GeneralizedLinearRegressionSuite</className>
          <testName>generalized linear regression: poisson family against glm (with zero values)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.773</duration>
          <className>org.apache.spark.ml.regression.GeneralizedLinearRegressionSuite</className>
          <testName>generalized linear regression: gamma family against glm</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.702</duration>
          <className>org.apache.spark.ml.regression.GeneralizedLinearRegressionSuite</className>
          <testName>glm summary: gaussian family with weight</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.28</duration>
          <className>org.apache.spark.ml.regression.GeneralizedLinearRegressionSuite</className>
          <testName>glm summary: binomial family with weight</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.321</duration>
          <className>org.apache.spark.ml.regression.GeneralizedLinearRegressionSuite</className>
          <testName>glm summary: poisson family with weight</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.343</duration>
          <className>org.apache.spark.ml.regression.GeneralizedLinearRegressionSuite</className>
          <testName>glm summary: gamma family with weight</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.ml.regression.GeneralizedLinearRegressionSuite</className>
          <testName>glm handle collinear features</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.161</duration>
          <className>org.apache.spark.ml.regression.GeneralizedLinearRegressionSuite</className>
          <testName>read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.317</duration>
          <className>org.apache.spark.ml.regression.GeneralizedLinearRegressionSuite</className>
          <testName>should support all NumericType labels and not support other types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.354</duration>
          <className>org.apache.spark.ml.regression.GeneralizedLinearRegressionSuite</className>
          <testName>glm accepts Dataset[LabeledPoint]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.422</duration>
          <className>org.apache.spark.ml.regression.GeneralizedLinearRegressionSuite</className>
          <testName>generalized linear regression: regularization parameter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.399</duration>
          <className>org.apache.spark.ml.regression.GeneralizedLinearRegressionSuite</className>
          <testName>evaluate with labels that are not doubles</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.regression.IsotonicRegressionSuite.xml</file>
      <name>org.apache.spark.ml.regression.IsotonicRegressionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.075</duration>
      <cases>
        <case>
          <duration>0.152</duration>
          <className>org.apache.spark.ml.regression.IsotonicRegressionSuite</className>
          <testName>isotonic regression predictions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.113</duration>
          <className>org.apache.spark.ml.regression.IsotonicRegressionSuite</className>
          <testName>antitonic regression predictions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.09</duration>
          <className>org.apache.spark.ml.regression.IsotonicRegressionSuite</className>
          <testName>params validation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.1</duration>
          <className>org.apache.spark.ml.regression.IsotonicRegressionSuite</className>
          <testName>default params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.regression.IsotonicRegressionSuite</className>
          <testName>set parameters</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.077</duration>
          <className>org.apache.spark.ml.regression.IsotonicRegressionSuite</className>
          <testName>missing column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.138</duration>
          <className>org.apache.spark.ml.regression.IsotonicRegressionSuite</className>
          <testName>vector features column with feature index</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.872</duration>
          <className>org.apache.spark.ml.regression.IsotonicRegressionSuite</className>
          <testName>read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.532</duration>
          <className>org.apache.spark.ml.regression.IsotonicRegressionSuite</className>
          <testName>should support all NumericType labels and not support other types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite.xml</file>
      <name>org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.246</duration>
      <cases>
        <case>
          <duration>0.246</duration>
          <className>org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite</className>
          <testName>runDT</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.regression.JavaGBTRegressorSuite.xml</file>
      <name>org.apache.spark.ml.regression.JavaGBTRegressorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.425</duration>
      <cases>
        <case>
          <duration>0.425</duration>
          <className>org.apache.spark.ml.regression.JavaGBTRegressorSuite</className>
          <testName>runDT</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.regression.JavaLinearRegressionSuite.xml</file>
      <name>org.apache.spark.ml.regression.JavaLinearRegressionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.65400004</duration>
      <cases>
        <case>
          <duration>0.27</duration>
          <className>org.apache.spark.ml.regression.JavaLinearRegressionSuite</className>
          <testName>linearRegressionDefaultParams</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.384</duration>
          <className>org.apache.spark.ml.regression.JavaLinearRegressionSuite</className>
          <testName>linearRegressionWithSetters</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.regression.JavaRandomForestRegressorSuite.xml</file>
      <name>org.apache.spark.ml.regression.JavaRandomForestRegressorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.222</duration>
      <cases>
        <case>
          <duration>0.222</duration>
          <className>org.apache.spark.ml.regression.JavaRandomForestRegressorSuite</className>
          <testName>runDT</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.regression.LinearRegressionSuite.xml</file>
      <name>org.apache.spark.ml.regression.LinearRegressionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>37.406</duration>
      <cases>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.ml.regression.LinearRegressionSuite</className>
          <testName>export test data into CSV format</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.regression.LinearRegressionSuite</className>
          <testName>params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.483</duration>
          <className>org.apache.spark.ml.regression.LinearRegressionSuite</className>
          <testName>linear regression: default params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.015</duration>
          <className>org.apache.spark.ml.regression.LinearRegressionSuite</className>
          <testName>linear regression handles singular matrices</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.855</duration>
          <className>org.apache.spark.ml.regression.LinearRegressionSuite</className>
          <testName>linear regression with intercept without regularization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.594</duration>
          <className>org.apache.spark.ml.regression.LinearRegressionSuite</className>
          <testName>linear regression without intercept without regularization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.929</duration>
          <className>org.apache.spark.ml.regression.LinearRegressionSuite</className>
          <testName>linear regression with intercept with L1 regularization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.988</duration>
          <className>org.apache.spark.ml.regression.LinearRegressionSuite</className>
          <testName>linear regression without intercept with L1 regularization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.673</duration>
          <className>org.apache.spark.ml.regression.LinearRegressionSuite</className>
          <testName>linear regression with intercept with L2 regularization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.899</duration>
          <className>org.apache.spark.ml.regression.LinearRegressionSuite</className>
          <testName>linear regression without intercept with L2 regularization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.905</duration>
          <className>org.apache.spark.ml.regression.LinearRegressionSuite</className>
          <testName>linear regression with intercept with ElasticNet regularization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.165</duration>
          <className>org.apache.spark.ml.regression.LinearRegressionSuite</className>
          <testName>linear regression without intercept with ElasticNet regularization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.84</duration>
          <className>org.apache.spark.ml.regression.LinearRegressionSuite</className>
          <testName>linear regression model with constant label</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.317</duration>
          <className>org.apache.spark.ml.regression.LinearRegressionSuite</className>
          <testName>regularized linear regression through origin with constant label</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.443</duration>
          <className>org.apache.spark.ml.regression.LinearRegressionSuite</className>
          <testName>linear regression with l-bfgs when training is not needed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.107</duration>
          <className>org.apache.spark.ml.regression.LinearRegressionSuite</className>
          <testName>linear regression model training summary</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.525</duration>
          <className>org.apache.spark.ml.regression.LinearRegressionSuite</className>
          <testName>linear regression model testset evaluation summary</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>6.754</duration>
          <className>org.apache.spark.ml.regression.LinearRegressionSuite</className>
          <testName>linear regression with weighted samples</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.991</duration>
          <className>org.apache.spark.ml.regression.LinearRegressionSuite</className>
          <testName>linear regression model with l-bfgs with big feature datasets</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.403</duration>
          <className>org.apache.spark.ml.regression.LinearRegressionSuite</className>
          <testName>linear regression summary with weighted samples and intercept by normal solver</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.183</duration>
          <className>org.apache.spark.ml.regression.LinearRegressionSuite</className>
          <testName>linear regression summary with weighted samples and w/o intercept by normal solver</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.892</duration>
          <className>org.apache.spark.ml.regression.LinearRegressionSuite</className>
          <testName>read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.444</duration>
          <className>org.apache.spark.ml.regression.LinearRegressionSuite</className>
          <testName>should support all NumericType labels and not support other types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.regression.RandomForestRegressorSuite.xml</file>
      <name>org.apache.spark.ml.regression.RandomForestRegressorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>3.6360002</duration>
      <cases>
        <case>
          <duration>0.519</duration>
          <className>org.apache.spark.ml.regression.RandomForestRegressorSuite</className>
          <testName> RandomForest(numTrees = 1)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.452</duration>
          <className>org.apache.spark.ml.regression.RandomForestRegressorSuite</className>
          <testName> RandomForest(numTrees = 1)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.134</duration>
          <className>org.apache.spark.ml.regression.RandomForestRegressorSuite</className>
          <testName>Feature importance with toy data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.746</duration>
          <className>org.apache.spark.ml.regression.RandomForestRegressorSuite</className>
          <testName>should support all NumericType labels and not support other types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.785</duration>
          <className>org.apache.spark.ml.regression.RandomForestRegressorSuite</className>
          <testName>read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite.xml</file>
      <name>org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.175</duration>
      <cases>
        <case>
          <duration>0.175</duration>
          <className>org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite</className>
          <testName>verifyLibSVMDF</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.source.libsvm.LibSVMRelationSuite.xml</file>
      <name>org.apache.spark.ml.source.libsvm.LibSVMRelationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.596</duration>
      <cases>
        <case>
          <duration>0.094</duration>
          <className>org.apache.spark.ml.source.libsvm.LibSVMRelationSuite</className>
          <testName>select as sparse vector</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.129</duration>
          <className>org.apache.spark.ml.source.libsvm.LibSVMRelationSuite</className>
          <testName>select as dense vector</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.ml.source.libsvm.LibSVMRelationSuite</className>
          <testName>select a vector with specifying the longer dimension</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.163</duration>
          <className>org.apache.spark.ml.source.libsvm.LibSVMRelationSuite</className>
          <testName>write libsvm data and read it again</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.078</duration>
          <className>org.apache.spark.ml.source.libsvm.LibSVMRelationSuite</className>
          <testName>write libsvm data failed due to invalid schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.101</duration>
          <className>org.apache.spark.ml.source.libsvm.LibSVMRelationSuite</className>
          <testName>select features from libsvm relation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.tree.impl.BaggedPointSuite.xml</file>
      <name>org.apache.spark.ml.tree.impl.BaggedPointSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.47600004</duration>
      <cases>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.ml.tree.impl.BaggedPointSuite</className>
          <testName>BaggedPoint RDD: without subsampling</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.136</duration>
          <className>org.apache.spark.ml.tree.impl.BaggedPointSuite</className>
          <testName>0)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.127</duration>
          <className>org.apache.spark.ml.tree.impl.BaggedPointSuite</className>
          <testName>5)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.092</duration>
          <className>org.apache.spark.ml.tree.impl.BaggedPointSuite</className>
          <testName>0)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.099</duration>
          <className>org.apache.spark.ml.tree.impl.BaggedPointSuite</className>
          <testName>5)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.tree.impl.GradientBoostedTreesSuite.xml</file>
      <name>org.apache.spark.ml.tree.impl.GradientBoostedTreesSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>7.862</duration>
      <cases>
        <case>
          <duration>7.862</duration>
          <className>org.apache.spark.ml.tree.impl.GradientBoostedTreesSuite</className>
          <testName>runWithValidation stops early and performs better on a validation dataset</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.tree.impl.RandomForestSuite.xml</file>
      <name>org.apache.spark.ml.tree.impl.RandomForestSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.861</duration>
      <cases>
        <case>
          <duration>0.066</duration>
          <className>org.apache.spark.ml.tree.impl.RandomForestSuite</className>
          <testName>Binary classification with continuous features: split calculation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.038</duration>
          <className>org.apache.spark.ml.tree.impl.RandomForestSuite</className>
          <testName>Binary classification with binary (ordered) categorical features: split calculation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.ml.tree.impl.RandomForestSuite</className>
          <testName>Binary classification with 3-ary (ordered) categorical features, with no samples for one category: split calculation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.595</duration>
          <className>org.apache.spark.ml.tree.impl.RandomForestSuite</className>
          <testName>find splits for a continuous feature</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.068</duration>
          <className>org.apache.spark.ml.tree.impl.RandomForestSuite</className>
          <testName>train with constant features</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.ml.tree.impl.RandomForestSuite</className>
          <testName>Multiclass classification with unordered categorical features: split calculations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.ml.tree.impl.RandomForestSuite</className>
          <testName>Multiclass classification with ordered categorical features: split calculations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.tree.impl.RandomForestSuite</className>
          <testName>extract categories from a number for multiclass classification</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.064</duration>
          <className>org.apache.spark.ml.tree.impl.RandomForestSuite</className>
          <testName>Avoid aggregation on the last level</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.06</duration>
          <className>org.apache.spark.ml.tree.impl.RandomForestSuite</className>
          <testName>0</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.ml.tree.impl.RandomForestSuite</className>
          <testName>Use soft prediction for binary classification with ordered categorical features</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.247</duration>
          <className>org.apache.spark.ml.tree.impl.RandomForestSuite</className>
          <testName> without groups</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.788</duration>
          <className>org.apache.spark.ml.tree.impl.RandomForestSuite</className>
          <testName>Binary classification with continuous features: subsampling features</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.793</duration>
          <className>org.apache.spark.ml.tree.impl.RandomForestSuite</className>
          <testName>Binary classification with continuous features and node Id cache: subsampling features</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.ml.tree.impl.RandomForestSuite</className>
          <testName>computeFeatureImportance, featureImportances</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.ml.tree.impl.RandomForestSuite</className>
          <testName>normalizeMapValues</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.tuning.CrossValidatorSuite.xml</file>
      <name>org.apache.spark.ml.tuning.CrossValidatorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>14.858999</duration>
      <cases>
        <case>
          <duration>4.46</duration>
          <className>org.apache.spark.ml.tuning.CrossValidatorSuite</className>
          <testName>cross validation with logistic regression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>6.693</duration>
          <className>org.apache.spark.ml.tuning.CrossValidatorSuite</className>
          <testName>cross validation with linear regression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.ml.tuning.CrossValidatorSuite</className>
          <testName>transformSchema should check estimatorParamMaps</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.664</duration>
          <className>org.apache.spark.ml.tuning.CrossValidatorSuite</className>
          <testName>read/write: CrossValidator with simple estimator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.658</duration>
          <className>org.apache.spark.ml.tuning.CrossValidatorSuite</className>
          <testName>read/write: CrossValidator with complex estimator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.tuning.CrossValidatorSuite</className>
          <testName>read/write: CrossValidator fails for extraneous Param</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.379</duration>
          <className>org.apache.spark.ml.tuning.CrossValidatorSuite</className>
          <testName>read/write: CrossValidatorModel</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.tuning.JavaCrossValidatorSuite.xml</file>
      <name>org.apache.spark.ml.tuning.JavaCrossValidatorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>3.376</duration>
      <cases>
        <case>
          <duration>3.376</duration>
          <className>org.apache.spark.ml.tuning.JavaCrossValidatorSuite</className>
          <testName>crossValidationWithLogisticRegression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.tuning.ParamGridBuilderSuite.xml</file>
      <name>org.apache.spark.ml.tuning.ParamGridBuilderSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.002</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.tuning.ParamGridBuilderSuite</className>
          <testName>param grid builder</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.tuning.TrainValidationSplitSuite.xml</file>
      <name>org.apache.spark.ml.tuning.TrainValidationSplitSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>5.4149995</duration>
      <cases>
        <case>
          <duration>1.42</duration>
          <className>org.apache.spark.ml.tuning.TrainValidationSplitSuite</className>
          <testName>train validation with logistic regression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.123</duration>
          <className>org.apache.spark.ml.tuning.TrainValidationSplitSuite</className>
          <testName>train validation with linear regression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.ml.tuning.TrainValidationSplitSuite</className>
          <testName>transformSchema should check estimatorParamMaps</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.638</duration>
          <className>org.apache.spark.ml.tuning.TrainValidationSplitSuite</className>
          <testName>read/write: TrainValidationSplit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.232</duration>
          <className>org.apache.spark.ml.tuning.TrainValidationSplitSuite</className>
          <testName>read/write: TrainValidationSplitModel</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.util.DefaultReadWriteSuite.xml</file>
      <name>org.apache.spark.ml.util.DefaultReadWriteSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.159</duration>
      <cases>
        <case>
          <duration>0.159</duration>
          <className>org.apache.spark.ml.util.DefaultReadWriteSuite</className>
          <testName>default read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.util.IdentifiableSuite.xml</file>
      <name>org.apache.spark.ml.util.IdentifiableSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.001</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.ml.util.IdentifiableSuite</className>
          <testName>Identifiable</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.util.JavaDefaultReadWriteSuite.xml</file>
      <name>org.apache.spark.ml.util.JavaDefaultReadWriteSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.194</duration>
      <cases>
        <case>
          <duration>0.194</duration>
          <className>org.apache.spark.ml.util.JavaDefaultReadWriteSuite</className>
          <testName>testDefaultReadWrite</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.ml.util.StopwatchSuite.xml</file>
      <name>org.apache.spark.ml.util.StopwatchSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.068</duration>
      <cases>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.ml.util.StopwatchSuite</className>
          <testName>LocalStopwatch</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.ml.util.StopwatchSuite</className>
          <testName>DistributedStopwatch on driver</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.ml.util.StopwatchSuite</className>
          <testName>DistributedStopwatch on executors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.ml.util.StopwatchSuite</className>
          <testName>MultiStopwatch</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.api.python.PythonMLLibAPISuite.xml</file>
      <name>org.apache.spark.mllib.api.python.PythonMLLibAPISuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.008</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.mllib.api.python.PythonMLLibAPISuite</className>
          <testName>pickle vector</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.mllib.api.python.PythonMLLibAPISuite</className>
          <testName>pickle labeled point</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.api.python.PythonMLLibAPISuite</className>
          <testName>pickle double</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.api.python.PythonMLLibAPISuite</className>
          <testName>pickle matrix</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.mllib.api.python.PythonMLLibAPISuite</className>
          <testName>pickle rating</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.classification.JavaLogisticRegressionSuite.xml</file>
      <name>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>3.6000001</duration>
      <cases>
        <case>
          <duration>0.424</duration>
          <className>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite</className>
          <testName>runLRUsingConstructor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.176</duration>
          <className>org.apache.spark.mllib.classification.JavaLogisticRegressionSuite</className>
          <testName>runLRUsingStaticMethods</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.classification.JavaNaiveBayesSuite.xml</file>
      <name>org.apache.spark.mllib.classification.JavaNaiveBayesSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.552</duration>
      <cases>
        <case>
          <duration>0.177</duration>
          <className>org.apache.spark.mllib.classification.JavaNaiveBayesSuite</className>
          <testName>testPredictJavaRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.135</duration>
          <className>org.apache.spark.mllib.classification.JavaNaiveBayesSuite</className>
          <testName>runUsingConstructor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.178</duration>
          <className>org.apache.spark.mllib.classification.JavaNaiveBayesSuite</className>
          <testName>runUsingStaticMethods</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.062</duration>
          <className>org.apache.spark.mllib.classification.JavaNaiveBayesSuite</className>
          <testName>testModelTypeSetters</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.classification.JavaSVMSuite.xml</file>
      <name>org.apache.spark.mllib.classification.JavaSVMSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.051</duration>
      <cases>
        <case>
          <duration>0.804</duration>
          <className>org.apache.spark.mllib.classification.JavaSVMSuite</className>
          <testName>runSVMUsingConstructor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.247</duration>
          <className>org.apache.spark.mllib.classification.JavaSVMSuite</className>
          <testName>runSVMUsingStaticMethods</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite.xml</file>
      <name>org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.399</duration>
      <cases>
        <case>
          <duration>0.399</duration>
          <className>org.apache.spark.mllib.classification.JavaStreamingLogisticRegressionSuite</className>
          <testName>javaAPI</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.classification.LogisticRegressionClusterSuite.xml</file>
      <name>org.apache.spark.mllib.classification.LogisticRegressionClusterSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>6.5620003</duration>
      <cases>
        <case>
          <duration>5.362</duration>
          <className>org.apache.spark.mllib.classification.LogisticRegressionClusterSuite</className>
          <testName>task size should be small in both training and prediction using SGD optimizer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.2</duration>
          <className>org.apache.spark.mllib.classification.LogisticRegressionClusterSuite</className>
          <testName>task size should be small in both training and prediction using LBFGS optimizer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.classification.LogisticRegressionSuite.xml</file>
      <name>org.apache.spark.mllib.classification.LogisticRegressionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>34.487</duration>
      <cases>
        <case>
          <duration>0.729</duration>
          <className>org.apache.spark.mllib.classification.LogisticRegressionSuite</className>
          <testName>logistic regression with SGD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.163</duration>
          <className>org.apache.spark.mllib.classification.LogisticRegressionSuite</className>
          <testName>logistic regression with LBFGS</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.57</duration>
          <className>org.apache.spark.mllib.classification.LogisticRegressionSuite</className>
          <testName>logistic regression with initial weights with SGD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.414</duration>
          <className>org.apache.spark.mllib.classification.LogisticRegressionSuite</className>
          <testName>logistic regression with initial weights and non-default regularization parameter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.516</duration>
          <className>org.apache.spark.mllib.classification.LogisticRegressionSuite</className>
          <testName>logistic regression with initial weights with LBFGS</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.499</duration>
          <className>org.apache.spark.mllib.classification.LogisticRegressionSuite</className>
          <testName>numerical stability of scaling features using logistic regression with LBFGS</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>11.466</duration>
          <className>org.apache.spark.mllib.classification.LogisticRegressionSuite</className>
          <testName>multinomial logistic regression with LBFGS</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.633</duration>
          <className>org.apache.spark.mllib.classification.LogisticRegressionSuite</className>
          <testName>model save/load: binary classification</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.272</duration>
          <className>org.apache.spark.mllib.classification.LogisticRegressionSuite</className>
          <testName>model save/load: multiclass classification</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.071</duration>
          <className>org.apache.spark.mllib.classification.LogisticRegressionSuite</className>
          <testName>binary logistic regression with intercept without regularization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.26</duration>
          <className>org.apache.spark.mllib.classification.LogisticRegressionSuite</className>
          <testName>binary logistic regression without intercept without regularization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>5.209</duration>
          <className>org.apache.spark.mllib.classification.LogisticRegressionSuite</className>
          <testName>binary logistic regression with intercept with L1 regularization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.578</duration>
          <className>org.apache.spark.mllib.classification.LogisticRegressionSuite</className>
          <testName>binary logistic regression without intercept with L1 regularization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.949</duration>
          <className>org.apache.spark.mllib.classification.LogisticRegressionSuite</className>
          <testName>binary logistic regression with intercept with L2 regularization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.158</duration>
          <className>org.apache.spark.mllib.classification.LogisticRegressionSuite</className>
          <testName>binary logistic regression without intercept with L2 regularization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.classification.NaiveBayesClusterSuite.xml</file>
      <name>org.apache.spark.mllib.classification.NaiveBayesClusterSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>5.384</duration>
      <cases>
        <case>
          <duration>5.384</duration>
          <className>org.apache.spark.mllib.classification.NaiveBayesClusterSuite</className>
          <testName>task size should be small in both training and prediction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.classification.NaiveBayesSuite.xml</file>
      <name>org.apache.spark.mllib.classification.NaiveBayesSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.187</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.classification.NaiveBayesSuite</className>
          <testName>model types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.mllib.classification.NaiveBayesSuite</className>
          <testName>get, set params</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.308</duration>
          <className>org.apache.spark.mllib.classification.NaiveBayesSuite</className>
          <testName>Naive Bayes Multinomial</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.603</duration>
          <className>org.apache.spark.mllib.classification.NaiveBayesSuite</className>
          <testName>Naive Bayes Bernoulli</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.188</duration>
          <className>org.apache.spark.mllib.classification.NaiveBayesSuite</className>
          <testName>detect negative values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.151</duration>
          <className>org.apache.spark.mllib.classification.NaiveBayesSuite</className>
          <testName>detect non zero or one values in Bernoulli</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.607</duration>
          <className>org.apache.spark.mllib.classification.NaiveBayesSuite</className>
          <testName>0</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.327</duration>
          <className>org.apache.spark.mllib.classification.NaiveBayesSuite</className>
          <testName>0</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.classification.SVMClusterSuite.xml</file>
      <name>org.apache.spark.mllib.classification.SVMClusterSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>4.919</duration>
      <cases>
        <case>
          <duration>4.919</duration>
          <className>org.apache.spark.mllib.classification.SVMClusterSuite</className>
          <testName>task size should be small in both training and prediction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.classification.SVMSuite.xml</file>
      <name>org.apache.spark.mllib.classification.SVMSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>5.124</duration>
      <cases>
        <case>
          <duration>0.684</duration>
          <className>org.apache.spark.mllib.classification.SVMSuite</className>
          <testName>SVM with threshold</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.51</duration>
          <className>org.apache.spark.mllib.classification.SVMSuite</className>
          <testName>SVM using local random SGD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.452</duration>
          <className>org.apache.spark.mllib.classification.SVMSuite</className>
          <testName>SVM local random SGD with initial weights</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.981</duration>
          <className>org.apache.spark.mllib.classification.SVMSuite</className>
          <testName>SVM with invalid labels</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.497</duration>
          <className>org.apache.spark.mllib.classification.SVMSuite</className>
          <testName>model save/load</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.classification.StreamingLogisticRegressionSuite.xml</file>
      <name>org.apache.spark.mllib.classification.StreamingLogisticRegressionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>10.923</duration>
      <cases>
        <case>
          <duration>4.255</duration>
          <className>org.apache.spark.mllib.classification.StreamingLogisticRegressionSuite</className>
          <testName>parameter accuracy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.29</duration>
          <className>org.apache.spark.mllib.classification.StreamingLogisticRegressionSuite</className>
          <testName>parameter convergence</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.272</duration>
          <className>org.apache.spark.mllib.classification.StreamingLogisticRegressionSuite</className>
          <testName>predictions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.636</duration>
          <className>org.apache.spark.mllib.classification.StreamingLogisticRegressionSuite</className>
          <testName>training and prediction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.47</duration>
          <className>org.apache.spark.mllib.classification.StreamingLogisticRegressionSuite</className>
          <testName>handling empty RDDs in a stream</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.clustering.BisectingKMeansSuite.xml</file>
      <name>org.apache.spark.mllib.clustering.BisectingKMeansSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.328</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.mllib.clustering.BisectingKMeansSuite</className>
          <testName>default values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.mllib.clustering.BisectingKMeansSuite</className>
          <testName>setter/getter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.188</duration>
          <className>org.apache.spark.mllib.clustering.BisectingKMeansSuite</className>
          <testName>1D data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.mllib.clustering.BisectingKMeansSuite</className>
          <testName>points are the same</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.14</duration>
          <className>org.apache.spark.mllib.clustering.BisectingKMeansSuite</className>
          <testName>more desired clusters than points</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.181</duration>
          <className>org.apache.spark.mllib.clustering.BisectingKMeansSuite</className>
          <testName>min divisible cluster</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.101</duration>
          <className>org.apache.spark.mllib.clustering.BisectingKMeansSuite</className>
          <testName>larger clusters get selected first</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.242</duration>
          <className>org.apache.spark.mllib.clustering.BisectingKMeansSuite</className>
          <testName>2D data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.436</duration>
          <className>org.apache.spark.mllib.clustering.BisectingKMeansSuite</className>
          <testName>BisectingKMeans model save/load</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.clustering.GaussianMixtureSuite.xml</file>
      <name>org.apache.spark.mllib.clustering.GaussianMixtureSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.8629999</duration>
      <cases>
        <case>
          <duration>0.37</duration>
          <className>org.apache.spark.mllib.clustering.GaussianMixtureSuite</className>
          <testName>single cluster</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.044</duration>
          <className>org.apache.spark.mllib.clustering.GaussianMixtureSuite</className>
          <testName>two clusters</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.407</duration>
          <className>org.apache.spark.mllib.clustering.GaussianMixtureSuite</className>
          <testName>two clusters with distributed decompositions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.384</duration>
          <className>org.apache.spark.mllib.clustering.GaussianMixtureSuite</className>
          <testName>single cluster with sparse data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.062</duration>
          <className>org.apache.spark.mllib.clustering.GaussianMixtureSuite</className>
          <testName>two clusters with sparse data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.48</duration>
          <className>org.apache.spark.mllib.clustering.GaussianMixtureSuite</className>
          <testName>model save / load</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.116</duration>
          <className>org.apache.spark.mllib.clustering.GaussianMixtureSuite</className>
          <testName>model prediction, parallel and local</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.clustering.JavaBisectingKMeansSuite.xml</file>
      <name>org.apache.spark.mllib.clustering.JavaBisectingKMeansSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.184</duration>
      <cases>
        <case>
          <duration>0.184</duration>
          <className>org.apache.spark.mllib.clustering.JavaBisectingKMeansSuite</className>
          <testName>twoDimensionalData</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.clustering.JavaGaussianMixtureSuite.xml</file>
      <name>org.apache.spark.mllib.clustering.JavaGaussianMixtureSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.098</duration>
      <cases>
        <case>
          <duration>0.098</duration>
          <className>org.apache.spark.mllib.clustering.JavaGaussianMixtureSuite</className>
          <testName>runGaussianMixture</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.clustering.JavaKMeansSuite.xml</file>
      <name>org.apache.spark.mllib.clustering.JavaKMeansSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.604</duration>
      <cases>
        <case>
          <duration>0.172</duration>
          <className>org.apache.spark.mllib.clustering.JavaKMeansSuite</className>
          <testName>testPredictJavaRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.226</duration>
          <className>org.apache.spark.mllib.clustering.JavaKMeansSuite</className>
          <testName>runKMeansUsingConstructor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.206</duration>
          <className>org.apache.spark.mllib.clustering.JavaKMeansSuite</className>
          <testName>runKMeansUsingStaticMethods</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.clustering.JavaLDASuite.xml</file>
      <name>org.apache.spark.mllib.clustering.JavaLDASuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.90099996</duration>
      <cases>
        <case>
          <duration>0.246</duration>
          <className>org.apache.spark.mllib.clustering.JavaLDASuite</className>
          <testName>onlineOptimizerCompatibility</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.506</duration>
          <className>org.apache.spark.mllib.clustering.JavaLDASuite</className>
          <testName>distributedLDAModel</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.05</duration>
          <className>org.apache.spark.mllib.clustering.JavaLDASuite</className>
          <testName>localLDAModel</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.099</duration>
          <className>org.apache.spark.mllib.clustering.JavaLDASuite</className>
          <testName>localLdaMethods</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.clustering.JavaStreamingKMeansSuite.xml</file>
      <name>org.apache.spark.mllib.clustering.JavaStreamingKMeansSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.274</duration>
      <cases>
        <case>
          <duration>0.274</duration>
          <className>org.apache.spark.mllib.clustering.JavaStreamingKMeansSuite</className>
          <testName>javaAPI</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.clustering.KMeansClusterSuite.xml</file>
      <name>org.apache.spark.mllib.clustering.KMeansClusterSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>5.365</duration>
      <cases>
        <case>
          <duration>5.365</duration>
          <className>org.apache.spark.mllib.clustering.KMeansClusterSuite</className>
          <testName>task size should be small in both training and prediction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.clustering.KMeansSuite.xml</file>
      <name>org.apache.spark.mllib.clustering.KMeansSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>4.765</duration>
      <cases>
        <case>
          <duration>0.533</duration>
          <className>org.apache.spark.mllib.clustering.KMeansSuite</className>
          <testName>single cluster</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.129</duration>
          <className>org.apache.spark.mllib.clustering.KMeansSuite</className>
          <testName>fewer distinct points than clusters</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.173</duration>
          <className>org.apache.spark.mllib.clustering.KMeansSuite</className>
          <testName>unique cluster centers</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.547</duration>
          <className>org.apache.spark.mllib.clustering.KMeansSuite</className>
          <testName>deterministic initialization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.58</duration>
          <className>org.apache.spark.mllib.clustering.KMeansSuite</className>
          <testName>single cluster with big dataset</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.796</duration>
          <className>org.apache.spark.mllib.clustering.KMeansSuite</className>
          <testName>single cluster with sparse data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.192</duration>
          <className>org.apache.spark.mllib.clustering.KMeansSuite</className>
          <testName>k-means|| initialization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.238</duration>
          <className>org.apache.spark.mllib.clustering.KMeansSuite</className>
          <testName>two clusters</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.572</duration>
          <className>org.apache.spark.mllib.clustering.KMeansSuite</className>
          <testName>model save/load</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.mllib.clustering.KMeansSuite</className>
          <testName>Initialize using given cluster centers</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.clustering.LDASuite.xml</file>
      <name>org.apache.spark.mllib.clustering.LDASuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>12.594001</duration>
      <cases>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.mllib.clustering.LDASuite</className>
          <testName>LocalLDAModel</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.723</duration>
          <className>org.apache.spark.mllib.clustering.LDASuite</className>
          <testName>running and DistributedLDAModel with default Optimizer (EM)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.mllib.clustering.LDASuite</className>
          <testName>vertex indexing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.mllib.clustering.LDASuite</className>
          <testName>setter alias</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.clustering.LDASuite</className>
          <testName>initializing with alpha length != k or 1 fails</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.mllib.clustering.LDASuite</className>
          <testName>initializing with elements in alpha &lt; 0 fails</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.mllib.clustering.LDASuite</className>
          <testName>OnlineLDAOptimizer initialization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.08</duration>
          <className>org.apache.spark.mllib.clustering.LDASuite</className>
          <testName>OnlineLDAOptimizer one iteration</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.303</duration>
          <className>org.apache.spark.mllib.clustering.LDASuite</className>
          <testName>OnlineLDAOptimizer with toy data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.mllib.clustering.LDASuite</className>
          <testName>LocalLDAModel logLikelihood</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.018</duration>
          <className>org.apache.spark.mllib.clustering.LDASuite</className>
          <testName>LocalLDAModel logPerplexity</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.05</duration>
          <className>org.apache.spark.mllib.clustering.LDASuite</className>
          <testName>LocalLDAModel predict</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.254</duration>
          <className>org.apache.spark.mllib.clustering.LDASuite</className>
          <testName>OnlineLDAOptimizer with asymmetric prior</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.272</duration>
          <className>org.apache.spark.mllib.clustering.LDASuite</className>
          <testName>OnlineLDAOptimizer alpha hyperparameter optimization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.455</duration>
          <className>org.apache.spark.mllib.clustering.LDASuite</className>
          <testName>model save/load</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.263</duration>
          <className>org.apache.spark.mllib.clustering.LDASuite</className>
          <testName>EMLDAOptimizer with empty docs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.092</duration>
          <className>org.apache.spark.mllib.clustering.LDASuite</className>
          <testName>OnlineLDAOptimizer with empty docs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.clustering.PowerIterationClusteringSuite.xml</file>
      <name>org.apache.spark.mllib.clustering.PowerIterationClusteringSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>13.474001</duration>
      <cases>
        <case>
          <duration>6.484</duration>
          <className>org.apache.spark.mllib.clustering.PowerIterationClusteringSuite</className>
          <testName>power iteration clustering</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>6.48</duration>
          <className>org.apache.spark.mllib.clustering.PowerIterationClusteringSuite</className>
          <testName>power iteration clustering on graph</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.18</duration>
          <className>org.apache.spark.mllib.clustering.PowerIterationClusteringSuite</className>
          <testName>normalize and powerIter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.33</duration>
          <className>org.apache.spark.mllib.clustering.PowerIterationClusteringSuite</className>
          <testName>model save/load</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.clustering.StreamingKMeansSuite.xml</file>
      <name>org.apache.spark.mllib.clustering.StreamingKMeansSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.921</duration>
      <cases>
        <case>
          <duration>0.651</duration>
          <className>org.apache.spark.mllib.clustering.StreamingKMeansSuite</className>
          <testName>accuracy for single center and equivalence to grand average</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.64</duration>
          <className>org.apache.spark.mllib.clustering.StreamingKMeansSuite</className>
          <testName>accuracy for two centers</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.63</duration>
          <className>org.apache.spark.mllib.clustering.StreamingKMeansSuite</className>
          <testName>detecting dying clusters</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.clustering.StreamingKMeansSuite</className>
          <testName>SPARK-7946 setDecayFactor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.evaluation.AreaUnderCurveSuite.xml</file>
      <name>org.apache.spark.mllib.evaluation.AreaUnderCurveSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.04</duration>
      <cases>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.mllib.evaluation.AreaUnderCurveSuite</className>
          <testName>auc computation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.mllib.evaluation.AreaUnderCurveSuite</className>
          <testName>auc of an empty curve</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.mllib.evaluation.AreaUnderCurveSuite</className>
          <testName>auc of a curve with a single point</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.evaluation.BinaryClassificationMetricsSuite.xml</file>
      <name>org.apache.spark.mllib.evaluation.BinaryClassificationMetricsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.526</duration>
      <cases>
        <case>
          <duration>0.162</duration>
          <className>org.apache.spark.mllib.evaluation.BinaryClassificationMetricsSuite</className>
          <testName>binary evaluation metrics</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.132</duration>
          <className>org.apache.spark.mllib.evaluation.BinaryClassificationMetricsSuite</className>
          <testName>binary evaluation metrics for RDD where all examples have positive label</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.148</duration>
          <className>org.apache.spark.mllib.evaluation.BinaryClassificationMetricsSuite</className>
          <testName>binary evaluation metrics for RDD where all examples have negative label</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.084</duration>
          <className>org.apache.spark.mllib.evaluation.BinaryClassificationMetricsSuite</className>
          <testName>binary evaluation metrics with downsampling</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.evaluation.JavaRankingMetricsSuite.xml</file>
      <name>org.apache.spark.mllib.evaluation.JavaRankingMetricsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.108</duration>
      <cases>
        <case>
          <duration>0.108</duration>
          <className>org.apache.spark.mllib.evaluation.JavaRankingMetricsSuite</className>
          <testName>rankingMetrics</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.evaluation.MulticlassMetricsSuite.xml</file>
      <name>org.apache.spark.mllib.evaluation.MulticlassMetricsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.079</duration>
      <cases>
        <case>
          <duration>0.079</duration>
          <className>org.apache.spark.mllib.evaluation.MulticlassMetricsSuite</className>
          <testName>Multiclass evaluation metrics</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.evaluation.MultilabelMetricsSuite.xml</file>
      <name>org.apache.spark.mllib.evaluation.MultilabelMetricsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.222</duration>
      <cases>
        <case>
          <duration>0.222</duration>
          <className>org.apache.spark.mllib.evaluation.MultilabelMetricsSuite</className>
          <testName>Multilabel evaluation metrics</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.evaluation.RankingMetricsSuite.xml</file>
      <name>org.apache.spark.mllib.evaluation.RankingMetricsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.164</duration>
      <cases>
        <case>
          <duration>0.128</duration>
          <className>org.apache.spark.mllib.evaluation.RankingMetricsSuite</className>
          <testName>Ranking metrics: MAP, NDCG</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.mllib.evaluation.RankingMetricsSuite</className>
          <testName>MAP, NDCG with few predictions (SPARK-14886)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.evaluation.RegressionMetricsSuite.xml</file>
      <name>org.apache.spark.mllib.evaluation.RegressionMetricsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.052</duration>
      <cases>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.mllib.evaluation.RegressionMetricsSuite</className>
          <testName>regression metrics for unbiased (includes intercept term) predictor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.mllib.evaluation.RegressionMetricsSuite</className>
          <testName>regression metrics for biased (no intercept term) predictor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.mllib.evaluation.RegressionMetricsSuite</className>
          <testName>regression metrics with complete fitting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.feature.ChiSqSelectorSuite.xml</file>
      <name>org.apache.spark.mllib.feature.ChiSqSelectorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.341</duration>
      <cases>
        <case>
          <duration>0.057</duration>
          <className>org.apache.spark.mllib.feature.ChiSqSelectorSuite</className>
          <testName>ChiSqSelector transform test (sparse &amp; dense vector)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.mllib.feature.ChiSqSelectorSuite</className>
          <testName>ChiSqSelector by fpr transform test (sparse &amp; dense vector)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.248</duration>
          <className>org.apache.spark.mllib.feature.ChiSqSelectorSuite</className>
          <testName>model load / save</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.feature.ElementwiseProductSuite.xml</file>
      <name>org.apache.spark.mllib.feature.ElementwiseProductSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.041</duration>
      <cases>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.mllib.feature.ElementwiseProductSuite</className>
          <testName>elementwise (hadamard) product should properly apply vector to dense data set</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.mllib.feature.ElementwiseProductSuite</className>
          <testName>elementwise (hadamard) product should properly apply vector to sparse data set</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.feature.HashingTFSuite.xml</file>
      <name>org.apache.spark.mllib.feature.HashingTFSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.024000002</duration>
      <cases>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.mllib.feature.HashingTFSuite</className>
          <testName>hashing tf on a single doc</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.mllib.feature.HashingTFSuite</className>
          <testName>hashing tf on an RDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.feature.HashingTFSuite</className>
          <testName>applying binary term freqs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.feature.IDFSuite.xml</file>
      <name>org.apache.spark.mllib.feature.IDFSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.052</duration>
      <cases>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.mllib.feature.IDFSuite</className>
          <testName>idf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.mllib.feature.IDFSuite</className>
          <testName>idf minimum document frequency filtering</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.feature.JavaTfIdfSuite.xml</file>
      <name>org.apache.spark.mllib.feature.JavaTfIdfSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.51</duration>
      <cases>
        <case>
          <duration>0.247</duration>
          <className>org.apache.spark.mllib.feature.JavaTfIdfSuite</className>
          <testName>tfIdfMinimumDocumentFrequency</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.263</duration>
          <className>org.apache.spark.mllib.feature.JavaTfIdfSuite</className>
          <testName>tfIdf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.feature.JavaWord2VecSuite.xml</file>
      <name>org.apache.spark.mllib.feature.JavaWord2VecSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.151</duration>
      <cases>
        <case>
          <duration>0.151</duration>
          <className>org.apache.spark.mllib.feature.JavaWord2VecSuite</className>
          <testName>word2Vec</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.feature.NormalizerSuite.xml</file>
      <name>org.apache.spark.mllib.feature.NormalizerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.061</duration>
      <cases>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.mllib.feature.NormalizerSuite</className>
          <testName>Normalization using L1 distance</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.mllib.feature.NormalizerSuite</className>
          <testName>Normalization using L2 distance</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.mllib.feature.NormalizerSuite</className>
          <testName>Normalization using L^Inf distance</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.feature.PCASuite.xml</file>
      <name>org.apache.spark.mllib.feature.PCASuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.108</duration>
      <cases>
        <case>
          <duration>0.108</duration>
          <className>org.apache.spark.mllib.feature.PCASuite</className>
          <testName>Correct computing use a PCA wrapper</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.feature.StandardScalerSuite.xml</file>
      <name>org.apache.spark.mllib.feature.StandardScalerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.49899998</duration>
      <cases>
        <case>
          <duration>0.13</duration>
          <className>org.apache.spark.mllib.feature.StandardScalerSuite</className>
          <testName>Standardization with dense input when means and stds are provided</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.137</duration>
          <className>org.apache.spark.mllib.feature.StandardScalerSuite</className>
          <testName>Standardization with dense input</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.088</duration>
          <className>org.apache.spark.mllib.feature.StandardScalerSuite</className>
          <testName>Standardization with sparse input when means and stds are provided</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.092</duration>
          <className>org.apache.spark.mllib.feature.StandardScalerSuite</className>
          <testName>Standardization with sparse input</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.mllib.feature.StandardScalerSuite</className>
          <testName>Standardization with constant input when means and stds are provided</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.mllib.feature.StandardScalerSuite</className>
          <testName>Standardization with constant input</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.mllib.feature.StandardScalerSuite</className>
          <testName>StandardScalerModel argument nulls are properly handled</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.feature.Word2VecSuite.xml</file>
      <name>org.apache.spark.mllib.feature.Word2VecSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.927</duration>
      <cases>
        <case>
          <duration>0.187</duration>
          <className>org.apache.spark.mllib.feature.Word2VecSuite</className>
          <testName>Word2Vec</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.mllib.feature.Word2VecSuite</className>
          <testName>Word2Vec throws exception when vocabulary is empty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.feature.Word2VecSuite</className>
          <testName>Word2VecModel</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.feature.Word2VecSuite</className>
          <testName>findSynonyms doesn&apos;t reject similar word vectors when called with a vector</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.299</duration>
          <className>org.apache.spark.mllib.feature.Word2VecSuite</className>
          <testName>model load / save</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.408</duration>
          <className>org.apache.spark.mllib.feature.Word2VecSuite</className>
          <testName>big model load / save</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.mllib.feature.Word2VecSuite</className>
          <testName>test similarity for word vectors with large values is not Infinity or NaN</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.fpm.AssociationRulesSuite.xml</file>
      <name>org.apache.spark.mllib.fpm.AssociationRulesSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.06</duration>
      <cases>
        <case>
          <duration>0.06</duration>
          <className>org.apache.spark.mllib.fpm.AssociationRulesSuite</className>
          <testName>association rules using String type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.fpm.FPGrowthSuite.xml</file>
      <name>org.apache.spark.mllib.fpm.FPGrowthSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.339</duration>
      <cases>
        <case>
          <duration>0.25</duration>
          <className>org.apache.spark.mllib.fpm.FPGrowthSuite</className>
          <testName>FP-Growth using String type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.07</duration>
          <className>org.apache.spark.mllib.fpm.FPGrowthSuite</className>
          <testName>FP-Growth String type association rule generation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.218</duration>
          <className>org.apache.spark.mllib.fpm.FPGrowthSuite</className>
          <testName>FP-Growth using Int type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.396</duration>
          <className>org.apache.spark.mllib.fpm.FPGrowthSuite</className>
          <testName>model save/load with String type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.405</duration>
          <className>org.apache.spark.mllib.fpm.FPGrowthSuite</className>
          <testName>model save/load with Int type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.fpm.FPTreeSuite.xml</file>
      <name>org.apache.spark.mllib.fpm.FPTreeSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.002</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.fpm.FPTreeSuite</className>
          <testName>add transaction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.fpm.FPTreeSuite</className>
          <testName>merge tree</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.fpm.FPTreeSuite</className>
          <testName>extract freq itemsets</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.fpm.JavaAssociationRulesSuite.xml</file>
      <name>org.apache.spark.mllib.fpm.JavaAssociationRulesSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.061</duration>
      <cases>
        <case>
          <duration>0.061</duration>
          <className>org.apache.spark.mllib.fpm.JavaAssociationRulesSuite</className>
          <testName>runAssociationRules</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.fpm.JavaFPGrowthSuite.xml</file>
      <name>org.apache.spark.mllib.fpm.JavaFPGrowthSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.555</duration>
      <cases>
        <case>
          <duration>0.455</duration>
          <className>org.apache.spark.mllib.fpm.JavaFPGrowthSuite</className>
          <testName>runFPGrowthSaveLoad</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.1</duration>
          <className>org.apache.spark.mllib.fpm.JavaFPGrowthSuite</className>
          <testName>runFPGrowth</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.fpm.JavaPrefixSpanSuite.xml</file>
      <name>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.684</duration>
      <cases>
        <case>
          <duration>0.148</duration>
          <className>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite</className>
          <testName>runPrefixSpan</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.536</duration>
          <className>org.apache.spark.mllib.fpm.JavaPrefixSpanSuite</className>
          <testName>runPrefixSpanSaveLoad</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.fpm.PrefixSpanSuite.xml</file>
      <name>org.apache.spark.mllib.fpm.PrefixSpanSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.911</duration>
      <cases>
        <case>
          <duration>0.18</duration>
          <className>org.apache.spark.mllib.fpm.PrefixSpanSuite</className>
          <testName>PrefixSpan internal (integer seq, 0 delim) run, singleton itemsets</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.mllib.fpm.PrefixSpanSuite</className>
          <testName>PrefixSpan internal (integer seq, -1 delim) run, variable-size itemsets</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.082</duration>
          <className>org.apache.spark.mllib.fpm.PrefixSpanSuite</className>
          <testName>PrefixSpan projections with multiple partial starts</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.064</duration>
          <className>org.apache.spark.mllib.fpm.PrefixSpanSuite</className>
          <testName>PrefixSpan Integer type, variable-size itemsets</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.067</duration>
          <className>org.apache.spark.mllib.fpm.PrefixSpanSuite</className>
          <testName>PrefixSpan String type, variable-size itemsets</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.476</duration>
          <className>org.apache.spark.mllib.fpm.PrefixSpanSuite</className>
          <testName>model save/load</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.impl.PeriodicGraphCheckpointerSuite.xml</file>
      <name>org.apache.spark.mllib.impl.PeriodicGraphCheckpointerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.631</duration>
      <cases>
        <case>
          <duration>0.039</duration>
          <className>org.apache.spark.mllib.impl.PeriodicGraphCheckpointerSuite</className>
          <testName>Persisting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.592</duration>
          <className>org.apache.spark.mllib.impl.PeriodicGraphCheckpointerSuite</className>
          <testName>Checkpointing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.impl.PeriodicRDDCheckpointerSuite.xml</file>
      <name>org.apache.spark.mllib.impl.PeriodicRDDCheckpointerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.27699998</duration>
      <cases>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.mllib.impl.PeriodicRDDCheckpointerSuite</className>
          <testName>Persisting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.267</duration>
          <className>org.apache.spark.mllib.impl.PeriodicRDDCheckpointerSuite</className>
          <testName>Checkpointing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.linalg.BLASSuite.xml</file>
      <name>org.apache.spark.mllib.linalg.BLASSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.016</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.mllib.linalg.BLASSuite</className>
          <testName>copy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.linalg.BLASSuite</className>
          <testName>scal</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.mllib.linalg.BLASSuite</className>
          <testName>axpy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.mllib.linalg.BLASSuite</className>
          <testName>dot</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.linalg.BLASSuite</className>
          <testName>spr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.mllib.linalg.BLASSuite</className>
          <testName>syr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.mllib.linalg.BLASSuite</className>
          <testName>gemm</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.mllib.linalg.BLASSuite</className>
          <testName>gemv</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.linalg.BreezeMatrixConversionSuite.xml</file>
      <name>org.apache.spark.mllib.linalg.BreezeMatrixConversionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.002</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.linalg.BreezeMatrixConversionSuite</className>
          <testName>dense matrix to breeze</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.linalg.BreezeMatrixConversionSuite</className>
          <testName>dense breeze matrix to matrix</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.linalg.BreezeMatrixConversionSuite</className>
          <testName>sparse matrix to breeze</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.linalg.BreezeMatrixConversionSuite</className>
          <testName>sparse breeze matrix to sparse matrix</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.linalg.BreezeVectorConversionSuite.xml</file>
      <name>org.apache.spark.mllib.linalg.BreezeVectorConversionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.002</duration>
      <cases>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.linalg.BreezeVectorConversionSuite</className>
          <testName>dense to breeze</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.linalg.BreezeVectorConversionSuite</className>
          <testName>sparse to breeze</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.linalg.BreezeVectorConversionSuite</className>
          <testName>dense breeze to vector</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.linalg.BreezeVectorConversionSuite</className>
          <testName>sparse breeze to vector</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.linalg.BreezeVectorConversionSuite</className>
          <testName>sparse breeze with partially-used arrays to vector</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.linalg.JavaMatricesSuite.xml</file>
      <name>org.apache.spark.mllib.linalg.JavaMatricesSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.004</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.linalg.JavaMatricesSuite</className>
          <testName>zerosMatrixConstruction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.linalg.JavaMatricesSuite</className>
          <testName>identityMatrixConstruction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.linalg.JavaMatricesSuite</className>
          <testName>concatenateMatrices</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.linalg.JavaMatricesSuite</className>
          <testName>sparseDenseConversion</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.linalg.JavaMatricesSuite</className>
          <testName>randMatrixConstruction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.mllib.linalg.JavaMatricesSuite</className>
          <testName>diagonalMatrixConstruction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.linalg.JavaVectorsSuite.xml</file>
      <name>org.apache.spark.mllib.linalg.JavaVectorsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.001</duration>
      <cases>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.linalg.JavaVectorsSuite</className>
          <testName>denseArrayConstruction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.linalg.JavaVectorsSuite</className>
          <testName>sparseArrayConstruction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.linalg.MatricesSuite.xml</file>
      <name>org.apache.spark.mllib.linalg.MatricesSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.3209999</duration>
      <cases>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.linalg.MatricesSuite</className>
          <testName>dense matrix construction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.linalg.MatricesSuite</className>
          <testName>dense matrix construction with wrong dimension</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.096</duration>
          <className>org.apache.spark.mllib.linalg.MatricesSuite</className>
          <testName>sparse matrix construction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.linalg.MatricesSuite</className>
          <testName>sparse matrix construction with wrong number of elements</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.mllib.linalg.MatricesSuite</className>
          <testName>index in matrices incorrect input</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.mllib.linalg.MatricesSuite</className>
          <testName>equals</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.linalg.MatricesSuite</className>
          <testName>matrix copies are deep copies</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.linalg.MatricesSuite</className>
          <testName>matrix indexing and updating</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.linalg.MatricesSuite</className>
          <testName>toSparse, toDense</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.linalg.MatricesSuite</className>
          <testName>map, update</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.linalg.MatricesSuite</className>
          <testName>transpose</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.linalg.MatricesSuite</className>
          <testName>foreachActive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.mllib.linalg.MatricesSuite</className>
          <testName>horzcat, vertcat, eye, speye</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.linalg.MatricesSuite</className>
          <testName>zeros</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.mllib.linalg.MatricesSuite</className>
          <testName>ones</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.linalg.MatricesSuite</className>
          <testName>eye</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.181</duration>
          <className>org.apache.spark.mllib.linalg.MatricesSuite</className>
          <testName>rand</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.linalg.MatricesSuite</className>
          <testName>randn</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.linalg.MatricesSuite</className>
          <testName>diag</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.mllib.linalg.MatricesSuite</className>
          <testName>sprand</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.linalg.MatricesSuite</className>
          <testName>sprandn</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.linalg.MatricesSuite</className>
          <testName>MatrixUDT</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.mllib.linalg.MatricesSuite</className>
          <testName>toString</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.linalg.MatricesSuite</className>
          <testName>numNonzeros and numActives</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.linalg.MatricesSuite</className>
          <testName>fromBreeze with sparse matrix</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.mllib.linalg.MatricesSuite</className>
          <testName>row/col iterator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.linalg.MatricesSuite</className>
          <testName>conversions between new local linalg and mllib linalg</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.linalg.MatricesSuite</className>
          <testName>implicit conversions between new local linalg and mllib linalg</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.linalg.VectorsSuite.xml</file>
      <name>org.apache.spark.mllib.linalg.VectorsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.094000004</duration>
      <cases>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.linalg.VectorsSuite</className>
          <testName>dense vector construction with varargs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.linalg.VectorsSuite</className>
          <testName>dense vector construction from a double array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.linalg.VectorsSuite</className>
          <testName>sparse vector construction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.mllib.linalg.VectorsSuite</className>
          <testName>sparse vector construction with unordered elements</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.linalg.VectorsSuite</className>
          <testName>sparse vector construction with mismatched indices/values array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.linalg.VectorsSuite</className>
          <testName>sparse vector construction with too many indices vs size</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.linalg.VectorsSuite</className>
          <testName>dense to array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.linalg.VectorsSuite</className>
          <testName>dense argmax</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.linalg.VectorsSuite</className>
          <testName>sparse to array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.linalg.VectorsSuite</className>
          <testName>sparse argmax</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.mllib.linalg.VectorsSuite</className>
          <testName>vector equals</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.linalg.VectorsSuite</className>
          <testName>vectors equals with explicit 0</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.linalg.VectorsSuite</className>
          <testName>indexing dense vectors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.linalg.VectorsSuite</className>
          <testName>indexing sparse vectors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.mllib.linalg.VectorsSuite</className>
          <testName>parse vectors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.linalg.VectorsSuite</className>
          <testName>zeros</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.linalg.VectorsSuite</className>
          <testName>copy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.linalg.VectorsSuite</className>
          <testName>VectorUDT</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.linalg.VectorsSuite</className>
          <testName>fromBreeze</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.063</duration>
          <className>org.apache.spark.mllib.linalg.VectorsSuite</className>
          <testName>sqdist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.mllib.linalg.VectorsSuite</className>
          <testName>foreachActive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.mllib.linalg.VectorsSuite</className>
          <testName>vector p-norm</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.linalg.VectorsSuite</className>
          <testName>Vector numActive and numNonzeros</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.linalg.VectorsSuite</className>
          <testName>Vector toSparse and toDense</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.linalg.VectorsSuite</className>
          <testName>compressed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.linalg.VectorsSuite</className>
          <testName>slice</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.mllib.linalg.VectorsSuite</className>
          <testName>toJson/fromJson</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.linalg.VectorsSuite</className>
          <testName>conversions between new local linalg and mllib linalg</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.linalg.VectorsSuite</className>
          <testName>implicit conversions between new local linalg and mllib linalg</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.linalg.distributed.BlockMatrixSuite.xml</file>
      <name>org.apache.spark.mllib.linalg.distributed.BlockMatrixSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.843</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.linalg.distributed.BlockMatrixSuite</className>
          <testName>size</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.mllib.linalg.distributed.BlockMatrixSuite</className>
          <testName>grid partitioner</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.mllib.linalg.distributed.BlockMatrixSuite</className>
          <testName>toCoordinateMatrix</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.114</duration>
          <className>org.apache.spark.mllib.linalg.distributed.BlockMatrixSuite</className>
          <testName>toIndexedRowMatrix</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.mllib.linalg.distributed.BlockMatrixSuite</className>
          <testName>toBreeze and toLocalMatrix</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.087</duration>
          <className>org.apache.spark.mllib.linalg.distributed.BlockMatrixSuite</className>
          <testName>add</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.084</duration>
          <className>org.apache.spark.mllib.linalg.distributed.BlockMatrixSuite</className>
          <testName>subtract</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.244</duration>
          <className>org.apache.spark.mllib.linalg.distributed.BlockMatrixSuite</className>
          <testName>multiply</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.mllib.linalg.distributed.BlockMatrixSuite</className>
          <testName>simulate multiply</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.194</duration>
          <className>org.apache.spark.mllib.linalg.distributed.BlockMatrixSuite</className>
          <testName>validate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.mllib.linalg.distributed.BlockMatrixSuite</className>
          <testName>transpose</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.linalg.distributed.CoordinateMatrixSuite.xml</file>
      <name>org.apache.spark.mllib.linalg.distributed.CoordinateMatrixSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.182</duration>
      <cases>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.mllib.linalg.distributed.CoordinateMatrixSuite</className>
          <testName>size</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.mllib.linalg.distributed.CoordinateMatrixSuite</className>
          <testName>empty entries</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.mllib.linalg.distributed.CoordinateMatrixSuite</className>
          <testName>toBreeze</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.mllib.linalg.distributed.CoordinateMatrixSuite</className>
          <testName>transpose</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.033</duration>
          <className>org.apache.spark.mllib.linalg.distributed.CoordinateMatrixSuite</className>
          <testName>toIndexedRowMatrix</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.mllib.linalg.distributed.CoordinateMatrixSuite</className>
          <testName>toRowMatrix</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.053</duration>
          <className>org.apache.spark.mllib.linalg.distributed.CoordinateMatrixSuite</className>
          <testName>toBlockMatrix</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.linalg.distributed.IndexedRowMatrixSuite.xml</file>
      <name>org.apache.spark.mllib.linalg.distributed.IndexedRowMatrixSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.45</duration>
      <cases>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.mllib.linalg.distributed.IndexedRowMatrixSuite</className>
          <testName>size</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.mllib.linalg.distributed.IndexedRowMatrixSuite</className>
          <testName>empty rows</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.mllib.linalg.distributed.IndexedRowMatrixSuite</className>
          <testName>toBreeze</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.mllib.linalg.distributed.IndexedRowMatrixSuite</className>
          <testName>toRowMatrix</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.mllib.linalg.distributed.IndexedRowMatrixSuite</className>
          <testName>toCoordinateMatrix</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.052</duration>
          <className>org.apache.spark.mllib.linalg.distributed.IndexedRowMatrixSuite</className>
          <testName>toBlockMatrix</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.053</duration>
          <className>org.apache.spark.mllib.linalg.distributed.IndexedRowMatrixSuite</className>
          <testName>multiply a local matrix</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.mllib.linalg.distributed.IndexedRowMatrixSuite</className>
          <testName>gram</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.077</duration>
          <className>org.apache.spark.mllib.linalg.distributed.IndexedRowMatrixSuite</className>
          <testName>svd</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.mllib.linalg.distributed.IndexedRowMatrixSuite</className>
          <testName>validate matrix sizes of svd</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.mllib.linalg.distributed.IndexedRowMatrixSuite</className>
          <testName>validate k in svd</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.076</duration>
          <className>org.apache.spark.mllib.linalg.distributed.IndexedRowMatrixSuite</className>
          <testName>similar columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.linalg.distributed.JavaRowMatrixSuite.xml</file>
      <name>org.apache.spark.mllib.linalg.distributed.JavaRowMatrixSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.088</duration>
      <cases>
        <case>
          <duration>0.088</duration>
          <className>org.apache.spark.mllib.linalg.distributed.JavaRowMatrixSuite</className>
          <testName>rowMatrixQRDecomposition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.linalg.distributed.RowMatrixClusterSuite.xml</file>
      <name>org.apache.spark.mllib.linalg.distributed.RowMatrixClusterSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>6.709</duration>
      <cases>
        <case>
          <duration>6.455</duration>
          <className>org.apache.spark.mllib.linalg.distributed.RowMatrixClusterSuite</className>
          <testName>task size should be small in svd</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.254</duration>
          <className>org.apache.spark.mllib.linalg.distributed.RowMatrixClusterSuite</className>
          <testName>task size should be small in summarize</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.linalg.distributed.RowMatrixSuite.xml</file>
      <name>org.apache.spark.mllib.linalg.distributed.RowMatrixSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.8989999</duration>
      <cases>
        <case>
          <duration>0.032</duration>
          <className>org.apache.spark.mllib.linalg.distributed.RowMatrixSuite</className>
          <testName>size</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.mllib.linalg.distributed.RowMatrixSuite</className>
          <testName>empty rows</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.mllib.linalg.distributed.RowMatrixSuite</className>
          <testName>toBreeze</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.mllib.linalg.distributed.RowMatrixSuite</className>
          <testName>gram</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.19</duration>
          <className>org.apache.spark.mllib.linalg.distributed.RowMatrixSuite</className>
          <testName>similar columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.969</duration>
          <className>org.apache.spark.mllib.linalg.distributed.RowMatrixSuite</className>
          <testName>svd of a full-rank matrix</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.063</duration>
          <className>org.apache.spark.mllib.linalg.distributed.RowMatrixSuite</className>
          <testName>svd of a low-rank matrix</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.linalg.distributed.RowMatrixSuite</className>
          <testName>validate k in svd</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.184</duration>
          <className>org.apache.spark.mllib.linalg.distributed.RowMatrixSuite</className>
          <testName>pca</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.mllib.linalg.distributed.RowMatrixSuite</className>
          <testName>multiply a local matrix</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.mllib.linalg.distributed.RowMatrixSuite</className>
          <testName>compute column summary statistics</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.15</duration>
          <className>org.apache.spark.mllib.linalg.distributed.RowMatrixSuite</className>
          <testName>QR Decomposition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.079</duration>
          <className>org.apache.spark.mllib.linalg.distributed.RowMatrixSuite</className>
          <testName>compute covariance</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.04</duration>
          <className>org.apache.spark.mllib.linalg.distributed.RowMatrixSuite</className>
          <testName>covariance matrix is symmetric (SPARK-10875)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.108</duration>
          <className>org.apache.spark.mllib.linalg.distributed.RowMatrixSuite</className>
          <testName>QR decomposition should aware of empty partition (SPARK-16369)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.optimization.GradientDescentClusterSuite.xml</file>
      <name>org.apache.spark.mllib.optimization.GradientDescentClusterSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>4.551</duration>
      <cases>
        <case>
          <duration>4.551</duration>
          <className>org.apache.spark.mllib.optimization.GradientDescentClusterSuite</className>
          <testName>task size should be small</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.optimization.GradientDescentSuite.xml</file>
      <name>org.apache.spark.mllib.optimization.GradientDescentSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.795</duration>
      <cases>
        <case>
          <duration>0.472</duration>
          <className>org.apache.spark.mllib.optimization.GradientDescentSuite</className>
          <testName>Assert the loss is decreasing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.174</duration>
          <className>org.apache.spark.mllib.optimization.GradientDescentSuite</className>
          <testName>Test the loss and gradient of first iteration with regularization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.149</duration>
          <className>org.apache.spark.mllib.optimization.GradientDescentSuite</className>
          <testName>iteration should end with convergence tolerance</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.optimization.LBFGSClusterSuite.xml</file>
      <name>org.apache.spark.mllib.optimization.LBFGSClusterSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>5.275</duration>
      <cases>
        <case>
          <duration>5.275</duration>
          <className>org.apache.spark.mllib.optimization.LBFGSClusterSuite</className>
          <testName>task size should be small</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.optimization.LBFGSSuite.xml</file>
      <name>org.apache.spark.mllib.optimization.LBFGSSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>6.976</duration>
      <cases>
        <case>
          <duration>2.094</duration>
          <className>org.apache.spark.mllib.optimization.LBFGSSuite</className>
          <testName>LBFGS loss should be decreasing and match the result of Gradient Descent</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.993</duration>
          <className>org.apache.spark.mllib.optimization.LBFGSSuite</className>
          <testName>LBFGS and Gradient Descent with L2 regularization should get the same result</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.741</duration>
          <className>org.apache.spark.mllib.optimization.LBFGSSuite</className>
          <testName>The convergence criteria should work as we expect</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.148</duration>
          <className>org.apache.spark.mllib.optimization.LBFGSSuite</className>
          <testName>Optimize via class LBFGS</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.optimization.NNLSSuite.xml</file>
      <name>org.apache.spark.mllib.optimization.NNLSSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.040999997</duration>
      <cases>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.mllib.optimization.NNLSSuite</className>
          <testName>NNLS: exact solution cases</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.optimization.NNLSSuite</className>
          <testName>NNLS: nonnegativity constraint active</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.mllib.optimization.NNLSSuite</className>
          <testName>NNLS: objective value test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.pmml.export.BinaryClassificationPMMLModelExportSuite.xml</file>
      <name>org.apache.spark.mllib.pmml.export.BinaryClassificationPMMLModelExportSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.002</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.pmml.export.BinaryClassificationPMMLModelExportSuite</className>
          <testName>logistic regression PMML export</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.pmml.export.BinaryClassificationPMMLModelExportSuite</className>
          <testName>linear SVM PMML export</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.pmml.export.GeneralizedLinearPMMLModelExportSuite.xml</file>
      <name>org.apache.spark.mllib.pmml.export.GeneralizedLinearPMMLModelExportSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.001</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.pmml.export.GeneralizedLinearPMMLModelExportSuite</className>
          <testName>linear regression PMML export</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.pmml.export.GeneralizedLinearPMMLModelExportSuite</className>
          <testName>ridge regression PMML export</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.pmml.export.GeneralizedLinearPMMLModelExportSuite</className>
          <testName>lasso PMML export</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.pmml.export.KMeansPMMLModelExportSuite.xml</file>
      <name>org.apache.spark.mllib.pmml.export.KMeansPMMLModelExportSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.001</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.pmml.export.KMeansPMMLModelExportSuite</className>
          <testName>KMeansPMMLModelExport generate PMML format</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.pmml.export.PMMLModelExportFactorySuite.xml</file>
      <name>org.apache.spark.mllib.pmml.export.PMMLModelExportFactorySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.039999995</duration>
      <cases>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.mllib.pmml.export.PMMLModelExportFactorySuite</className>
          <testName>PMMLModelExportFactory create KMeansPMMLModelExport when passing a KMeansModel</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.mllib.pmml.export.PMMLModelExportFactorySuite</className>
          <testName>PMMLModelExportFactory create GeneralizedLinearPMMLModelExport when passing a LinearRegressionModel, RidgeRegressionModel or LassoModel</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.mllib.pmml.export.PMMLModelExportFactorySuite</className>
          <testName>PMMLModelExportFactory create BinaryClassificationPMMLModelExport when passing a LogisticRegressionModel or SVMModel</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.pmml.export.PMMLModelExportFactorySuite</className>
          <testName>PMMLModelExportFactory throw IllegalArgumentException when passing a Multinomial Logistic Regression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.pmml.export.PMMLModelExportFactorySuite</className>
          <testName>PMMLModelExportFactory throw IllegalArgumentException when passing an unsupported model</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.random.JavaRandomRDDsSuite.xml</file>
      <name>org.apache.spark.mllib.random.JavaRandomRDDsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.425</duration>
      <cases>
        <case>
          <duration>0.118</duration>
          <className>org.apache.spark.mllib.random.JavaRandomRDDsSuite</className>
          <testName>testNormalVectorRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.1</duration>
          <className>org.apache.spark.mllib.random.JavaRandomRDDsSuite</className>
          <testName>testArbitrary</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.107</duration>
          <className>org.apache.spark.mllib.random.JavaRandomRDDsSuite</className>
          <testName>testLogNormalVectorRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.107</duration>
          <className>org.apache.spark.mllib.random.JavaRandomRDDsSuite</className>
          <testName>testExponentialVectorRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.081</duration>
          <className>org.apache.spark.mllib.random.JavaRandomRDDsSuite</className>
          <testName>testUniformRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.127</duration>
          <className>org.apache.spark.mllib.random.JavaRandomRDDsSuite</className>
          <testName>testRandomVectorRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.089</duration>
          <className>org.apache.spark.mllib.random.JavaRandomRDDsSuite</className>
          <testName>testGammaRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.103</duration>
          <className>org.apache.spark.mllib.random.JavaRandomRDDsSuite</className>
          <testName>testUniformVectorRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.083</duration>
          <className>org.apache.spark.mllib.random.JavaRandomRDDsSuite</className>
          <testName>testPoissonRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.104</duration>
          <className>org.apache.spark.mllib.random.JavaRandomRDDsSuite</className>
          <testName>testNormalRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.103</duration>
          <className>org.apache.spark.mllib.random.JavaRandomRDDsSuite</className>
          <testName>testPoissonVectorRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.107</duration>
          <className>org.apache.spark.mllib.random.JavaRandomRDDsSuite</className>
          <testName>testGammaVectorRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.086</duration>
          <className>org.apache.spark.mllib.random.JavaRandomRDDsSuite</className>
          <testName>testExponentialRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.11</duration>
          <className>org.apache.spark.mllib.random.JavaRandomRDDsSuite</className>
          <testName>testLNormalRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.random.RandomDataGeneratorSuite.xml</file>
      <name>org.apache.spark.mllib.random.RandomDataGeneratorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>5.335</duration>
      <cases>
        <case>
          <duration>0.068</duration>
          <className>org.apache.spark.mllib.random.RandomDataGeneratorSuite</className>
          <testName>UniformGenerator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.085</duration>
          <className>org.apache.spark.mllib.random.RandomDataGeneratorSuite</className>
          <testName>StandardNormalGenerator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.465</duration>
          <className>org.apache.spark.mllib.random.RandomDataGeneratorSuite</className>
          <testName>LogNormalGenerator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.776</duration>
          <className>org.apache.spark.mllib.random.RandomDataGeneratorSuite</className>
          <testName>PoissonGenerator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.485</duration>
          <className>org.apache.spark.mllib.random.RandomDataGeneratorSuite</className>
          <testName>ExponentialGenerator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.606</duration>
          <className>org.apache.spark.mllib.random.RandomDataGeneratorSuite</className>
          <testName>GammaGenerator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.85</duration>
          <className>org.apache.spark.mllib.random.RandomDataGeneratorSuite</className>
          <testName>WeibullGenerator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.random.RandomRDDsSuite.xml</file>
      <name>org.apache.spark.mllib.random.RandomRDDsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>5.1540003</duration>
      <cases>
        <case>
          <duration>0.108</duration>
          <className>org.apache.spark.mllib.random.RandomRDDsSuite</className>
          <testName>RandomRDD sizes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.063</duration>
          <className>org.apache.spark.mllib.random.RandomRDDsSuite</className>
          <testName>randomRDD for different distributions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.983</duration>
          <className>org.apache.spark.mllib.random.RandomRDDsSuite</className>
          <testName>randomVectorRDD for different distributions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.rdd.MLPairRDDFunctionsSuite.xml</file>
      <name>org.apache.spark.mllib.rdd.MLPairRDDFunctionsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.039</duration>
      <cases>
        <case>
          <duration>0.039</duration>
          <className>org.apache.spark.mllib.rdd.MLPairRDDFunctionsSuite</className>
          <testName>topByKey</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.rdd.RDDFunctionsSuite.xml</file>
      <name>org.apache.spark.mllib.rdd.RDDFunctionsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.5049999</duration>
      <cases>
        <case>
          <duration>2.461</duration>
          <className>org.apache.spark.mllib.rdd.RDDFunctionsSuite</className>
          <testName>sliding</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.044</duration>
          <className>org.apache.spark.mllib.rdd.RDDFunctionsSuite</className>
          <testName>sliding with empty partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.recommendation.ALSSuite.xml</file>
      <name>org.apache.spark.mllib.recommendation.ALSSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>14.984001</duration>
      <cases>
        <case>
          <duration>0.887</duration>
          <className>org.apache.spark.mllib.recommendation.ALSSuite</className>
          <testName>rank-1 matrices</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.961</duration>
          <className>org.apache.spark.mllib.recommendation.ALSSuite</className>
          <testName>rank-1 matrices bulk</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.955</duration>
          <className>org.apache.spark.mllib.recommendation.ALSSuite</className>
          <testName>rank-2 matrices</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.189</duration>
          <className>org.apache.spark.mllib.recommendation.ALSSuite</className>
          <testName>rank-2 matrices bulk</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.466</duration>
          <className>org.apache.spark.mllib.recommendation.ALSSuite</className>
          <testName>rank-1 matrices implicit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.487</duration>
          <className>org.apache.spark.mllib.recommendation.ALSSuite</className>
          <testName>rank-1 matrices implicit bulk</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.337</duration>
          <className>org.apache.spark.mllib.recommendation.ALSSuite</className>
          <testName>rank-2 matrices implicit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.588</duration>
          <className>org.apache.spark.mllib.recommendation.ALSSuite</className>
          <testName>rank-2 matrices implicit bulk</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.319</duration>
          <className>org.apache.spark.mllib.recommendation.ALSSuite</className>
          <testName>rank-2 matrices implicit negative</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.969</duration>
          <className>org.apache.spark.mllib.recommendation.ALSSuite</className>
          <testName>rank-2 matrices with different user and product blocks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.659</duration>
          <className>org.apache.spark.mllib.recommendation.ALSSuite</className>
          <testName>pseudorandomness</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.392</duration>
          <className>org.apache.spark.mllib.recommendation.ALSSuite</className>
          <testName>Storage Level for RDDs in model</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.858</duration>
          <className>org.apache.spark.mllib.recommendation.ALSSuite</className>
          <testName>negative ids</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.917</duration>
          <className>org.apache.spark.mllib.recommendation.ALSSuite</className>
          <testName>NNALS, rank 2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.recommendation.JavaALSSuite.xml</file>
      <name>org.apache.spark.mllib.recommendation.JavaALSSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>7.929</duration>
      <cases>
        <case>
          <duration>0.969</duration>
          <className>org.apache.spark.mllib.recommendation.JavaALSSuite</className>
          <testName>runALSUsingStaticMethods</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.602</duration>
          <className>org.apache.spark.mllib.recommendation.JavaALSSuite</className>
          <testName>runImplicitALSUsingConstructor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.359</duration>
          <className>org.apache.spark.mllib.recommendation.JavaALSSuite</className>
          <testName>runRecommend</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.433</duration>
          <className>org.apache.spark.mllib.recommendation.JavaALSSuite</className>
          <testName>runImplicitALSWithNegativeWeight</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.438</duration>
          <className>org.apache.spark.mllib.recommendation.JavaALSSuite</className>
          <testName>runImplicitALSUsingStaticMethods</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.128</duration>
          <className>org.apache.spark.mllib.recommendation.JavaALSSuite</className>
          <testName>runALSUsingConstructor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.recommendation.MatrixFactorizationModelSuite.xml</file>
      <name>org.apache.spark.mllib.recommendation.MatrixFactorizationModelSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.84300005</duration>
      <cases>
        <case>
          <duration>0.094</duration>
          <className>org.apache.spark.mllib.recommendation.MatrixFactorizationModelSuite</className>
          <testName>constructor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.628</duration>
          <className>org.apache.spark.mllib.recommendation.MatrixFactorizationModelSuite</className>
          <testName>save/load</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.067</duration>
          <className>org.apache.spark.mllib.recommendation.MatrixFactorizationModelSuite</className>
          <testName>batch predict API recommendProductsForUsers</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.054</duration>
          <className>org.apache.spark.mllib.recommendation.MatrixFactorizationModelSuite</className>
          <testName>batch predict API recommendUsersForProducts</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.regression.IsotonicRegressionSuite.xml</file>
      <name>org.apache.spark.mllib.regression.IsotonicRegressionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.83800006</duration>
      <cases>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.mllib.regression.IsotonicRegressionSuite</className>
          <testName>increasing isotonic regression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.263</duration>
          <className>org.apache.spark.mllib.regression.IsotonicRegressionSuite</className>
          <testName>model save/load</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.mllib.regression.IsotonicRegressionSuite</className>
          <testName>isotonic regression with size 0</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.mllib.regression.IsotonicRegressionSuite</className>
          <testName>isotonic regression with size 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.mllib.regression.IsotonicRegressionSuite</className>
          <testName>isotonic regression strictly increasing sequence</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.mllib.regression.IsotonicRegressionSuite</className>
          <testName>isotonic regression strictly decreasing sequence</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.mllib.regression.IsotonicRegressionSuite</className>
          <testName>isotonic regression with last element violating monotonicity</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.mllib.regression.IsotonicRegressionSuite</className>
          <testName>isotonic regression with first element violating monotonicity</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.mllib.regression.IsotonicRegressionSuite</className>
          <testName>isotonic regression with negative labels</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.mllib.regression.IsotonicRegressionSuite</className>
          <testName>isotonic regression with unordered input</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.mllib.regression.IsotonicRegressionSuite</className>
          <testName>weighted isotonic regression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.mllib.regression.IsotonicRegressionSuite</className>
          <testName>weighted isotonic regression with weights lower than 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.mllib.regression.IsotonicRegressionSuite</className>
          <testName>weighted isotonic regression with negative weights</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.mllib.regression.IsotonicRegressionSuite</className>
          <testName>weighted isotonic regression with zero weights</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.mllib.regression.IsotonicRegressionSuite</className>
          <testName>SPARK-16426 isotonic regression with duplicate features that produce NaNs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.mllib.regression.IsotonicRegressionSuite</className>
          <testName>isotonic regression prediction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.mllib.regression.IsotonicRegressionSuite</className>
          <testName>isotonic regression prediction with duplicate features</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.mllib.regression.IsotonicRegressionSuite</className>
          <testName>antitonic regression prediction with duplicate features</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.038</duration>
          <className>org.apache.spark.mllib.regression.IsotonicRegressionSuite</className>
          <testName>isotonic regression RDD prediction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.mllib.regression.IsotonicRegressionSuite</className>
          <testName>antitonic regression prediction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.mllib.regression.IsotonicRegressionSuite</className>
          <testName>model construction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite.xml</file>
      <name>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.18900001</duration>
      <cases>
        <case>
          <duration>0.085</duration>
          <className>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite</className>
          <testName>testIsotonicRegressionJavaRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.104</duration>
          <className>org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite</className>
          <testName>testIsotonicRegressionPredictionsJavaRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.regression.JavaLassoSuite.xml</file>
      <name>org.apache.spark.mllib.regression.JavaLassoSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.115</duration>
      <cases>
        <case>
          <duration>0.76</duration>
          <className>org.apache.spark.mllib.regression.JavaLassoSuite</className>
          <testName>runLassoUsingConstructor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.355</duration>
          <className>org.apache.spark.mllib.regression.JavaLassoSuite</className>
          <testName>runLassoUsingStaticMethods</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.regression.JavaLinearRegressionSuite.xml</file>
      <name>org.apache.spark.mllib.regression.JavaLinearRegressionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.81</duration>
      <cases>
        <case>
          <duration>0.637</duration>
          <className>org.apache.spark.mllib.regression.JavaLinearRegressionSuite</className>
          <testName>testPredictJavaRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.58</duration>
          <className>org.apache.spark.mllib.regression.JavaLinearRegressionSuite</className>
          <testName>runLinearRegressionUsingStaticMethods</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.593</duration>
          <className>org.apache.spark.mllib.regression.JavaLinearRegressionSuite</className>
          <testName>runLinearRegressionUsingConstructor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.regression.JavaRidgeRegressionSuite.xml</file>
      <name>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>5.103</duration>
      <cases>
        <case>
          <duration>2.463</duration>
          <className>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite</className>
          <testName>runRidgeRegressionUsingConstructor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.64</duration>
          <className>org.apache.spark.mllib.regression.JavaRidgeRegressionSuite</className>
          <testName>runRidgeRegressionUsingStaticMethods</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite.xml</file>
      <name>org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.332</duration>
      <cases>
        <case>
          <duration>0.332</duration>
          <className>org.apache.spark.mllib.regression.JavaStreamingLinearRegressionSuite</className>
          <testName>javaAPI</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.regression.LabeledPointSuite.xml</file>
      <name>org.apache.spark.mllib.regression.LabeledPointSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.004</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.regression.LabeledPointSuite</className>
          <testName>parse labeled points</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.regression.LabeledPointSuite</className>
          <testName>parse labeled points with whitespaces</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.regression.LabeledPointSuite</className>
          <testName>9 format</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.regression.LabeledPointSuite</className>
          <testName>conversions between new ml LabeledPoint and mllib LabeledPoint</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.regression.LassoClusterSuite.xml</file>
      <name>org.apache.spark.mllib.regression.LassoClusterSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>4.891</duration>
      <cases>
        <case>
          <duration>4.891</duration>
          <className>org.apache.spark.mllib.regression.LassoClusterSuite</className>
          <testName>task size should be small in both training and prediction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.regression.LassoSuite.xml</file>
      <name>org.apache.spark.mllib.regression.LassoSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.18</duration>
      <cases>
        <case>
          <duration>0.478</duration>
          <className>org.apache.spark.mllib.regression.LassoSuite</className>
          <testName>Lasso local random SGD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.47</duration>
          <className>org.apache.spark.mllib.regression.LassoSuite</className>
          <testName>Lasso local random SGD with initial weights</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.232</duration>
          <className>org.apache.spark.mllib.regression.LassoSuite</className>
          <testName>model save/load</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.regression.LinearRegressionClusterSuite.xml</file>
      <name>org.apache.spark.mllib.regression.LinearRegressionClusterSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>4.722</duration>
      <cases>
        <case>
          <duration>4.722</duration>
          <className>org.apache.spark.mllib.regression.LinearRegressionClusterSuite</className>
          <testName>task size should be small in both training and prediction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.regression.LinearRegressionSuite.xml</file>
      <name>org.apache.spark.mllib.regression.LinearRegressionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.0080001</duration>
      <cases>
        <case>
          <duration>0.58</duration>
          <className>org.apache.spark.mllib.regression.LinearRegressionSuite</className>
          <testName>linear regression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.554</duration>
          <className>org.apache.spark.mllib.regression.LinearRegressionSuite</className>
          <testName>linear regression without intercept</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.608</duration>
          <className>org.apache.spark.mllib.regression.LinearRegressionSuite</className>
          <testName>sparse linear regression without intercept</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.266</duration>
          <className>org.apache.spark.mllib.regression.LinearRegressionSuite</className>
          <testName>model save/load</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.regression.RidgeRegressionClusterSuite.xml</file>
      <name>org.apache.spark.mllib.regression.RidgeRegressionClusterSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>4.529</duration>
      <cases>
        <case>
          <duration>4.529</duration>
          <className>org.apache.spark.mllib.regression.RidgeRegressionClusterSuite</className>
          <testName>task size should be small in both training and prediction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.regression.RidgeRegressionSuite.xml</file>
      <name>org.apache.spark.mllib.regression.RidgeRegressionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.681</duration>
      <cases>
        <case>
          <duration>2.402</duration>
          <className>org.apache.spark.mllib.regression.RidgeRegressionSuite</className>
          <testName>ridge regression can help avoid overfitting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.279</duration>
          <className>org.apache.spark.mllib.regression.RidgeRegressionSuite</className>
          <testName>model save/load</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.regression.StreamingLinearRegressionSuite.xml</file>
      <name>org.apache.spark.mllib.regression.StreamingLinearRegressionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>8.165</duration>
      <cases>
        <case>
          <duration>3.361</duration>
          <className>org.apache.spark.mllib.regression.StreamingLinearRegressionSuite</className>
          <testName>parameter accuracy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.098</duration>
          <className>org.apache.spark.mllib.regression.StreamingLinearRegressionSuite</className>
          <testName>parameter convergence</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.332</duration>
          <className>org.apache.spark.mllib.regression.StreamingLinearRegressionSuite</className>
          <testName>predictions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.944</duration>
          <className>org.apache.spark.mllib.regression.StreamingLinearRegressionSuite</className>
          <testName>training and prediction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.43</duration>
          <className>org.apache.spark.mllib.regression.StreamingLinearRegressionSuite</className>
          <testName>handling empty RDDs in a stream</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.stat.CorrelationSuite.xml</file>
      <name>org.apache.spark.mllib.stat.CorrelationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.784</duration>
      <cases>
        <case>
          <duration>0.141</duration>
          <className>org.apache.spark.mllib.stat.CorrelationSuite</className>
          <testName>corr(x, y) pearson, 1 value in data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.184</duration>
          <className>org.apache.spark.mllib.stat.CorrelationSuite</className>
          <testName>corr(x, y) default, pearson</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.353</duration>
          <className>org.apache.spark.mllib.stat.CorrelationSuite</className>
          <testName>corr(x, y) spearman</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.mllib.stat.CorrelationSuite</className>
          <testName>corr(X) default, pearson</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.064</duration>
          <className>org.apache.spark.mllib.stat.CorrelationSuite</className>
          <testName>corr(X) spearman</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.stat.CorrelationSuite</className>
          <testName>method identification</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.mllib.stat.CorrelationSuite</className>
          <testName>Pearson correlation of very large uncorrelated values (SPARK-14533)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.stat.HypothesisTestSuite.xml</file>
      <name>org.apache.spark.mllib.stat.HypothesisTestSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>4.178</duration>
      <cases>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.mllib.stat.HypothesisTestSuite</className>
          <testName>chi squared pearson goodness of fit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.mllib.stat.HypothesisTestSuite</className>
          <testName>chi squared pearson matrix independence</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.339</duration>
          <className>org.apache.spark.mllib.stat.HypothesisTestSuite</className>
          <testName>chi squared pearson RDD[LabeledPoint]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.777</duration>
          <className>org.apache.spark.mllib.stat.HypothesisTestSuite</className>
          <testName>1 sample Kolmogorov-Smirnov test: apache commons math3 implementation equivalence</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.056</duration>
          <className>org.apache.spark.mllib.stat.HypothesisTestSuite</className>
          <testName>1 sample Kolmogorov-Smirnov test: R implementation equivalence</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.stat.JavaStatisticsSuite.xml</file>
      <name>org.apache.spark.mllib.stat.JavaStatisticsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.543</duration>
      <cases>
        <case>
          <duration>0.107</duration>
          <className>org.apache.spark.mllib.stat.JavaStatisticsSuite</className>
          <testName>testCorr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.089</duration>
          <className>org.apache.spark.mllib.stat.JavaStatisticsSuite</className>
          <testName>chiSqTest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.225</duration>
          <className>org.apache.spark.mllib.stat.JavaStatisticsSuite</className>
          <testName>streamingTest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.122</duration>
          <className>org.apache.spark.mllib.stat.JavaStatisticsSuite</className>
          <testName>kolmogorovSmirnovTest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.stat.KernelDensitySuite.xml</file>
      <name>org.apache.spark.mllib.stat.KernelDensitySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.026</duration>
      <cases>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.mllib.stat.KernelDensitySuite</className>
          <testName>kernel density single sample</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.mllib.stat.KernelDensitySuite</className>
          <testName>kernel density multiple samples</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.stat.MultivariateOnlineSummarizerSuite.xml</file>
      <name>org.apache.spark.mllib.stat.MultivariateOnlineSummarizerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.010000001</duration>
      <cases>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.mllib.stat.MultivariateOnlineSummarizerSuite</className>
          <testName>basic error handing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.stat.MultivariateOnlineSummarizerSuite</className>
          <testName>dense vector input</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.stat.MultivariateOnlineSummarizerSuite</className>
          <testName>sparse vector input</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.stat.MultivariateOnlineSummarizerSuite</className>
          <testName>mixing dense and sparse vector input</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.stat.MultivariateOnlineSummarizerSuite</className>
          <testName>merging two summarizers</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.stat.MultivariateOnlineSummarizerSuite</className>
          <testName>merging summarizer with empty summarizer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.stat.MultivariateOnlineSummarizerSuite</className>
          <testName>merging summarizer when one side has zero mean (SPARK-4355)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.stat.MultivariateOnlineSummarizerSuite</className>
          <testName>merging summarizer with weighted samples</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.mllib.stat.MultivariateOnlineSummarizerSuite</className>
          <testName>test min/max with weighted samples (SPARK-16561)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.stat.StreamingTestSuite.xml</file>
      <name>org.apache.spark.mllib.stat.StreamingTestSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.143</duration>
      <cases>
        <case>
          <duration>0.358</duration>
          <className>org.apache.spark.mllib.stat.StreamingTestSuite</className>
          <testName>accuracy for null hypothesis using welch t-test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.272</duration>
          <className>org.apache.spark.mllib.stat.StreamingTestSuite</className>
          <testName>accuracy for alternative hypothesis using welch t-test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.28</duration>
          <className>org.apache.spark.mllib.stat.StreamingTestSuite</className>
          <testName>accuracy for null hypothesis using student t-test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.321</duration>
          <className>org.apache.spark.mllib.stat.StreamingTestSuite</className>
          <testName>accuracy for alternative hypothesis using student t-test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.336</duration>
          <className>org.apache.spark.mllib.stat.StreamingTestSuite</className>
          <testName>batches within same test window are grouped</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.276</duration>
          <className>org.apache.spark.mllib.stat.StreamingTestSuite</className>
          <testName>entries in peace period are dropped</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.3</duration>
          <className>org.apache.spark.mllib.stat.StreamingTestSuite</className>
          <testName>null hypothesis when only data from one group is present</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.stat.distribution.MultivariateGaussianSuite.xml</file>
      <name>org.apache.spark.mllib.stat.distribution.MultivariateGaussianSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.001</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.stat.distribution.MultivariateGaussianSuite</className>
          <testName>univariate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.stat.distribution.MultivariateGaussianSuite</className>
          <testName>multivariate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.stat.distribution.MultivariateGaussianSuite</className>
          <testName>multivariate degenerate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.stat.distribution.MultivariateGaussianSuite</className>
          <testName>SPARK-11302</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.tree.DecisionTreeSuite.xml</file>
      <name>org.apache.spark.mllib.tree.DecisionTreeSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.917</duration>
      <cases>
        <case>
          <duration>0.077</duration>
          <className>org.apache.spark.mllib.tree.DecisionTreeSuite</className>
          <testName>Binary classification stump with ordered categorical features</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.094</duration>
          <className>org.apache.spark.mllib.tree.DecisionTreeSuite</className>
          <testName>Regression stump with 3-ary (ordered) categorical features</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.089</duration>
          <className>org.apache.spark.mllib.tree.DecisionTreeSuite</className>
          <testName>Regression stump with binary (ordered) categorical features</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.108</duration>
          <className>org.apache.spark.mllib.tree.DecisionTreeSuite</className>
          <testName>Binary classification stump with fixed label 0 for Gini</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.117</duration>
          <className>org.apache.spark.mllib.tree.DecisionTreeSuite</className>
          <testName>Binary classification stump with fixed label 1 for Gini</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.106</duration>
          <className>org.apache.spark.mllib.tree.DecisionTreeSuite</className>
          <testName>Binary classification stump with fixed label 0 for Entropy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.113</duration>
          <className>org.apache.spark.mllib.tree.DecisionTreeSuite</className>
          <testName>Binary classification stump with fixed label 1 for Entropy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.122</duration>
          <className>org.apache.spark.mllib.tree.DecisionTreeSuite</className>
          <testName>Multiclass classification stump with 3-ary (unordered) categorical features</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.058</duration>
          <className>org.apache.spark.mllib.tree.DecisionTreeSuite</className>
          <testName>Binary classification stump with 1 continuous feature, to check off-by-1 error</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.059</duration>
          <className>org.apache.spark.mllib.tree.DecisionTreeSuite</className>
          <testName>Binary classification stump with 2 continuous features</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.14</duration>
          <className>org.apache.spark.mllib.tree.DecisionTreeSuite</className>
          <testName>Multiclass classification stump with unordered categorical features, with just enough bins</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.163</duration>
          <className>org.apache.spark.mllib.tree.DecisionTreeSuite</className>
          <testName>Multiclass classification stump with continuous features</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.215</duration>
          <className>org.apache.spark.mllib.tree.DecisionTreeSuite</className>
          <testName>Multiclass classification stump with continuous + unordered categorical features</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.131</duration>
          <className>org.apache.spark.mllib.tree.DecisionTreeSuite</className>
          <testName>Multiclass classification stump with 10-ary (ordered) categorical features</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.136</duration>
          <className>org.apache.spark.mllib.tree.DecisionTreeSuite</className>
          <testName>Multiclass classification tree with 10-ary (ordered) categorical features, with just enough bins</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.071</duration>
          <className>org.apache.spark.mllib.tree.DecisionTreeSuite</className>
          <testName>split must satisfy min instances per node requirements</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.07</duration>
          <className>org.apache.spark.mllib.tree.DecisionTreeSuite</className>
          <testName>do not choose split that does not satisfy min instance per node requirements</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.094</duration>
          <className>org.apache.spark.mllib.tree.DecisionTreeSuite</className>
          <testName>split must satisfy min info gain requirements</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.tree.DecisionTreeSuite</className>
          <testName>subtreeIterator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.953</duration>
          <className>org.apache.spark.mllib.tree.DecisionTreeSuite</className>
          <testName>model save/load</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.tree.GradientBoostedTreesSuite.xml</file>
      <name>org.apache.spark.mllib.tree.GradientBoostedTreesSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>15.157001</duration>
      <cases>
        <case>
          <duration>4.446</duration>
          <className>org.apache.spark.mllib.tree.GradientBoostedTreesSuite</className>
          <testName>Regression with continuous features: SquaredError</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.592</duration>
          <className>org.apache.spark.mllib.tree.GradientBoostedTreesSuite</className>
          <testName>Regression with continuous features: Absolute Error</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.566</duration>
          <className>org.apache.spark.mllib.tree.GradientBoostedTreesSuite</className>
          <testName>Binary classification with continuous features: Log Loss</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.tree.GradientBoostedTreesSuite</className>
          <testName>defaultParams should recognize Classification</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.964</duration>
          <className>org.apache.spark.mllib.tree.GradientBoostedTreesSuite</className>
          <testName>model save/load</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.588</duration>
          <className>org.apache.spark.mllib.tree.GradientBoostedTreesSuite</className>
          <testName>Checkpointing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.tree.ImpuritySuite.xml</file>
      <name>org.apache.spark.mllib.tree.ImpuritySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.003</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.tree.ImpuritySuite</className>
          <testName>Gini impurity does not support negative labels</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.mllib.tree.ImpuritySuite</className>
          <testName>Entropy does not support negative labels</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.tree.JavaDecisionTreeSuite.xml</file>
      <name>org.apache.spark.mllib.tree.JavaDecisionTreeSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.31599998</duration>
      <cases>
        <case>
          <duration>0.152</duration>
          <className>org.apache.spark.mllib.tree.JavaDecisionTreeSuite</className>
          <testName>runDTUsingStaticMethods</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.164</duration>
          <className>org.apache.spark.mllib.tree.JavaDecisionTreeSuite</className>
          <testName>runDTUsingConstructor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.tree.RandomForestSuite.xml</file>
      <name>org.apache.spark.mllib.tree.RandomForestSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>4.053</duration>
      <cases>
        <case>
          <duration>1.038</duration>
          <className>org.apache.spark.mllib.tree.RandomForestSuite</className>
          <testName> RandomForest(numTrees = 1)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.484</duration>
          <className>org.apache.spark.mllib.tree.RandomForestSuite</className>
          <testName> RandomForest(numTrees = 1)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.42</duration>
          <className>org.apache.spark.mllib.tree.RandomForestSuite</className>
          <testName> RandomForest(numTrees = 1)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.413</duration>
          <className>org.apache.spark.mllib.tree.RandomForestSuite</className>
          <testName> RandomForest(numTrees = 1)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.112</duration>
          <className>org.apache.spark.mllib.tree.RandomForestSuite</className>
          <testName>alternating categorical and continuous features with multiclass labels to test indexing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.266</duration>
          <className>org.apache.spark.mllib.tree.RandomForestSuite</className>
          <testName>subsampling rate in RandomForest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.32</duration>
          <className>org.apache.spark.mllib.tree.RandomForestSuite</className>
          <testName>model save/load</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.util.JavaMLUtilsSuite.xml</file>
      <name>org.apache.spark.mllib.util.JavaMLUtilsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.2</duration>
      <cases>
        <case>
          <duration>0.096</duration>
          <className>org.apache.spark.mllib.util.JavaMLUtilsSuite</className>
          <testName>testConvertMatrixColumnsToAndFromML</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.104</duration>
          <className>org.apache.spark.mllib.util.JavaMLUtilsSuite</className>
          <testName>testConvertVectorColumnsToAndFromML</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.util.MLUtilsSuite.xml</file>
      <name>org.apache.spark.mllib.util.MLUtilsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>8.158</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.util.MLUtilsSuite</className>
          <testName>epsilon computation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.mllib.util.MLUtilsSuite</className>
          <testName>fast squared distance</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.121</duration>
          <className>org.apache.spark.mllib.util.MLUtilsSuite</className>
          <testName>loadLibSVMFile</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.mllib.util.MLUtilsSuite</className>
          <testName>loadLibSVMFile throws IllegalArgumentException when indices is zero-based</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.055</duration>
          <className>org.apache.spark.mllib.util.MLUtilsSuite</className>
          <testName>loadLibSVMFile throws IllegalArgumentException when indices is not in ascending order</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.054</duration>
          <className>org.apache.spark.mllib.util.MLUtilsSuite</className>
          <testName>saveAsLibSVMFile</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.util.MLUtilsSuite</className>
          <testName>appendBias</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>7.037</duration>
          <className>org.apache.spark.mllib.util.MLUtilsSuite</className>
          <testName>kFold</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.085</duration>
          <className>org.apache.spark.mllib.util.MLUtilsSuite</className>
          <testName>loadVectors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.102</duration>
          <className>org.apache.spark.mllib.util.MLUtilsSuite</className>
          <testName>loadLabeledPoints</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.util.MLUtilsSuite</className>
          <testName>log1pExp</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.154</duration>
          <className>org.apache.spark.mllib.util.MLUtilsSuite</className>
          <testName>convertVectorColumnsToML</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.173</duration>
          <className>org.apache.spark.mllib.util.MLUtilsSuite</className>
          <testName>convertVectorColumnsFromML</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.183</duration>
          <className>org.apache.spark.mllib.util.MLUtilsSuite</className>
          <testName>convertMatrixColumnsToML</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.144</duration>
          <className>org.apache.spark.mllib.util.MLUtilsSuite</className>
          <testName>convertMatrixColumnsFromML</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.util.NumericParserSuite.xml</file>
      <name>org.apache.spark.mllib.util.NumericParserSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.001</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.mllib.util.NumericParserSuite</className>
          <testName>parser</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.mllib.util.NumericParserSuite</className>
          <testName>parser with whitespaces</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/mllib/target/test-reports/org.apache.spark.mllib.util.TestingUtilsSuite.xml</file>
      <name>org.apache.spark.mllib.util.TestingUtilsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.34399998</duration>
      <cases>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.mllib.util.TestingUtilsSuite</className>
          <testName>Comparing doubles using relative error</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.mllib.util.TestingUtilsSuite</className>
          <testName>Comparing doubles using absolute error</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.mllib.util.TestingUtilsSuite</className>
          <testName>Comparing vectors using relative error</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.mllib.util.TestingUtilsSuite</className>
          <testName>Comparing vectors using absolute error</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.32</duration>
          <className>org.apache.spark.mllib.util.TestingUtilsSuite</className>
          <testName>Comparing Matrices using absolute error</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.mllib.util.TestingUtilsSuite</className>
          <testName>Comparing Matrices using relative error</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/repl/target/test-reports/org.apache.spark.repl.ExecutorClassLoaderSuite.xml</file>
      <name>org.apache.spark.repl.ExecutorClassLoaderSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.33</duration>
      <cases>
        <case>
          <duration>0.86</duration>
          <className>org.apache.spark.repl.ExecutorClassLoaderSuite</className>
          <testName>child first</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.057</duration>
          <className>org.apache.spark.repl.ExecutorClassLoaderSuite</className>
          <testName>parent first</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.repl.ExecutorClassLoaderSuite</className>
          <testName>child first can fall back</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.052</duration>
          <className>org.apache.spark.repl.ExecutorClassLoaderSuite</className>
          <testName>child first can fail</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.repl.ExecutorClassLoaderSuite</className>
          <testName>resource from parent</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.repl.ExecutorClassLoaderSuite</className>
          <testName>resources from parent</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.229</duration>
          <className>org.apache.spark.repl.ExecutorClassLoaderSuite</className>
          <testName>fetch classes using Spark&apos;s RpcEnv</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/repl/target/test-reports/org.apache.spark.repl.ReplSuite.xml</file>
      <name>org.apache.spark.repl.ReplSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>102.87399</duration>
      <cases>
        <case>
          <duration>6.573</duration>
          <className>org.apache.spark.repl.ReplSuite</className>
          <testName>propagation of local properties</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>8.457</duration>
          <className>org.apache.spark.repl.ReplSuite</className>
          <testName>SPARK-15236: use Hive catalog</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.008</duration>
          <className>org.apache.spark.repl.ReplSuite</className>
          <testName>SPARK-15236: use in-memory catalog</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>5.059</duration>
          <className>org.apache.spark.repl.ReplSuite</className>
          <testName>simple foreach with accumulator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.3</duration>
          <className>org.apache.spark.repl.ReplSuite</className>
          <testName>external vars</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.616</duration>
          <className>org.apache.spark.repl.ReplSuite</className>
          <testName>external classes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.953</duration>
          <className>org.apache.spark.repl.ReplSuite</className>
          <testName>external functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.491</duration>
          <className>org.apache.spark.repl.ReplSuite</className>
          <testName>external functions that access vars</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.338</duration>
          <className>org.apache.spark.repl.ReplSuite</className>
          <testName>broadcast vars</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.031</duration>
          <className>org.apache.spark.repl.ReplSuite</className>
          <testName>interacting with files</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>6.744</duration>
          <className>org.apache.spark.repl.ReplSuite</className>
          <testName>local-cluster mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.241</duration>
          <className>org.apache.spark.repl.ReplSuite</className>
          <testName>SPARK-1199 two instances of same class don&apos;t type check</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.474</duration>
          <className>org.apache.spark.repl.ReplSuite</className>
          <testName>SPARK-2452 compound statements</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>7.439</duration>
          <className>org.apache.spark.repl.ReplSuite</className>
          <testName>SPARK-2576 importing implicits</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.658</duration>
          <className>org.apache.spark.repl.ReplSuite</className>
          <testName>Datasets and encoders</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.497</duration>
          <className>org.apache.spark.repl.ReplSuite</className>
          <testName>SPARK-2632 importing a method from non serializable class and not using it</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.636</duration>
          <className>org.apache.spark.repl.ReplSuite</className>
          <testName>collecting objects of class defined in repl</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>6.237</duration>
          <className>org.apache.spark.repl.ReplSuite</className>
          <testName>collecting objects of class defined in repl - shuffling</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>5.881</duration>
          <className>org.apache.spark.repl.ReplSuite</className>
          <testName>replicating blocks of object with class defined in repl</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.222</duration>
          <className>org.apache.spark.repl.ReplSuite</className>
          <testName>line wrapper only initialized once when used as encoder outer scope</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.961</duration>
          <className>org.apache.spark.repl.ReplSuite</className>
          <testName>define case class and create Dataset together with paste mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>5.058</duration>
          <className>org.apache.spark.repl.ReplSuite</className>
          <testName>should clone and clean line object in ClosureCleaner</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.RandomDataGeneratorSuite.xml</file>
      <name>org.apache.spark.sql.RandomDataGeneratorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>6.954997</duration>
      <cases>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StringType (nullable=true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StringType (nullable=false)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>LongType (nullable=true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>LongType (nullable=false)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>IntegerType (nullable=true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>IntegerType (nullable=false)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>TimestampType (nullable=true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>TimestampType (nullable=false)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>DoubleType (nullable=true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>DoubleType (nullable=false)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>DateType (nullable=true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>DateType (nullable=false)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>BinaryType (nullable=true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>BinaryType (nullable=false)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>BooleanType (nullable=true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>BooleanType (nullable=false)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>ByteType (nullable=true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>ByteType (nullable=false)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>FloatType (nullable=true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>FloatType (nullable=false)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>ShortType (nullable=true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>ShortType (nullable=false)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>ArrayType(FloatType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>ArrayType(LongType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>ArrayType(IntegerType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>ArrayType(TimestampType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>ArrayType(ByteType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>ArrayType(ShortType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>ArrayType(DecimalType(20,5),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>ArrayType(DateType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>ArrayType(BooleanType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>ArrayType(BinaryType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>ArrayType(DecimalType(10,0),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.12</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>ArrayType(StringType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>ArrayType(DoubleType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>ArrayType(DecimalType(38,18),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.098</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(StringType,StringType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.033</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(StringType,LongType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(StringType,IntegerType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.059</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(StringType,DecimalType(20,5),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.097</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(StringType,TimestampType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(StringType,DoubleType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.066</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(StringType,DateType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.086</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(StringType,DecimalType(10,0),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.064</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(StringType,BinaryType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.039</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(StringType,BooleanType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.088</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(StringType,DecimalType(38,18),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(StringType,ByteType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.052</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(StringType,FloatType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.08</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(StringType,ShortType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.055</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(LongType,StringType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(LongType,LongType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(LongType,IntegerType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(LongType,DecimalType(20,5),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(LongType,TimestampType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(LongType,DoubleType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(LongType,DateType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(LongType,DecimalType(10,0),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(LongType,BinaryType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(LongType,BooleanType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(LongType,DecimalType(38,18),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(LongType,ByteType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(LongType,FloatType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(LongType,ShortType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.129</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(IntegerType,StringType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(IntegerType,LongType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(IntegerType,IntegerType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(IntegerType,DecimalType(20,5),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(IntegerType,TimestampType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(IntegerType,DoubleType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(IntegerType,DateType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(IntegerType,DecimalType(10,0),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(IntegerType,BinaryType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(IntegerType,BooleanType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(IntegerType,DecimalType(38,18),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(IntegerType,ByteType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(IntegerType,FloatType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(IntegerType,ShortType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.032</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(TimestampType,StringType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(TimestampType,LongType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(TimestampType,IntegerType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(TimestampType,DecimalType(20,5),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(TimestampType,TimestampType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(TimestampType,DoubleType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(TimestampType,DateType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(TimestampType,DecimalType(10,0),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(TimestampType,BinaryType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(TimestampType,BooleanType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(TimestampType,DecimalType(38,18),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(TimestampType,ByteType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(TimestampType,FloatType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(TimestampType,ShortType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.045</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(DoubleType,StringType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(DoubleType,LongType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(DoubleType,IntegerType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(DoubleType,DecimalType(20,5),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(DoubleType,TimestampType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(DoubleType,DoubleType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(DoubleType,DateType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(DoubleType,DecimalType(10,0),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(DoubleType,BinaryType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(DoubleType,BooleanType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(DoubleType,DecimalType(38,18),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(DoubleType,ByteType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(DoubleType,FloatType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(DoubleType,ShortType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.059</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(DateType,StringType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(DateType,LongType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(DateType,IntegerType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(DateType,DecimalType(20,5),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(DateType,TimestampType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(DateType,DoubleType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(DateType,DateType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(DateType,DecimalType(10,0),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(DateType,BinaryType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(DateType,BooleanType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(DateType,DecimalType(38,18),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(DateType,ByteType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(DateType,FloatType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(DateType,ShortType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(BinaryType,StringType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(BinaryType,LongType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(BinaryType,IntegerType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(BinaryType,DecimalType(20,5),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(BinaryType,TimestampType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(BinaryType,DoubleType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(BinaryType,DateType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(BinaryType,DecimalType(10,0),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(BinaryType,BinaryType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(BinaryType,BooleanType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(BinaryType,DecimalType(38,18),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(BinaryType,ByteType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(BinaryType,FloatType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(BinaryType,ShortType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(BooleanType,StringType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(BooleanType,LongType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(BooleanType,IntegerType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(BooleanType,DecimalType(20,5),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(BooleanType,TimestampType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(BooleanType,DoubleType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(BooleanType,DateType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(BooleanType,DecimalType(10,0),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(BooleanType,BinaryType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(BooleanType,BooleanType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(BooleanType,DecimalType(38,18),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(BooleanType,ByteType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(BooleanType,FloatType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(BooleanType,ShortType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(ByteType,StringType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(ByteType,LongType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(ByteType,IntegerType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(ByteType,DecimalType(20,5),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(ByteType,TimestampType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(ByteType,DoubleType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(ByteType,DateType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(ByteType,DecimalType(10,0),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(ByteType,BinaryType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(ByteType,BooleanType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(ByteType,DecimalType(38,18),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(ByteType,ByteType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(ByteType,FloatType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(ByteType,ShortType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.087</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(FloatType,StringType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(FloatType,LongType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(FloatType,IntegerType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(FloatType,DecimalType(20,5),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(FloatType,TimestampType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(FloatType,DoubleType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(FloatType,DateType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(FloatType,DecimalType(10,0),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(FloatType,BinaryType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(FloatType,BooleanType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(FloatType,DecimalType(38,18),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(FloatType,ByteType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(FloatType,FloatType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(FloatType,ShortType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.082</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(ShortType,StringType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(ShortType,LongType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(ShortType,IntegerType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(ShortType,DecimalType(20,5),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(ShortType,TimestampType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(ShortType,DoubleType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(ShortType,DateType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(ShortType,DecimalType(10,0),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(ShortType,BinaryType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(ShortType,BooleanType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(ShortType,DecimalType(38,18),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(ShortType,ByteType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(ShortType,FloatType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>MapType(ShortType,ShortType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,StringType,true), StructField(b,StringType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,StringType,true), StructField(b,LongType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,StringType,true), StructField(b,IntegerType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,StringType,true), StructField(b,DecimalType(20,5),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,StringType,true), StructField(b,TimestampType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,StringType,true), StructField(b,DoubleType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,StringType,true), StructField(b,DateType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,StringType,true), StructField(b,DecimalType(10,0),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,StringType,true), StructField(b,BinaryType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,StringType,true), StructField(b,BooleanType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,StringType,true), StructField(b,DecimalType(38,18),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,StringType,true), StructField(b,ByteType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,StringType,true), StructField(b,FloatType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,StringType,true), StructField(b,ShortType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,LongType,true), StructField(b,StringType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,LongType,true), StructField(b,LongType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,LongType,true), StructField(b,IntegerType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,LongType,true), StructField(b,DecimalType(20,5),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,LongType,true), StructField(b,TimestampType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,LongType,true), StructField(b,DoubleType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,LongType,true), StructField(b,DateType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,LongType,true), StructField(b,DecimalType(10,0),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,LongType,true), StructField(b,BinaryType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,LongType,true), StructField(b,BooleanType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,LongType,true), StructField(b,DecimalType(38,18),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,LongType,true), StructField(b,ByteType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,LongType,true), StructField(b,FloatType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,LongType,true), StructField(b,ShortType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,IntegerType,true), StructField(b,StringType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,IntegerType,true), StructField(b,LongType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,IntegerType,true), StructField(b,IntegerType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,IntegerType,true), StructField(b,DecimalType(20,5),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,IntegerType,true), StructField(b,TimestampType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,IntegerType,true), StructField(b,DoubleType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,IntegerType,true), StructField(b,DateType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,IntegerType,true), StructField(b,DecimalType(10,0),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,IntegerType,true), StructField(b,BinaryType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,IntegerType,true), StructField(b,BooleanType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,IntegerType,true), StructField(b,DecimalType(38,18),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,IntegerType,true), StructField(b,ByteType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,IntegerType,true), StructField(b,FloatType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,IntegerType,true), StructField(b,ShortType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(20,5),true), StructField(b,StringType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(20,5),true), StructField(b,LongType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(20,5),true), StructField(b,IntegerType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(20,5),true), StructField(b,DecimalType(20,5),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(20,5),true), StructField(b,TimestampType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(20,5),true), StructField(b,DoubleType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(20,5),true), StructField(b,DateType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(20,5),true), StructField(b,DecimalType(10,0),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(20,5),true), StructField(b,BinaryType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(20,5),true), StructField(b,BooleanType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(20,5),true), StructField(b,DecimalType(38,18),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(20,5),true), StructField(b,ByteType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(20,5),true), StructField(b,FloatType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(20,5),true), StructField(b,ShortType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,TimestampType,true), StructField(b,StringType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,TimestampType,true), StructField(b,LongType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,TimestampType,true), StructField(b,IntegerType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,TimestampType,true), StructField(b,DecimalType(20,5),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,TimestampType,true), StructField(b,TimestampType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,TimestampType,true), StructField(b,DoubleType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,TimestampType,true), StructField(b,DateType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,TimestampType,true), StructField(b,DecimalType(10,0),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,TimestampType,true), StructField(b,BinaryType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,TimestampType,true), StructField(b,BooleanType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,TimestampType,true), StructField(b,DecimalType(38,18),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,TimestampType,true), StructField(b,ByteType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,TimestampType,true), StructField(b,FloatType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,TimestampType,true), StructField(b,ShortType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DoubleType,true), StructField(b,StringType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DoubleType,true), StructField(b,LongType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DoubleType,true), StructField(b,IntegerType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DoubleType,true), StructField(b,DecimalType(20,5),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DoubleType,true), StructField(b,TimestampType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DoubleType,true), StructField(b,DoubleType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DoubleType,true), StructField(b,DateType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DoubleType,true), StructField(b,DecimalType(10,0),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DoubleType,true), StructField(b,BinaryType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DoubleType,true), StructField(b,BooleanType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DoubleType,true), StructField(b,DecimalType(38,18),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DoubleType,true), StructField(b,ByteType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DoubleType,true), StructField(b,FloatType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DoubleType,true), StructField(b,ShortType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DateType,true), StructField(b,StringType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DateType,true), StructField(b,LongType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DateType,true), StructField(b,IntegerType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DateType,true), StructField(b,DecimalType(20,5),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DateType,true), StructField(b,TimestampType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DateType,true), StructField(b,DoubleType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DateType,true), StructField(b,DateType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DateType,true), StructField(b,DecimalType(10,0),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DateType,true), StructField(b,BinaryType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DateType,true), StructField(b,BooleanType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DateType,true), StructField(b,DecimalType(38,18),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DateType,true), StructField(b,ByteType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DateType,true), StructField(b,FloatType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DateType,true), StructField(b,ShortType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(10,0),true), StructField(b,StringType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(10,0),true), StructField(b,LongType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(10,0),true), StructField(b,IntegerType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(10,0),true), StructField(b,DecimalType(20,5),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(10,0),true), StructField(b,TimestampType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(10,0),true), StructField(b,DoubleType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(10,0),true), StructField(b,DateType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(10,0),true), StructField(b,DecimalType(10,0),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(10,0),true), StructField(b,BinaryType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(10,0),true), StructField(b,BooleanType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(10,0),true), StructField(b,DecimalType(38,18),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(10,0),true), StructField(b,ByteType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(10,0),true), StructField(b,FloatType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(10,0),true), StructField(b,ShortType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,BinaryType,true), StructField(b,StringType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,BinaryType,true), StructField(b,LongType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,BinaryType,true), StructField(b,IntegerType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,BinaryType,true), StructField(b,DecimalType(20,5),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,BinaryType,true), StructField(b,TimestampType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,BinaryType,true), StructField(b,DoubleType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,BinaryType,true), StructField(b,DateType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,BinaryType,true), StructField(b,DecimalType(10,0),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,BinaryType,true), StructField(b,BinaryType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,BinaryType,true), StructField(b,BooleanType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,BinaryType,true), StructField(b,DecimalType(38,18),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,BinaryType,true), StructField(b,ByteType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,BinaryType,true), StructField(b,FloatType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,BinaryType,true), StructField(b,ShortType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,BooleanType,true), StructField(b,StringType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,BooleanType,true), StructField(b,LongType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,BooleanType,true), StructField(b,IntegerType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,BooleanType,true), StructField(b,DecimalType(20,5),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,BooleanType,true), StructField(b,TimestampType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,BooleanType,true), StructField(b,DoubleType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,BooleanType,true), StructField(b,DateType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,BooleanType,true), StructField(b,DecimalType(10,0),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,BooleanType,true), StructField(b,BinaryType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,BooleanType,true), StructField(b,BooleanType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,BooleanType,true), StructField(b,DecimalType(38,18),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,BooleanType,true), StructField(b,ByteType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,BooleanType,true), StructField(b,FloatType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,BooleanType,true), StructField(b,ShortType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(38,18),true), StructField(b,StringType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(38,18),true), StructField(b,LongType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(38,18),true), StructField(b,IntegerType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(38,18),true), StructField(b,DecimalType(20,5),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(38,18),true), StructField(b,TimestampType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(38,18),true), StructField(b,DoubleType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(38,18),true), StructField(b,DateType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(38,18),true), StructField(b,DecimalType(10,0),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(38,18),true), StructField(b,BinaryType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(38,18),true), StructField(b,BooleanType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(38,18),true), StructField(b,DecimalType(38,18),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(38,18),true), StructField(b,ByteType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(38,18),true), StructField(b,FloatType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,DecimalType(38,18),true), StructField(b,ShortType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,ByteType,true), StructField(b,StringType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,ByteType,true), StructField(b,LongType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,ByteType,true), StructField(b,IntegerType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,ByteType,true), StructField(b,DecimalType(20,5),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,ByteType,true), StructField(b,TimestampType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,ByteType,true), StructField(b,DoubleType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,ByteType,true), StructField(b,DateType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,ByteType,true), StructField(b,DecimalType(10,0),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,ByteType,true), StructField(b,BinaryType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,ByteType,true), StructField(b,BooleanType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,ByteType,true), StructField(b,DecimalType(38,18),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,ByteType,true), StructField(b,ByteType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,ByteType,true), StructField(b,FloatType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,ByteType,true), StructField(b,ShortType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,FloatType,true), StructField(b,StringType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,FloatType,true), StructField(b,LongType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,FloatType,true), StructField(b,IntegerType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,FloatType,true), StructField(b,DecimalType(20,5),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,FloatType,true), StructField(b,TimestampType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,FloatType,true), StructField(b,DoubleType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,FloatType,true), StructField(b,DateType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,FloatType,true), StructField(b,DecimalType(10,0),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,FloatType,true), StructField(b,BinaryType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,FloatType,true), StructField(b,BooleanType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,FloatType,true), StructField(b,DecimalType(38,18),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,FloatType,true), StructField(b,ByteType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,FloatType,true), StructField(b,FloatType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,FloatType,true), StructField(b,ShortType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,ShortType,true), StructField(b,StringType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,ShortType,true), StructField(b,LongType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,ShortType,true), StructField(b,IntegerType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,ShortType,true), StructField(b,DecimalType(20,5),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,ShortType,true), StructField(b,TimestampType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,ShortType,true), StructField(b,DoubleType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,ShortType,true), StructField(b,DateType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,ShortType,true), StructField(b,DecimalType(10,0),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,ShortType,true), StructField(b,BinaryType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,ShortType,true), StructField(b,BooleanType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,ShortType,true), StructField(b,DecimalType(38,18),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,ShortType,true), StructField(b,ByteType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,ShortType,true), StructField(b,FloatType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>StructType(StructField(a,ShortType,true), StructField(b,ShortType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.909</duration>
          <className>org.apache.spark.sql.RandomDataGeneratorSuite</className>
          <testName>check size of generated map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.RowTest.xml</file>
      <name>org.apache.spark.sql.RowTest</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.014</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.RowTest</className>
          <testName>Row (without schema) throws an exception when accessing by fieldName</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.RowTest</className>
          <testName>Row (with schema) fieldIndex(name) returns field index</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RowTest</className>
          <testName>Row (with schema) getAs[T] retrieves a value by fieldname</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RowTest</className>
          <testName>Row (with schema) Accessing non existent field throws an exception</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RowTest</className>
          <testName>Row (with schema) getValuesMap() retrieves values of multiple fields as a Map(field -&gt; value)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RowTest</className>
          <testName>Row (with schema) getValuesMap() retrieves null value on non AnyVal Type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RowTest</className>
          <testName>Row (with schema) getAs() on type extending AnyVal throws an exception when accessing field that is null</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RowTest</className>
          <testName>Row (with schema) getAs() on type extending AnyVal does not throw exception when value is null</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RowTest</className>
          <testName>row equals equality check for external rows</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RowTest</className>
          <testName>row equals equality check for internal rows</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RowTest</className>
          <testName>row immutability copy should return same ref for external rows</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RowTest</className>
          <testName>row immutability copy should return same ref for internal rows</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.RowTest</className>
          <testName>row immutability toSeq should not expose internal state for external rows</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RowTest</className>
          <testName>row immutability toSeq should not expose internal state for internal rows</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.CatalystTypeConvertersSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.CatalystTypeConvertersSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.006</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.CatalystTypeConvertersSuite</className>
          <testName>null handling in rows</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.CatalystTypeConvertersSuite</className>
          <testName>null handling for individual values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.CatalystTypeConvertersSuite</className>
          <testName>option handling in convertToCatalyst</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.CatalystTypeConvertersSuite</className>
          <testName>option handling in createToCatalystConverter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.CatalystTypeConvertersSuite</className>
          <testName>primitive array handling</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.CatalystTypeConvertersSuite</className>
          <testName>An array with null handling</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.DistributionSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.DistributionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.006</duration>
      <cases>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.DistributionSuite</className>
          <testName>HashPartitioning (with nullSafe = true) is the output partitioning</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.DistributionSuite</className>
          <testName>RangePartitioning is the output partitioning</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.PartitioningSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.PartitioningSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.006</duration>
      <cases>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.PartitioningSuite</className>
          <testName>HashPartitioning compatibility should be sensitive to expression ordering (SPARK-9785)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.ScalaReflectionSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.ScalaReflectionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>144.733</duration>
      <cases>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.sql.catalyst.ScalaReflectionSuite</className>
          <testName>SQLUserDefinedType annotation on Scala structure</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.catalyst.ScalaReflectionSuite</className>
          <testName>primitive data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.sql.catalyst.ScalaReflectionSuite</className>
          <testName>nullable data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.sql.catalyst.ScalaReflectionSuite</className>
          <testName>optional data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.sql.catalyst.ScalaReflectionSuite</className>
          <testName>complex data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.sql.catalyst.ScalaReflectionSuite</className>
          <testName>generic data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.sql.catalyst.ScalaReflectionSuite</className>
          <testName>tuple data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.sql.catalyst.ScalaReflectionSuite</className>
          <testName>type-aliased data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.catalyst.ScalaReflectionSuite</className>
          <testName>convert PrimitiveData to catalyst</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.sql.catalyst.ScalaReflectionSuite</className>
          <testName>convert Option[Product] to catalyst</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.sql.catalyst.ScalaReflectionSuite</className>
          <testName>infer schema from case class with multiple constructors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.sql.catalyst.ScalaReflectionSuite</className>
          <testName>get parameter type from a function object</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.catalyst.ScalaReflectionSuite</className>
          <testName>SPARK-15062: Get correct serializer for List[_]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>7.891</duration>
          <className>org.apache.spark.sql.catalyst.ScalaReflectionSuite</className>
          <testName>SPARK-13640: thread safety of mirror</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>10.147</duration>
          <className>org.apache.spark.sql.catalyst.ScalaReflectionSuite</className>
          <testName>SPARK-13640: thread safety of dataTypeFor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>61.13</duration>
          <className>org.apache.spark.sql.catalyst.ScalaReflectionSuite</className>
          <testName>SPARK-13640: thread safety of constructorFor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>23.348</duration>
          <className>org.apache.spark.sql.catalyst.ScalaReflectionSuite</className>
          <testName>SPARK-13640: thread safety of extractorsFor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.134</duration>
          <className>org.apache.spark.sql.catalyst.ScalaReflectionSuite</className>
          <testName>SPARK-13640: thread safety of getConstructorParameters(cls)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.109</duration>
          <className>org.apache.spark.sql.catalyst.ScalaReflectionSuite</className>
          <testName>SPARK-13640: thread safety of getConstructorParameterNames</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>7.27</duration>
          <className>org.apache.spark.sql.catalyst.ScalaReflectionSuite</className>
          <testName>SPARK-13640: thread safety of getClassFromType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>25.108</duration>
          <className>org.apache.spark.sql.catalyst.ScalaReflectionSuite</className>
          <testName>SPARK-13640: thread safety of schemaFor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>9.253</duration>
          <className>org.apache.spark.sql.catalyst.ScalaReflectionSuite</className>
          <testName>SPARK-13640: thread safety of localTypeOf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.sql.catalyst.ScalaReflectionSuite</className>
          <testName>SPARK-13640: thread safety of getClassNameFromType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.05</duration>
          <className>org.apache.spark.sql.catalyst.ScalaReflectionSuite</className>
          <testName>SPARK-13640: thread safety of getParameterTypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.04</duration>
          <className>org.apache.spark.sql.catalyst.ScalaReflectionSuite</className>
          <testName>SPARK-13640: thread safety of getConstructorParameters(tpe)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.12599999</duration>
      <cases>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>scalar subquery with 2 columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>scalar subquery with no column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>single invalid type, single arg</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>single invalid type, second arg</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>multiple invalid type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>invalid window function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>distinct window function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>nested aggregate functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>offset window function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>too many generators</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>unresolved attributes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>unresolved attributes with a generated name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>unresolved star expansion in max</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>bad casts</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>sorting by unsupported column types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>sorting by attributes are not from grouping expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>non-boolean filters</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>non-boolean join conditions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>missing group by</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>ambiguous field</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>ambiguous field due to case insensitivity</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>missing field</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>catch all unresolved plan</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>union with unequal number of columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>intersect with unequal number of columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>except with unequal number of columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>union with incompatible column types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>intersect with incompatible column types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>except with incompatible column types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>SPARK-9955: correct error message for aggregate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>slide duration greater than window in time window</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>start time greater than slide duration in time window</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>start time equal to slide duration in time window</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>negative window duration in time window</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>zero window duration in time window</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>negative slide duration in time window</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>zero slide duration in time window</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>negative start time in time window</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>generator nested in expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>generator appears in operator which is not Project</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>num_rows in limit clause must be equal to or greater than 0</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>more than one generators in SELECT</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>SPARK-6452 regression test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>error test for self-join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>check grouping expression data types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>we should fail analysis when we find nested aggregate functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>Join can work on binary types but can&apos;t work on map types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>PredicateSubQuery is used outside of a filter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>PredicateSubQuery is used is a nested condition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisErrorSuite</className>
          <testName>PredicateSubQuery correlated predicate is nested in an illegal plan</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.analysis.AnalysisSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.analysis.AnalysisSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.26500002</duration>
      <cases>
        <case>
          <duration>0.164</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisSuite</className>
          <testName>union project *</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisSuite</className>
          <testName>check project&apos;s resolved</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisSuite</className>
          <testName>analyze project</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisSuite</className>
          <testName>resolve sort references - filter/limit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisSuite</className>
          <testName>resolve sort references - join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisSuite</className>
          <testName>resolve sort references - aggregate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisSuite</className>
          <testName>resolve relations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisSuite</className>
          <testName>divide should be casted into fractional types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisSuite</className>
          <testName>pull out nondeterministic expressions from RepartitionByExpression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisSuite</className>
          <testName>pull out nondeterministic expressions from Sort</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisSuite</className>
          <testName>SPARK-9634: cleanup unnecessary Aliases in LogicalPlan</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisSuite</className>
          <testName>Analysis may leave unnecassary aliases</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisSuite</className>
          <testName>SPARK-10534: resolve attribute references in order by clause</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisSuite</className>
          <testName>self intersect should resolve duplicate expression IDs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisSuite</className>
          <testName>) expression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisSuite</className>
          <testName>SPARK-8654: different types in inlist but can be converted to a common type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisSuite</className>
          <testName>SPARK-8654: check type compatibility error</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisSuite</className>
          <testName>SPARK-11725: correctly handle null inputs for ScalaUDF</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisSuite</className>
          <testName>SPARK-11863 mixture of aliases and real columns in order by clause - tpcds 19,55,71</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisSuite</className>
          <testName>Eliminate the unnecessary union</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisSuite</className>
          <testName>SPARK-12102: Ignore nullablity when comparing two sides of case</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisSuite</className>
          <testName>Keep attribute qualifiers after dedup</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisSuite</className>
          <testName>SPARK-15776: test whether Divide expression&apos;s data type can be deduced correctly by analyzer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.AnalysisSuite</className>
          <testName>SPARK-18058: union and set operations shall not care about the nullability when comparing column types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.analysis.DecimalPrecisionSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.analysis.DecimalPrecisionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.8219999</duration>
      <cases>
        <case>
          <duration>0.369</duration>
          <className>org.apache.spark.sql.catalyst.analysis.DecimalPrecisionSuite</className>
          <testName>basic operations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.049</duration>
          <className>org.apache.spark.sql.catalyst.analysis.DecimalPrecisionSuite</className>
          <testName>Comparison operations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.115</duration>
          <className>org.apache.spark.sql.catalyst.analysis.DecimalPrecisionSuite</className>
          <testName>decimal precision for union</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.038</duration>
          <className>org.apache.spark.sql.catalyst.analysis.DecimalPrecisionSuite</className>
          <testName>bringing in primitive types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.117</duration>
          <className>org.apache.spark.sql.catalyst.analysis.DecimalPrecisionSuite</className>
          <testName>maximum decimals</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.DecimalPrecisionSuite</className>
          <testName>isWiderThan</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.061</duration>
          <className>org.apache.spark.sql.catalyst.analysis.DecimalPrecisionSuite</className>
          <testName>strength reduction for integer/decimal comparisons - basic test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.071</duration>
          <className>org.apache.spark.sql.catalyst.analysis.DecimalPrecisionSuite</className>
          <testName>strength reduction for integer/decimal comparisons - overflow test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.analysis.ExpressionTypeCheckingSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.analysis.ExpressionTypeCheckingSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.28800002</duration>
      <cases>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.sql.catalyst.analysis.ExpressionTypeCheckingSuite</className>
          <testName>check types for unary arithmetic</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.071</duration>
          <className>org.apache.spark.sql.catalyst.analysis.ExpressionTypeCheckingSuite</className>
          <testName>check types for binary arithmetic</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.084</duration>
          <className>org.apache.spark.sql.catalyst.analysis.ExpressionTypeCheckingSuite</className>
          <testName>check types for predicates</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.sql.catalyst.analysis.ExpressionTypeCheckingSuite</className>
          <testName>check types for aggregates</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.018</duration>
          <className>org.apache.spark.sql.catalyst.analysis.ExpressionTypeCheckingSuite</className>
          <testName>check types for others</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.analysis.ExpressionTypeCheckingSuite</className>
          <testName>check types for CreateNamedStruct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.sql.catalyst.analysis.ExpressionTypeCheckingSuite</className>
          <testName>check types for CreateMap</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.035</duration>
          <className>org.apache.spark.sql.catalyst.analysis.ExpressionTypeCheckingSuite</className>
          <testName>check types for ROUND/BROUND</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.catalyst.analysis.ExpressionTypeCheckingSuite</className>
          <testName>check types for Greatest/Least</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.analysis.ResolveInlineTablesSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.analysis.ResolveInlineTablesSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.011</duration>
      <cases>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.analysis.ResolveInlineTablesSuite</className>
          <testName>validate inputs are foldable</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.ResolveInlineTablesSuite</className>
          <testName>validate input dimensions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.ResolveInlineTablesSuite</className>
          <testName>do not fire the rule if not all expressions are resolved</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.analysis.ResolveInlineTablesSuite</className>
          <testName>convert</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.analysis.ResolveInlineTablesSuite</className>
          <testName>nullability inference in convert</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.analysis.ResolveNaturalJoinSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.analysis.ResolveNaturalJoinSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.07000001</duration>
      <cases>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.catalyst.analysis.ResolveNaturalJoinSuite</className>
          <testName>natural/using inner join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.analysis.ResolveNaturalJoinSuite</className>
          <testName>natural/using left join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.analysis.ResolveNaturalJoinSuite</className>
          <testName>natural/using right join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.analysis.ResolveNaturalJoinSuite</className>
          <testName>natural/using full outer join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.analysis.ResolveNaturalJoinSuite</className>
          <testName>natural/using inner join with no nullability</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.analysis.ResolveNaturalJoinSuite</className>
          <testName>natural/using left join with no nullability</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.analysis.ResolveNaturalJoinSuite</className>
          <testName>natural/using right join with no nullability</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.analysis.ResolveNaturalJoinSuite</className>
          <testName>natural/using full outer join with no nullability</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.ResolveNaturalJoinSuite</className>
          <testName>using unresolved attribute</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.analysis.ResolveNaturalJoinSuite</className>
          <testName>using join with a case sensitive analyzer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.analysis.ResolveNaturalJoinSuite</className>
          <testName>using join with a case insensitive analyzer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.analysis.SubstituteUnresolvedOrdinalsSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.analysis.SubstituteUnresolvedOrdinalsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.031000001</duration>
      <cases>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.analysis.SubstituteUnresolvedOrdinalsSuite</className>
          <testName>unresolved ordinal should not be unresolved</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.sql.catalyst.analysis.SubstituteUnresolvedOrdinalsSuite</className>
          <testName>order by ordinal</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.catalyst.analysis.SubstituteUnresolvedOrdinalsSuite</className>
          <testName>group by ordinal</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.075</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>implicit type cast - ByteType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>implicit type cast - ShortType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>implicit type cast - IntegerType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>implicit type cast - LongType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>implicit type cast - FloatType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>implicit type cast - DoubleType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>implicit type cast - DecimalType(10, 2)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>implicit type cast - BinaryType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>implicit type cast - BooleanType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>implicit type cast - StringType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>implicit type cast - DateType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>implicit type cast - TimestampType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>implicit type cast - ArrayType(StringType)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>implicit type cast - MapType(StringType, StringType)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>add(&quot;a1&quot;, StringType)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>implicit type cast - NullType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>implicit type cast - CalendarIntervalType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>eligible implicit type cast - TypeCollection</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>ineligible implicit type cast - TypeCollection</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>tightest common bound for types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>cast NullType for expressions that implement ExpectsInputTypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>cast NullType for binary operators</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>coalesce casts</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>CreateArray casts</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>CreateMap casts</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>greatest/least cast</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>nanvl casts</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>type coercion for If</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>type coercion for CaseKeyWhen</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>BooleanEquality type cast</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>BooleanEquality simplification</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>WidenSetOperationTypes for except and intersect</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>WidenSetOperationTypes for union</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>Transform Decimal precision/scale for union except and intersect</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>rule for date/timestamp operations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>make sure rules do not fire early</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>SPARK-15776 Divide expression&apos;s dataType should be casted to Double or Decimal in aggregation function like sum</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.TypeCoercionSuite</className>
          <testName>SPARK-17117 null type coercion in divide</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.06799998</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>batch plan - local relation: supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>batch plan - streaming source: not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>batch plan - select on streaming source: not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - no streaming source</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - commmands: not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - aggregate - multiple batch aggregations: supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - aggregate - multiple aggregations but only one streaming aggregation: supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - aggregate - multiple streaming aggregations: not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - inner join with stream-stream relations: not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - inner join with stream-batch relations: supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - inner join with batch-stream relations: supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - inner join with batch-batch relations: supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - full outer join with stream-stream relations: not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - full outer join with stream-batch relations: not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - full outer join with batch-stream relations: not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - full outer join with batch-batch relations: supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - left outer join with stream-stream relations: not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - left outer join with stream-batch relations: supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - left outer join with batch-stream relations: not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - left outer join with batch-batch relations: supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - left semi join with stream-stream relations: not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - left semi join with stream-batch relations: supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - left semi join with batch-stream relations: not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - left semi join with batch-batch relations: supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - left anti join with stream-stream relations: not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - left anti join with stream-batch relations: supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - left anti join with batch-stream relations: not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - left anti join with batch-batch relations: supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - right outer join with stream-stream relations: not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - right outer join with stream-batch relations: not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - right outer join with batch-stream relations: supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - right outer join with batch-batch relations: supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - cogroup with stream-stream relations: not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - cogroup with stream-batch relations: not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - cogroup with batch-stream relations: not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - cogroup with batch-batch relations: supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - union with stream-stream relations: supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - union with stream-batch relations: not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - union with batch-stream relations: not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - union with batch-batch relations: supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - except with stream-stream relations: not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - except with stream-batch relations: supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - except with batch-stream relations: not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - except with batch-batch relations: supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - intersect with stream-stream relations: not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - intersect with stream-batch relations: supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - intersect with batch-stream relations: supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - intersect with batch-batch relations: supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - sort with stream relation: not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - sort with batch relation: supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - sort - sort over aggregated data in Complete output mode: supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - sort - sort over aggregated data in Update output mode: not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - sort partitions with stream relation: not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - sort partitions with batch relation: supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - sample with stream relation: not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - sample with batch relation: supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - window with stream relation: not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - window with batch relation: supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - Append output mode - no aggregation: supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - Append output mode - aggregation: not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - Update output mode - no aggregation: not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - Update output mode - aggregation: supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - Complete output mode - no aggregation: not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite</className>
          <testName>streaming plan - Complete output mode - aggregation: supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.478</duration>
      <cases>
        <case>
          <duration>0.044</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>basic create and list databases</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>get database when a database exists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.041</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>get database should throw exception when the database does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.04</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>list databases without pattern</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>list databases with pattern</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.058</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>drop database</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.195</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>drop database when the database is not empty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.069</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>drop database when the database does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>alter database</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>alter database should throw exception when the database does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.032</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>the table type of an external table should be EXTERNAL_TABLE</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>create table when the table already exists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>drop table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>drop table when database/table does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>rename table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>rename table when database/table does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>rename table when destination table already exists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>alter table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>alter table when database/table does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>get table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.182</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>get table when database/table does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>list tables without pattern</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>list tables with pattern</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>column names should be case-preserving and column nullability should be retained</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>basic create and list partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>create partitions when database/table does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.032</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>create partitions that already exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>create partitions without location</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.033</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>list partitions with partial partition spec</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>drop partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>drop partitions when database/table does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>drop partitions that do not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>get partition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>get partition when database/table does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.032</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>rename partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.039</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>rename partitions should update the location for managed table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>rename partitions when database/table does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>rename partitions when the new partition already exists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>alter partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>alter partitions when database/table does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>basic create and list functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>create function when database does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>create function that already exists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>drop function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>drop function when database does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>drop function that does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>get function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>get function when database does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>rename function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>rename function when database does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>rename function when new function already exists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>list functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.057</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>create/drop database should create/delete the directory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.13</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>create/drop/rename table should create/delete/rename the directory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.32</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>create/drop/rename partitions should create/delete/rename the directory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.045</duration>
          <className>org.apache.spark.sql.catalyst.catalog.InMemoryCatalogSuite</className>
          <testName>drop partition from external table should not delete the directory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.459</duration>
      <cases>
        <case>
          <duration>0.633</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>basic create and list databases</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.059</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>get database when a database exists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>get database should throw exception when the database does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>list databases without pattern</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>list databases with pattern</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.032</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>drop database</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.07</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>drop database when the database is not empty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>drop database when the database does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>drop current database and drop default database</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>alter database</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>alter database should throw exception when the database does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>get/set current database</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>create table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>create table when database does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>create temp table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>drop table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>drop table when database/table does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.073</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>drop temp table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>rename table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>rename table when database/table does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>rename temp table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>alter table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>alter table when database/table does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>get table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>get table when database/table does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>get option of table metadata</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>lookup table relation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>lookup table relation with alias</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>lookup view with view name in alias</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>table exists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>getTempViewOrPermanentTableMetadata on temporary views</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>list tables without pattern</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>list tables with pattern</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>basic create and list partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>create partitions when database/table does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>create partitions that already exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>create partitions with invalid part spec</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>drop partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>drop partitions when database/table does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>drop partitions that do not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>drop partitions with invalid partition spec</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>get partition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>get partition when database/table does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>get partition with invalid partition spec</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>rename partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>rename partitions when database/table does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>rename partition with invalid partition spec</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>alter partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>alter partitions when database/table does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>alter partition with invalid partition spec</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>list partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>list partitions when database/table does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>basic create and list functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>create function when database does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>create function that already exists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>create temp function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.165</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>isTemporaryFunction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>drop function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>drop function when database/function does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>drop temp function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>get function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>get function when database/function does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>lookup temp function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>list functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.sql.catalyst.catalog.SessionCatalogSuite</className>
          <testName>list functions when database does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.encoders.EncoderErrorMessageSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.encoders.EncoderErrorMessageSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.038</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.encoders.EncoderErrorMessageSuite</className>
          <testName>primitive types in encoders using Kryo serialization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.encoders.EncoderErrorMessageSuite</className>
          <testName>primitive types in encoders using Java serialization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.035</duration>
          <className>org.apache.spark.sql.catalyst.encoders.EncoderErrorMessageSuite</className>
          <testName>nice error message for missing encoder</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.encoders.EncoderResolutionSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.encoders.EncoderResolutionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.22100003</duration>
      <cases>
        <case>
          <duration>0.038</duration>
          <className>org.apache.spark.sql.catalyst.encoders.EncoderResolutionSuite</className>
          <testName>real type doesn&apos;t match encoder schema but they are compatible: product</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.sql.catalyst.encoders.EncoderResolutionSuite</className>
          <testName>real type doesn&apos;t match encoder schema but they are compatible: nested product</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.sql.catalyst.encoders.EncoderResolutionSuite</className>
          <testName>real type doesn&apos;t match encoder schema but they are compatible: tupled encoder</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.catalyst.encoders.EncoderResolutionSuite</className>
          <testName>nullability of array type element should not fail analysis</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.catalyst.encoders.EncoderResolutionSuite</className>
          <testName>the real number of fields doesn&apos;t match encoder schema: tuple encoder</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.sql.catalyst.encoders.EncoderResolutionSuite</className>
          <testName>the real number of fields doesn&apos;t match encoder schema: nested tuple encoder</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.sql.catalyst.encoders.EncoderResolutionSuite</className>
          <testName>nested case class can have different number of fields from the real schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.sql.catalyst.encoders.EncoderResolutionSuite</className>
          <testName>throw exception if real type is not compatible with encoder schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.encoders.EncoderResolutionSuite</className>
          <testName>cast from int to Long should success</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.encoders.EncoderResolutionSuite</className>
          <testName>Timestamp should success</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.encoders.EncoderResolutionSuite</className>
          <testName>cast from bigint to String should success</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.encoders.EncoderResolutionSuite</className>
          <testName>BigDecimal should success</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.encoders.EncoderResolutionSuite</className>
          <testName>BigDecimal should success</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.encoders.EncoderResolutionSuite</className>
          <testName>cast from bigint to Int should fail</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.encoders.EncoderResolutionSuite</className>
          <testName>Date should fail</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.encoders.EncoderResolutionSuite</className>
          <testName>cast from decimal(38,18) to Double should fail</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.encoders.EncoderResolutionSuite</className>
          <testName>BigDecimal should fail</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.encoders.EncoderResolutionSuite</className>
          <testName>cast from decimal(38,18) to Int should fail</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.encoders.EncoderResolutionSuite</className>
          <testName>cast from string to Long should fail</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.9079993</duration>
      <cases>
        <case>
          <duration>0.033</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for primitive boolean: false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for primitive byte: -3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for primitive short: -3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for primitive int: -3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.018</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for primitive long: -3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>7</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.018</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>7</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for boxed boolean: false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.018</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for boxed byte: -3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for boxed short: -3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for boxed int: -3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.018</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for boxed long: -3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>7</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>7</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>211321313</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.018</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>23123</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for scala biginteger: 23134123123</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for java BigInteger: 23134123123</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>211321313</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for string: hello</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for date: 2012-12-23</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>0</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>Timestamp;@30a6cb8c</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for binary: [B@55a95479</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for seq of int: List(31, -123, 4)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for seq of string: List(abc, xyz)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.063</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for seq of string with null: List(abc, null, xyz)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for empty seq of int: List()</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.033</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for empty seq of string: List()</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for seq of seq of int: List(List(31, -123), null, List(4, 67))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for seq of seq of string: List(List(abc, xyz), List(null), null, List(1, null, 2))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for array of int: [I@3bff4ca8</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>String;@41e49358</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>String;@3746e4eb</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for empty array of int: [I@1fcf3adb</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>String;@7435225e</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for array of array of int: [[I@2fd81ee0</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>String;@5c38213e</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.039</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for map: Map(1 -&gt; a, 2 -&gt; b)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for map with null: Map(1 -&gt; a, 2 -&gt; null)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.041</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for map of map: Map(1 -&gt; Map(a -&gt; 1), 2 -&gt; Map(b -&gt; 2))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for null seq in tuple: (null)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for null map in tuple: (null)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for list of int: List(1, 2)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.032</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for list with String and null: List(a, null)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>org/)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for kryo string: hello</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>KryoSerializable@f</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for java string: hello</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>JavaSerializable@f</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for InnerClass: InnerClass(1)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>ExpressionEncoderSuite$InnerClass;@4bd169e8</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>Option;@16ee43b6</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>0,1,1,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.06</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>0,1,1,true)))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for OptionalData: OptionalData(None,None,None,None,None,None,None,None)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for Option in array: List(Some(1), None)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for Option in map: Map(1 -&gt; Some(10), 2 -&gt; Some(20), 3 -&gt; None)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.035</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>0,1,1,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for BoxedData: BoxedData(null,null,null,null,null,null,null)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.073</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>0,1,1,true)))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>0,1,1,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.079</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>0,1,1,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for NestedArray: NestedArray([[I@4c93b45d)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for Tuple2: (Seq[(String, String)],List((a,b)))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for Tuple2: (Seq[(Int, Int)],List((1,2)))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.038</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for Tuple2: (Seq[(Long, Long)],List((1,2)))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.039</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>0)))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.044</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>0)))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.039</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for Tuple2: (Seq[(Short, Short)],List((1,2)))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.041</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for Tuple2: (Seq[(Byte, Byte)],List((1,2)))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.039</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for Tuple2: (Seq[(Boolean, Boolean)],List((true,false)))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.044</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for Tuple2: (ArrayBuffer[(String, String)],ArrayBuffer((a,b)))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.04</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for Tuple2: (ArrayBuffer[(Int, Int)],ArrayBuffer((1,2)))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for Tuple2: (ArrayBuffer[(Long, Long)],ArrayBuffer((1,2)))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>0)))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.046</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>0)))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.041</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for Tuple2: (ArrayBuffer[(Short, Short)],ArrayBuffer((1,2)))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.04</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for Tuple2: (ArrayBuffer[(Byte, Byte)],ArrayBuffer((1,2)))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.038</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for Tuple2: (ArrayBuffer[(Boolean, Boolean)],ArrayBuffer((true,false)))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.076</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for Tuple2: (Seq[Seq[(Int, Int)]],List(List((1,2))))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for tuple with 2 flat encoders: (1,10)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.049</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>0,1,1,true),(3,30))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>0,1,1,true),3)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>0,1,1,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for nested tuple encoder: (1,(10,100))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for primitive value class: PrimitiveValueClass(42)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>encode/decode for reference value class: ReferenceValueClass(Container(1))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>ExamplePoint@691)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.08</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>nullable of encoder schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite</className>
          <testName>null check for map key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.encoders.RowEncoderSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.encoders.RowEncoderSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>6.542</duration>
      <cases>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.sql.catalyst.encoders.RowEncoderSuite</className>
          <testName>encode/decode: struct&lt;null:null,boolean:boolean,byte:tinyint,short:smallint,int:int,long:bigint,float:float,double:double,decimal:decimal(38,18),string:string,binary:binary,date:date,timestamp:timestamp,udt:examplepoint&gt;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.82</duration>
          <className>org.apache.spark.sql.catalyst.encoders.RowEncoderSuite</className>
          <testName>encode/decode: struct&lt;arrayOfNull:array&lt;null&gt;,arrayOfString:array&lt;string&gt;,arrayOfArrayOfString:array&lt;array&lt;string&gt;&gt;,arrayOfArrayOfInt:array&lt;array&lt;int&gt;&gt;,arrayOfMap:array&lt;map&lt;string,string&gt;&gt;,arrayOfStruct:array&lt;struct&lt;str:string&gt;&gt;,arrayOfUDT:array&lt;examplepoint&gt;&gt;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.344</duration>
          <className>org.apache.spark.sql.catalyst.encoders.RowEncoderSuite</className>
          <testName>encode/decode: struct&lt;mapOfIntAndString:map&lt;int,string&gt;,mapOfStringAndArray:map&lt;string,array&lt;string&gt;&gt;,mapOfArrayAndInt:map&lt;array&lt;string&gt;,int&gt;,mapOfArray:map&lt;array&lt;string&gt;,array&lt;string&gt;&gt;,mapOfStringAndStruct:map&lt;string,struct&lt;str:string&gt;&gt;,mapOfStructAndString:map&lt;struct&lt;str:string&gt;,string&gt;,mapOfStruct:map&lt;struct&lt;str:string&gt;,struct&lt;str:string&gt;&gt;&gt;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.17</duration>
          <className>org.apache.spark.sql.catalyst.encoders.RowEncoderSuite</className>
          <testName>encode/decode: struct&lt;structOfString:struct&lt;str:string&gt;,structOfStructOfString:struct&lt;struct:struct&lt;str:string&gt;&gt;,structOfArray:struct&lt;array:array&lt;string&gt;&gt;,structOfMap:struct&lt;map:map&lt;string,string&gt;&gt;,structOfArrayAndMap:struct&lt;array:array&lt;string&gt;,map:map&lt;string,string&gt;&gt;,structOfUDT:struct&lt;udt:examplepoint&gt;&gt;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.sql.catalyst.encoders.RowEncoderSuite</className>
          <testName>encode/decode decimal type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.sql.catalyst.encoders.RowEncoderSuite</className>
          <testName>RowEncoder should preserve decimal precision and scale</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.encoders.RowEncoderSuite</className>
          <testName>RowEncoder should preserve schema nullability</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.encoders.RowEncoderSuite</className>
          <testName>RowEncoder should preserve nested column name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.sql.catalyst.encoders.RowEncoderSuite</className>
          <testName>RowEncoder should support primitive arrays</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.sql.catalyst.encoders.RowEncoderSuite</className>
          <testName>RowEncoder should support array as the external type for ArrayType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.encoders.RowEncoderSuite</className>
          <testName>RowEncoder should throw RuntimeException if input row object is null</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.sql.catalyst.encoders.RowEncoderSuite</className>
          <testName>RowEncoder should validate external type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.ArithmeticExpressionSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.ArithmeticExpressionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>53.342</duration>
      <cases>
        <case>
          <duration>6.09</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ArithmeticExpressionSuite</className>
          <testName>+ (Add)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.446</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ArithmeticExpressionSuite</className>
          <testName>- (UnaryMinus)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.624</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ArithmeticExpressionSuite</className>
          <testName>- (Minus)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.609</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ArithmeticExpressionSuite</className>
          <testName>* (Multiply)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.853</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ArithmeticExpressionSuite</className>
          <testName>/ (Divide) basic</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ArithmeticExpressionSuite</className>
          <testName>/ (Divide) for integral type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.246</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ArithmeticExpressionSuite</className>
          <testName>% (Remainder)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.051</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ArithmeticExpressionSuite</className>
          <testName>SPARK-17617: % (Remainder) double % double on super big double</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.351</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ArithmeticExpressionSuite</className>
          <testName>Abs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.75</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ArithmeticExpressionSuite</className>
          <testName>pmod</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>14.302</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ArithmeticExpressionSuite</className>
          <testName>function least</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>14.021</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ArithmeticExpressionSuite</className>
          <testName>function greatest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.AttributeSetSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.AttributeSetSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.003</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.AttributeSetSuite</className>
          <testName>sanity check</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.expressions.AttributeSetSuite</className>
          <testName>checks by id not name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.expressions.AttributeSetSuite</className>
          <testName>++ preserves AttributeSet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.AttributeSetSuite</className>
          <testName>extracts all references references</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.expressions.AttributeSetSuite</className>
          <testName>dedups attributes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.expressions.AttributeSetSuite</className>
          <testName>subset</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.AttributeSetSuite</className>
          <testName>equality</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.BitwiseExpressionsSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.BitwiseExpressionsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>43.903</duration>
      <cases>
        <case>
          <duration>7.351</duration>
          <className>org.apache.spark.sql.catalyst.expressions.BitwiseExpressionsSuite</className>
          <testName>BitwiseNOT</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>11.859</duration>
          <className>org.apache.spark.sql.catalyst.expressions.BitwiseExpressionsSuite</className>
          <testName>BitwiseAnd</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>12.357</duration>
          <className>org.apache.spark.sql.catalyst.expressions.BitwiseExpressionsSuite</className>
          <testName>BitwiseOr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>12.336</duration>
          <className>org.apache.spark.sql.catalyst.expressions.BitwiseExpressionsSuite</className>
          <testName>BitwiseXor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.CallMethodViaReflectionSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.CallMethodViaReflectionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.137</duration>
      <cases>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CallMethodViaReflectionSuite</className>
          <testName>findMethod via reflection for static methods</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CallMethodViaReflectionSuite</className>
          <testName>findMethod for a JDK library</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CallMethodViaReflectionSuite</className>
          <testName>class not found</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CallMethodViaReflectionSuite</className>
          <testName>method not found because name does not match</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CallMethodViaReflectionSuite</className>
          <testName>method not found because there is no static method</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CallMethodViaReflectionSuite</className>
          <testName>input type checking</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CallMethodViaReflectionSuite</className>
          <testName>unsupported type checking</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.088</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CallMethodViaReflectionSuite</className>
          <testName>invoking methods using acceptable types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.CastSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.CastSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>24.398998</duration>
      <cases>
        <case>
          <duration>10.45</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CastSuite</className>
          <testName>null cast</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.145</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CastSuite</className>
          <testName>cast string to date</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.218</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CastSuite</className>
          <testName>cast string to timestamp</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.099</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CastSuite</className>
          <testName>cast from int</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.895</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CastSuite</className>
          <testName>cast from long</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.39</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CastSuite</className>
          <testName>cast from boolean</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.156</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CastSuite</className>
          <testName>cast from int 2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.56</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CastSuite</className>
          <testName>cast from float</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.612</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CastSuite</className>
          <testName>cast from double</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CastSuite</className>
          <testName>cast from string</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.324</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CastSuite</className>
          <testName>data type casting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.373</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CastSuite</className>
          <testName>cast and add</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.541</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CastSuite</className>
          <testName>from decimal</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.921</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CastSuite</className>
          <testName>casting to fixed-precision decimals</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.636</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CastSuite</className>
          <testName>cast from date</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.257</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CastSuite</className>
          <testName>cast from timestamp</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.354</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CastSuite</className>
          <testName>cast from array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.394</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CastSuite</className>
          <testName>cast from map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.588</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CastSuite</className>
          <testName>cast from struct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.111</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CastSuite</className>
          <testName>cast struct with a timestamp field</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CastSuite</className>
          <testName>complex casting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.217</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CastSuite</className>
          <testName>cast between string and interval</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.149</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CastSuite</className>
          <testName>cast string to boolean</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CastSuite</className>
          <testName>SPARK-16729 type checking for casting to date type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>8.577999</duration>
      <cases>
        <case>
          <duration>0.061</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite</className>
          <testName>multithreaded eval</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite</className>
          <testName>metrics are recorded on compile</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.048</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite</className>
          <testName>SPARK-8443: split wide projections into blocks due to JVM code size limit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.032</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite</className>
          <testName>SPARK-13242: case-when expression with large number of branches (or cases)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.582</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite</className>
          <testName>SPARK-14793: split wide array creation into blocks due to JVM code size limit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.613</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite</className>
          <testName>SPARK-14793: split wide map creation into blocks due to JVM code size limit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.575</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite</className>
          <testName>SPARK-14793: split wide struct creation into blocks due to JVM code size limit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.196</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite</className>
          <testName>SPARK-14793: split wide named struct creation into blocks due to JVM code size limit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.331</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite</className>
          <testName>SPARK-14224: split wide external row creation into blocks due to JVM code size limit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.073</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite</className>
          <testName>SPARK-17702: split wide constructor into blocks due to JVM code size limit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite</className>
          <testName>test generated safe and unsafe projection</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite</className>
          <testName>*/ in the data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite</className>
          <testName>\u in the data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite</className>
          <testName>check compilation error doesn&apos;t occur caused by specific literal</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite</className>
          <testName>SPARK-17160: field names are properly escaped by GetExternalRowField</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite</className>
          <testName>SPARK-17160: field names are properly escaped by AssertTrue</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.CollectionExpressionsSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.CollectionExpressionsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.49300003</duration>
      <cases>
        <case>
          <duration>0.089</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CollectionExpressionsSuite</className>
          <testName>Array and Map Size</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.103</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CollectionExpressionsSuite</className>
          <testName>MapKeys/MapValues</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.143</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CollectionExpressionsSuite</className>
          <testName>Sort Array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.158</duration>
          <className>org.apache.spark.sql.catalyst.expressions.CollectionExpressionsSuite</className>
          <testName>Array contains</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.ComplexTypeSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.ComplexTypeSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.73599994</duration>
      <cases>
        <case>
          <duration>0.12</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ComplexTypeSuite</className>
          <testName>GetArrayItem</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.07</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ComplexTypeSuite</className>
          <testName>GetMapValue</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ComplexTypeSuite</className>
          <testName>GetStructField</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.035</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ComplexTypeSuite</className>
          <testName>GetArrayStructFields</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.117</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ComplexTypeSuite</className>
          <testName>CreateArray</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.171</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ComplexTypeSuite</className>
          <testName>CreateMap</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.038</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ComplexTypeSuite</className>
          <testName>CreateStruct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.051</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ComplexTypeSuite</className>
          <testName>CreateNamedStruct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ComplexTypeSuite</className>
          <testName>test dsl for complex type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ComplexTypeSuite</className>
          <testName>error message of ExtractValue</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ComplexTypeSuite</className>
          <testName>ensure to preserve metadata</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.05</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ComplexTypeSuite</className>
          <testName>StringToMap</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.ConditionalExpressionSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.ConditionalExpressionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>3.775</duration>
      <cases>
        <case>
          <duration>3.68</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ConditionalExpressionSuite</className>
          <testName>if</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.045</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ConditionalExpressionSuite</className>
          <testName>case when</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.05</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ConditionalExpressionSuite</className>
          <testName>case key when</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.DateExpressionsSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.DateExpressionsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>49.511993</duration>
      <cases>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.expressions.DateExpressionsSuite</className>
          <testName>datetime function current_date</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.expressions.DateExpressionsSuite</className>
          <testName>datetime function current_timestamp</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.765</duration>
          <className>org.apache.spark.sql.catalyst.expressions.DateExpressionsSuite</className>
          <testName>DayOfYear</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.973</duration>
          <className>org.apache.spark.sql.catalyst.expressions.DateExpressionsSuite</className>
          <testName>Year</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.428</duration>
          <className>org.apache.spark.sql.catalyst.expressions.DateExpressionsSuite</className>
          <testName>Quarter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.205</duration>
          <className>org.apache.spark.sql.catalyst.expressions.DateExpressionsSuite</className>
          <testName>Month</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>15.23</duration>
          <className>org.apache.spark.sql.catalyst.expressions.DateExpressionsSuite</className>
          <testName>Day / DayOfMonth</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.844</duration>
          <className>org.apache.spark.sql.catalyst.expressions.DateExpressionsSuite</className>
          <testName>Seconds</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.338</duration>
          <className>org.apache.spark.sql.catalyst.expressions.DateExpressionsSuite</className>
          <testName>WeekOfYear</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.139</duration>
          <className>org.apache.spark.sql.catalyst.expressions.DateExpressionsSuite</className>
          <testName>DateFormat</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>14.244</duration>
          <className>org.apache.spark.sql.catalyst.expressions.DateExpressionsSuite</className>
          <testName>Hour</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.44</duration>
          <className>org.apache.spark.sql.catalyst.expressions.DateExpressionsSuite</className>
          <testName>Minute</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.881</duration>
          <className>org.apache.spark.sql.catalyst.expressions.DateExpressionsSuite</className>
          <testName>date_add</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.793</duration>
          <className>org.apache.spark.sql.catalyst.expressions.DateExpressionsSuite</className>
          <testName>date_sub</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.022</duration>
          <className>org.apache.spark.sql.catalyst.expressions.DateExpressionsSuite</className>
          <testName>time_add</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.988</duration>
          <className>org.apache.spark.sql.catalyst.expressions.DateExpressionsSuite</className>
          <testName>time_sub</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.739</duration>
          <className>org.apache.spark.sql.catalyst.expressions.DateExpressionsSuite</className>
          <testName>add_months</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.458</duration>
          <className>org.apache.spark.sql.catalyst.expressions.DateExpressionsSuite</className>
          <testName>months_between</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.988</duration>
          <className>org.apache.spark.sql.catalyst.expressions.DateExpressionsSuite</className>
          <testName>last_day</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.278</duration>
          <className>org.apache.spark.sql.catalyst.expressions.DateExpressionsSuite</className>
          <testName>next_day</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.417</duration>
          <className>org.apache.spark.sql.catalyst.expressions.DateExpressionsSuite</className>
          <testName>function to_date</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.292</duration>
          <className>org.apache.spark.sql.catalyst.expressions.DateExpressionsSuite</className>
          <testName>function trunc</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.161</duration>
          <className>org.apache.spark.sql.catalyst.expressions.DateExpressionsSuite</className>
          <testName>from_unixtime</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.171</duration>
          <className>org.apache.spark.sql.catalyst.expressions.DateExpressionsSuite</className>
          <testName>unix_timestamp</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.catalyst.expressions.DateExpressionsSuite</className>
          <testName>to_unix_timestamp</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.116</duration>
          <className>org.apache.spark.sql.catalyst.expressions.DateExpressionsSuite</className>
          <testName>datediff</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.28</duration>
          <className>org.apache.spark.sql.catalyst.expressions.DateExpressionsSuite</className>
          <testName>to_utc_timestamp</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.301</duration>
          <className>org.apache.spark.sql.catalyst.expressions.DateExpressionsSuite</className>
          <testName>from_utc_timestamp</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.DecimalExpressionSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.DecimalExpressionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.135</duration>
      <cases>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.sql.catalyst.expressions.DecimalExpressionSuite</className>
          <testName>UnscaledValue</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.catalyst.expressions.DecimalExpressionSuite</className>
          <testName>MakeDecimal</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.sql.catalyst.expressions.DecimalExpressionSuite</className>
          <testName>PromotePrecision</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.071</duration>
          <className>org.apache.spark.sql.catalyst.expressions.DecimalExpressionSuite</className>
          <testName>CheckOverflow</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.ExpressionEvalHelperSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.ExpressionEvalHelperSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.023</duration>
      <cases>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ExpressionEvalHelperSuite</className>
          <testName>SPARK-16489 checkEvaluation should fail if expression reuses variable names</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.ExpressionSetSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.ExpressionSetSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.019000001</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ExpressionSetSuite</className>
          <testName>expect 1: (A#1 + 1), (a#1 + 1)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ExpressionSetSuite</className>
          <testName>expect 2: (A#1 + 1), (a#1 + 2)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ExpressionSetSuite</className>
          <testName>expect 2: (A#1 + 1), (a#3 + 1)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ExpressionSetSuite</className>
          <testName>expect 2: (A#1 + 1), (B#2 + 1)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ExpressionSetSuite</className>
          <testName>expect 1: (A#1 + a#1), (a#1 + A#1)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ExpressionSetSuite</className>
          <testName>expect 1: (A#1 + B#2), (B#2 + A#1)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ExpressionSetSuite</className>
          <testName>expect 1: ((A#1 + B#2) + 3), ((B#2 + 3) + A#1), ((B#2 + A#1) + 3), ((3 + A#1) + B#2)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ExpressionSetSuite</className>
          <testName>expect 1: ((A#1 * B#2) * 3), ((B#2 * 3) * A#1), ((B#2 * A#1) * 3), ((3 * A#1) * B#2)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ExpressionSetSuite</className>
          <testName>expect 1: (A#1 = B#2), (B#2 = A#1)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ExpressionSetSuite</className>
          <testName>expect 1: ((A#1 + 1) = B#2), (B#2 = (1 + A#1))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ExpressionSetSuite</className>
          <testName>expect 2: (A#1 - B#2), (B#2 - A#1)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ExpressionSetSuite</className>
          <testName>expect 1: (A#1 &gt; B#2), (B#2 &lt; A#1)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ExpressionSetSuite</className>
          <testName>expect 1: (A#1 &gt;= B#2), (B#2 &lt;= A#1)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ExpressionSetSuite</className>
          <testName>expect 1: NOT (A#1 &gt; 1), (A#1 &lt;= 1), NOT (1 &lt; A#1), (1 &gt;= A#1)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ExpressionSetSuite</className>
          <testName>expect 1: NOT (A#1 &lt; 1), (A#1 &gt;= 1), NOT (1 &gt; A#1), (1 &lt;= A#1)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ExpressionSetSuite</className>
          <testName>expect 1: NOT (A#1 &gt;= 1), (A#1 &lt; 1), NOT (1 &lt;= A#1), (1 &gt; A#1)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ExpressionSetSuite</className>
          <testName>expect 1: NOT (A#1 &lt;= 1), (A#1 &gt; 1), NOT (1 &gt;= A#1), (1 &lt; A#1)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ExpressionSetSuite</className>
          <testName>expect 1: ((A#1 &gt; B#2) &amp;&amp; (A#1 &lt;= 10)), ((A#1 &lt;= 10) &amp;&amp; (A#1 &gt; B#2))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ExpressionSetSuite</className>
          <testName>expect 1: (((A#1 &gt; B#2) &amp;&amp; (B#2 &gt; 100)) &amp;&amp; (A#1 &lt;= 10)), (((B#2 &gt; 100) &amp;&amp; (A#1 &lt;= 10)) &amp;&amp; (A#1 &gt; B#2))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ExpressionSetSuite</className>
          <testName>expect 1: ((A#1 &gt; B#2) || (A#1 &lt;= 10)), ((A#1 &lt;= 10) || (A#1 &gt; B#2))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ExpressionSetSuite</className>
          <testName>expect 1: (((A#1 &gt; B#2) || (B#2 &gt; 100)) || (A#1 &lt;= 10)), (((B#2 &gt; 100) || (A#1 &lt;= 10)) || (A#1 &gt; B#2))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ExpressionSetSuite</className>
          <testName>expect 1: (((A#1 &lt;= 10) &amp;&amp; (A#1 &gt; B#2)) || (B#2 &gt; 100)), ((B#2 &gt; 100) || ((A#1 &lt;= 10) &amp;&amp; (A#1 &gt; B#2)))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ExpressionSetSuite</className>
          <testName>expect 1: ((A#1 &gt;= B#2) || ((A#1 &gt; 10) &amp;&amp; (B#2 &lt; 10))), (((B#2 &lt; 10) &amp;&amp; (A#1 &gt; 10)) || (A#1 &gt;= B#2))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ExpressionSetSuite</className>
          <testName>expect 1: (((B#2 &gt; 100) || ((A#1 &lt; 100) &amp;&amp; (B#2 &lt;= A#1))) || ((A#1 &gt;= 10) &amp;&amp; (B#2 &gt;= 50))), ((((A#1 &gt;= 10) &amp;&amp; (B#2 &gt;= 50)) || (B#2 &gt; 100)) || ((A#1 &lt; 100) &amp;&amp; (B#2 &lt;= A#1))), ((((B#2 &gt;= 50) &amp;&amp; (A#1 &gt;= 10)) || ((B#2 &lt;= A#1) &amp;&amp; (A#1 &lt; 100))) || (B#2 &gt; 100))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ExpressionSetSuite</className>
          <testName>expect 1: ((((B#2 &gt; 100) &amp;&amp; (A#1 &lt; 100)) &amp;&amp; (B#2 &lt;= A#1)) || ((A#1 &gt;= 10) &amp;&amp; (B#2 &gt;= 50))), (((A#1 &gt;= 10) &amp;&amp; (B#2 &gt;= 50)) || (((A#1 &lt; 100) &amp;&amp; (B#2 &gt; 100)) &amp;&amp; (B#2 &lt;= A#1))), (((B#2 &gt;= 50) &amp;&amp; (A#1 &gt;= 10)) || (((B#2 &lt;= A#1) &amp;&amp; (A#1 &lt; 100)) &amp;&amp; (B#2 &gt; 100)))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ExpressionSetSuite</className>
          <testName>expect 1: (((A#1 &gt;= 10) || (((B#2 &lt;= 10) &amp;&amp; (A#1 = B#2)) &amp;&amp; (A#1 &lt; 100))) || (B#2 &gt;= 100)), (((((A#1 = B#2) &amp;&amp; (A#1 &lt; 100)) &amp;&amp; (B#2 &lt;= 10)) || (B#2 &gt;= 100)) || (A#1 &gt;= 10)), (((((A#1 &lt; 100) &amp;&amp; (B#2 &lt;= 10)) &amp;&amp; (A#1 = B#2)) || (A#1 &gt;= 10)) || (B#2 &gt;= 100)), ((((B#2 &lt;= 10) &amp;&amp; (A#1 = B#2)) &amp;&amp; (A#1 &lt; 100)) || ((A#1 &gt;= 10) || (B#2 &gt;= 100)))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ExpressionSetSuite</className>
          <testName>expect 2: ((rand(1) &gt; A#1) &amp;&amp; (A#1 &lt;= 10)), ((A#1 &lt;= 10) &amp;&amp; (rand(1) &gt; A#1))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ExpressionSetSuite</className>
          <testName>expect 2: (((A#1 &gt; B#2) &amp;&amp; (B#2 &gt; 100)) &amp;&amp; (rand(1) &gt; A#1)), (((B#2 &gt; 100) &amp;&amp; (rand(1) &gt; A#1)) &amp;&amp; (A#1 &gt; B#2))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ExpressionSetSuite</className>
          <testName>expect 2: ((rand(1) &gt; A#1) || (A#1 &lt;= 10)), ((A#1 &lt;= 10) || (rand(1) &gt; A#1))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ExpressionSetSuite</className>
          <testName>expect 2: (((A#1 &gt; B#2) || (A#1 &lt;= rand(1))) || (A#1 &lt;= 10)), (((A#1 &lt;= rand(1)) || (A#1 &lt;= 10)) || (A#1 &gt; B#2))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ExpressionSetSuite</className>
          <testName>expect 1: (((A#1 &gt; B#2) || (B#2 &gt; 100)) &amp;&amp; (A#1 = rand(1))), (((B#2 &gt; 100) || (A#1 &gt; B#2)) &amp;&amp; (A#1 = rand(1)))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ExpressionSetSuite</className>
          <testName>expect 1: (((rand(1) &gt; A#1) || ((A#1 &lt;= rand(1)) &amp;&amp; (A#1 &gt; B#2))) || ((A#1 &gt; 10) &amp;&amp; (B#2 &gt; 10))), (((rand(1) &gt; A#1) || ((A#1 &lt;= rand(1)) &amp;&amp; (A#1 &gt; B#2))) || ((B#2 &gt; 10) &amp;&amp; (A#1 &gt; 10)))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ExpressionSetSuite</className>
          <testName>expect 2: (((rand(1) &gt; A#1) || ((A#1 &lt;= rand(1)) &amp;&amp; (A#1 &gt; B#2))) || ((A#1 &gt; 10) &amp;&amp; (B#2 &gt; 10))), (((rand(1) &gt; A#1) || ((A#1 &gt; B#2) &amp;&amp; (A#1 &lt;= rand(1)))) || ((A#1 &gt; 10) &amp;&amp; (B#2 &gt; 10)))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ExpressionSetSuite</className>
          <testName>add to / remove from set</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.GeneratorExpressionSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.GeneratorExpressionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.009</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.GeneratorExpressionSuite</className>
          <testName>explode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.expressions.GeneratorExpressionSuite</className>
          <testName>posexplode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.expressions.GeneratorExpressionSuite</className>
          <testName>inline</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.expressions.GeneratorExpressionSuite</className>
          <testName>stack</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.HashExpressionsSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.HashExpressionsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>32.652</duration>
      <cases>
        <case>
          <duration>0.135</duration>
          <className>org.apache.spark.sql.catalyst.expressions.HashExpressionsSuite</className>
          <testName>md5</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.145</duration>
          <className>org.apache.spark.sql.catalyst.expressions.HashExpressionsSuite</className>
          <testName>sha1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.307</duration>
          <className>org.apache.spark.sql.catalyst.expressions.HashExpressionsSuite</className>
          <testName>sha2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.104</duration>
          <className>org.apache.spark.sql.catalyst.expressions.HashExpressionsSuite</className>
          <testName>crc32</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.333</duration>
          <className>org.apache.spark.sql.catalyst.expressions.HashExpressionsSuite</className>
          <testName>murmur3/xxHash64/hive hash: struct&lt;null:null,boolean:boolean,byte:tinyint,short:smallint,int:int,long:bigint,float:float,double:double,bigDecimal:decimal(38,18),smallDecimal:decimal(10,0),string:string,binary:binary,date:date,timestamp:timestamp,udt:examplepoint&gt;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>14.362</duration>
          <className>org.apache.spark.sql.catalyst.expressions.HashExpressionsSuite</className>
          <testName>murmur3/xxHash64/hive hash: struct&lt;arrayOfNull:array&lt;null&gt;,arrayOfString:array&lt;string&gt;,arrayOfArrayOfString:array&lt;array&lt;string&gt;&gt;,arrayOfArrayOfInt:array&lt;array&lt;int&gt;&gt;,arrayOfMap:array&lt;map&lt;string,string&gt;&gt;,arrayOfStruct:array&lt;struct&lt;str:string&gt;&gt;,arrayOfUDT:array&lt;examplepoint&gt;&gt;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>12.404</duration>
          <className>org.apache.spark.sql.catalyst.expressions.HashExpressionsSuite</className>
          <testName>murmur3/xxHash64/hive hash: struct&lt;mapOfIntAndString:map&lt;int,string&gt;,mapOfStringAndArray:map&lt;string,array&lt;string&gt;&gt;,mapOfArrayAndInt:map&lt;array&lt;string&gt;,int&gt;,mapOfArray:map&lt;array&lt;string&gt;,array&lt;string&gt;&gt;,mapOfStringAndStruct:map&lt;string,struct&lt;str:string&gt;&gt;,mapOfStructAndString:map&lt;struct&lt;str:string&gt;,string&gt;,mapOfStruct:map&lt;struct&lt;str:string&gt;,struct&lt;str:string&gt;&gt;&gt;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.526</duration>
          <className>org.apache.spark.sql.catalyst.expressions.HashExpressionsSuite</className>
          <testName>murmur3/xxHash64/hive hash: struct&lt;structOfString:struct&lt;str:string&gt;,structOfStructOfString:struct&lt;struct:struct&lt;str:string&gt;&gt;,structOfArray:struct&lt;array:array&lt;string&gt;&gt;,structOfMap:struct&lt;map:map&lt;string,string&gt;&gt;,structOfArrayAndMap:struct&lt;array:array&lt;string&gt;,map:map&lt;string,string&gt;&gt;,structOfUDT:struct&lt;udt:examplepoint&gt;&gt;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.336</duration>
          <className>org.apache.spark.sql.catalyst.expressions.HashExpressionsSuite</className>
          <testName>SPARK-18207: Compute hash for a lot of expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.HiveHasherSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.HiveHasherSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.332</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.expressions.HiveHasherSuite</className>
          <testName>testKnownStringAndIntInputs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.HiveHasherSuite</className>
          <testName>testKnownLongInputs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.HiveHasherSuite</className>
          <testName>testKnownIntegerInputs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.sql.catalyst.expressions.HiveHasherSuite</className>
          <testName>randomizedStressTest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.052</duration>
          <className>org.apache.spark.sql.catalyst.expressions.HiveHasherSuite</className>
          <testName>randomizedStressTestPaddedStrings</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.247</duration>
          <className>org.apache.spark.sql.catalyst.expressions.HiveHasherSuite</className>
          <testName>randomizedStressTestBytes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.28599998</duration>
      <cases>
        <case>
          <duration>0.062</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>bicycle</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>book</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>book[0]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>book[*]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>$</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>category</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>category</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>isbn</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>reader</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>basket[0][1]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>basket[*]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>basket[*][0]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>basket[0][*]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>basket[*][*]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>b</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>b</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>zip code</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>fb:testid</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>preserve newlines</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>escape</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>non_exist_key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>no_recursive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>book[10]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>non_exist_key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>non_exist_key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>non foldable literal</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>json_tuple - hive key 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>json_tuple - hive key 2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>json_tuple - hive key 2 (mix of foldable fields)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>json_tuple - hive key 3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>json_tuple - hive key 3 (nonfoldable json)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>json_tuple - hive key 3 (nonfoldable fields)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>json_tuple - hive key 4 - null json</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>json_tuple - hive key 5 - null and empty fields</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>json_tuple - hive key 6 - invalid json (array)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>json_tuple - invalid json (object start only)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>json_tuple - invalid json (no object end)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>json_tuple - invalid json (invalid json)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>json_tuple - preserve newlines</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.056</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>from_json</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>from_json - invalid data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>from_json null input column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>to_json</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.expressions.JsonExpressionsSuite</className>
          <testName>to_json null input column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.LiteralExpressionSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.LiteralExpressionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.963</duration>
      <cases>
        <case>
          <duration>0.191</duration>
          <className>org.apache.spark.sql.catalyst.expressions.LiteralExpressionSuite</className>
          <testName>null</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.268</duration>
          <className>org.apache.spark.sql.catalyst.expressions.LiteralExpressionSuite</className>
          <testName>default</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.sql.catalyst.expressions.LiteralExpressionSuite</className>
          <testName>boolean literals</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.157</duration>
          <className>org.apache.spark.sql.catalyst.expressions.LiteralExpressionSuite</className>
          <testName>int literals</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.088</duration>
          <className>org.apache.spark.sql.catalyst.expressions.LiteralExpressionSuite</className>
          <testName>double literals</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.sql.catalyst.expressions.LiteralExpressionSuite</className>
          <testName>string literals</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.sql.catalyst.expressions.LiteralExpressionSuite</className>
          <testName>sum two literals</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.catalyst.expressions.LiteralExpressionSuite</className>
          <testName>binary literals</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.162</duration>
          <className>org.apache.spark.sql.catalyst.expressions.LiteralExpressionSuite</className>
          <testName>decimal</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.058</duration>
          <className>org.apache.spark.sql.catalyst.expressions.LiteralExpressionSuite</className>
          <testName>array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.LiteralExpressionSuite</className>
          <testName>unsupported types (map and struct) in literals</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.MapDataSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.MapDataSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.055</duration>
      <cases>
        <case>
          <duration>0.055</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MapDataSuite</className>
          <testName>inequality tests</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>78.69401</duration>
      <cases>
        <case>
          <duration>0.27</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>conv</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.041</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>e</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>pi</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.558</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>sin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.195</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>asin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.497</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>sinh</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.448</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>cos</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.346</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>acos</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.805</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>cosh</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.783</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>tan</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.874</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>atan</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.68</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>tanh</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.704</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>toDegrees</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.57</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>toRadians</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.896</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>cbrt</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.244</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>ceil</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.317</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>floor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.078</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>factorial</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.132</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>rint</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.152</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>exp</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.351</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>expm1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.317</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>signum</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.476</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>log</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.794</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>log10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.028</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>log1p</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.997</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>bin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.778</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>log2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.738</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>sqrt</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.966</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>pow</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.047</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>shift left</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.14</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>shift right</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.782</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>shift right unsigned</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.107</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>hex</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.171</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>unhex</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.392</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>hypot</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.038</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>atan2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.855</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>binary log</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>6.122</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MathExpressionsSuite</className>
          <testName>round/bround</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.MiscExpressionsSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.MiscExpressionsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.035</duration>
      <cases>
        <case>
          <duration>0.035</duration>
          <className>org.apache.spark.sql.catalyst.expressions.MiscExpressionsSuite</className>
          <testName>assert_true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.NondeterministicSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.NondeterministicSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.045</duration>
      <cases>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.catalyst.expressions.NondeterministicSuite</className>
          <testName>MonotonicallyIncreasingID</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.sql.catalyst.expressions.NondeterministicSuite</className>
          <testName>SparkPartitionID</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.catalyst.expressions.NondeterministicSuite</className>
          <testName>InputFileName</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.NullExpressionsSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.NullExpressionsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.711</duration>
      <cases>
        <case>
          <duration>0.408</duration>
          <className>org.apache.spark.sql.catalyst.expressions.NullExpressionsSuite</className>
          <testName>isnull and isnotnull</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.expressions.NullExpressionsSuite</className>
          <testName>AssertNotNUll</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.106</duration>
          <className>org.apache.spark.sql.catalyst.expressions.NullExpressionsSuite</className>
          <testName>IsNaN</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.118</duration>
          <className>org.apache.spark.sql.catalyst.expressions.NullExpressionsSuite</className>
          <testName>nanvl</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.892</duration>
          <className>org.apache.spark.sql.catalyst.expressions.NullExpressionsSuite</className>
          <testName>coalesce</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.expressions.NullExpressionsSuite</className>
          <testName>SPARK-16602 Nvl should support numeric-string cases</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.178</duration>
          <className>org.apache.spark.sql.catalyst.expressions.NullExpressionsSuite</className>
          <testName>AtLeastNNonNulls</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.ObjectExpressionsSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.ObjectExpressionsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.117</duration>
      <cases>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ObjectExpressionsSuite</className>
          <testName>SPARK-16622: The returned value of the called method in Invoke can be null</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.11</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ObjectExpressionsSuite</className>
          <testName>MapObjects should make copies of unsafe-backed data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.OrderingSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.OrderingSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.292</duration>
      <cases>
        <case>
          <duration>0.231</duration>
          <className>org.apache.spark.sql.catalyst.expressions.OrderingSuite</className>
          <testName>compare two arrays: a = List(), b = List()</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.035</duration>
          <className>org.apache.spark.sql.catalyst.expressions.OrderingSuite</className>
          <testName>compare two arrays: a = List(1), b = List(1)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.sql.catalyst.expressions.OrderingSuite</className>
          <testName>compare two arrays: a = List(1, 2), b = List(1, 2)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.sql.catalyst.expressions.OrderingSuite</className>
          <testName>compare two arrays: a = List(1, 2, 2), b = List(1, 2, 3)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.032</duration>
          <className>org.apache.spark.sql.catalyst.expressions.OrderingSuite</className>
          <testName>compare two arrays: a = List(), b = List(1)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.sql.catalyst.expressions.OrderingSuite</className>
          <testName>compare two arrays: a = List(1, 2, 3), b = List(1, 2, 3, 4)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.sql.catalyst.expressions.OrderingSuite</className>
          <testName>compare two arrays: a = List(1, 2, 3), b = List(1, 2, 3, 2)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.sql.catalyst.expressions.OrderingSuite</className>
          <testName>compare two arrays: a = List(1, 2, 3), b = List(1, 2, 2, 2)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.sql.catalyst.expressions.OrderingSuite</className>
          <testName>compare two arrays: a = List(1, 2, 3), b = List(1, 2, 3, null)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.sql.catalyst.expressions.OrderingSuite</className>
          <testName>compare two arrays: a = List(), b = List(null)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.sql.catalyst.expressions.OrderingSuite</className>
          <testName>compare two arrays: a = List(null), b = List(null)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.sql.catalyst.expressions.OrderingSuite</className>
          <testName>compare two arrays: a = List(null, null), b = List(null, null)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.sql.catalyst.expressions.OrderingSuite</className>
          <testName>compare two arrays: a = List(null), b = List(null, null)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.sql.catalyst.expressions.OrderingSuite</className>
          <testName>compare two arrays: a = List(null), b = List(1)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.sql.catalyst.expressions.OrderingSuite</className>
          <testName>compare two arrays: a = List(null), b = List(null, 1)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.sql.catalyst.expressions.OrderingSuite</className>
          <testName>compare two arrays: a = List(null, 1), b = List(1, 1)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.catalyst.expressions.OrderingSuite</className>
          <testName>compare two arrays: a = List(1, null, 1), b = List(1, null, 1)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.sql.catalyst.expressions.OrderingSuite</className>
          <testName>compare two arrays: a = List(1, null, 1), b = List(1, null, 2)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.039</duration>
          <className>org.apache.spark.sql.catalyst.expressions.OrderingSuite</className>
          <testName>GenerateOrdering with StringType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.catalyst.expressions.OrderingSuite</className>
          <testName>GenerateOrdering with NullType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.sql.catalyst.expressions.OrderingSuite</className>
          <testName>GenerateOrdering with ArrayType(IntegerType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.catalyst.expressions.OrderingSuite</className>
          <testName>GenerateOrdering with LongType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.catalyst.expressions.OrderingSuite</className>
          <testName>GenerateOrdering with IntegerType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.sql.catalyst.expressions.OrderingSuite</className>
          <testName>GenerateOrdering with DecimalType(20,5)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.expressions.OrderingSuite</className>
          <testName>GenerateOrdering with TimestampType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.sql.catalyst.expressions.OrderingSuite</className>
          <testName>GenerateOrdering with DoubleType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.expressions.OrderingSuite</className>
          <testName>GenerateOrdering with DateType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.039</duration>
          <className>org.apache.spark.sql.catalyst.expressions.OrderingSuite</className>
          <testName>GenerateOrdering with StructType(StructField(f1,FloatType,true), StructField(f2,ArrayType(BooleanType,true),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.346</duration>
          <className>org.apache.spark.sql.catalyst.expressions.OrderingSuite</className>
          <testName>GenerateOrdering with ArrayType(StructType(StructField(f1,FloatType,true), StructField(f2,ArrayType(BooleanType,true),true)),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.sql.catalyst.expressions.OrderingSuite</className>
          <testName>GenerateOrdering with DecimalType(10,0)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.sql.catalyst.expressions.OrderingSuite</className>
          <testName>GenerateOrdering with BinaryType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.catalyst.expressions.OrderingSuite</className>
          <testName>GenerateOrdering with BooleanType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.catalyst.expressions.OrderingSuite</className>
          <testName>GenerateOrdering with DecimalType(38,18)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.expressions.OrderingSuite</className>
          <testName>GenerateOrdering with ByteType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.catalyst.expressions.OrderingSuite</className>
          <testName>GenerateOrdering with FloatType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.expressions.OrderingSuite</className>
          <testName>GenerateOrdering with ShortType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.PredicateSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.PredicateSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>117.03901</duration>
      <cases>
        <case>
          <duration>0.135</duration>
          <className>org.apache.spark.sql.catalyst.expressions.PredicateSuite</className>
          <testName>3VL Not</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>29.468</duration>
          <className>org.apache.spark.sql.catalyst.expressions.PredicateSuite</className>
          <testName>AND, OR, EqualTo, EqualNullSafe consistency check</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.438</duration>
          <className>org.apache.spark.sql.catalyst.expressions.PredicateSuite</className>
          <testName>3VL AND</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.435</duration>
          <className>org.apache.spark.sql.catalyst.expressions.PredicateSuite</className>
          <testName>3VL OR</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.413</duration>
          <className>org.apache.spark.sql.catalyst.expressions.PredicateSuite</className>
          <testName>3VL =</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.372</duration>
          <className>org.apache.spark.sql.catalyst.expressions.PredicateSuite</className>
          <testName>IN</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.017</duration>
          <className>org.apache.spark.sql.catalyst.expressions.PredicateSuite</className>
          <testName>INSET</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>76.026</duration>
          <className>org.apache.spark.sql.catalyst.expressions.PredicateSuite</className>
          <testName>BinaryComparison consistency check</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.135</duration>
          <className>org.apache.spark.sql.catalyst.expressions.PredicateSuite</className>
          <testName>BinaryComparison: lessThan</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.095</duration>
          <className>org.apache.spark.sql.catalyst.expressions.PredicateSuite</className>
          <testName>BinaryComparison: LessThanOrEqual</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.079</duration>
          <className>org.apache.spark.sql.catalyst.expressions.PredicateSuite</className>
          <testName>BinaryComparison: GreaterThan</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.128</duration>
          <className>org.apache.spark.sql.catalyst.expressions.PredicateSuite</className>
          <testName>BinaryComparison: GreaterThanOrEqual</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.147</duration>
          <className>org.apache.spark.sql.catalyst.expressions.PredicateSuite</className>
          <testName>BinaryComparison: EqualTo</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.959</duration>
          <className>org.apache.spark.sql.catalyst.expressions.PredicateSuite</className>
          <testName>BinaryComparison: EqualNullSafe</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.017</duration>
          <className>org.apache.spark.sql.catalyst.expressions.PredicateSuite</className>
          <testName>BinaryComparison: null test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.175</duration>
          <className>org.apache.spark.sql.catalyst.expressions.PredicateSuite</className>
          <testName>EqualTo on complex type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.RandomSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.RandomSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.435</duration>
      <cases>
        <case>
          <duration>0.307</duration>
          <className>org.apache.spark.sql.catalyst.expressions.RandomSuite</className>
          <testName>random</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.128</duration>
          <className>org.apache.spark.sql.catalyst.expressions.RandomSuite</className>
          <testName>SPARK-9127 codegen with long seed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.RegexpExpressionsSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.RegexpExpressionsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>3.92</duration>
      <cases>
        <case>
          <duration>1.427</duration>
          <className>org.apache.spark.sql.catalyst.expressions.RegexpExpressionsSuite</className>
          <testName>LIKE literal Regular Expression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.261</duration>
          <className>org.apache.spark.sql.catalyst.expressions.RegexpExpressionsSuite</className>
          <testName>LIKE Non-literal Regular Expression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.317</duration>
          <className>org.apache.spark.sql.catalyst.expressions.RegexpExpressionsSuite</className>
          <testName>RLIKE literal Regular Expression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.106</duration>
          <className>org.apache.spark.sql.catalyst.expressions.RegexpExpressionsSuite</className>
          <testName>RLIKE Non-literal Regular Expression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.202</duration>
          <className>org.apache.spark.sql.catalyst.expressions.RegexpExpressionsSuite</className>
          <testName>RegexReplace</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.417</duration>
          <className>org.apache.spark.sql.catalyst.expressions.RegexpExpressionsSuite</className>
          <testName>RegexExtract</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.19</duration>
          <className>org.apache.spark.sql.catalyst.expressions.RegexpExpressionsSuite</className>
          <testName>SPLIT</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatchSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatchSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.429</duration>
      <cases>
        <case>
          <duration>0.07</duration>
          <className>org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatchSuite</className>
          <testName>failureToAllocateFirstPage</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.208</duration>
          <className>org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatchSuite</className>
          <testName>setAndRetrieve</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatchSuite</className>
          <testName>emptyBatch</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.053</duration>
          <className>org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatchSuite</className>
          <testName>batchType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatchSuite</className>
          <testName>randomizedTest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatchSuite</className>
          <testName>iteratorTest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatchSuite</className>
          <testName>setUpdateAndRetrieve</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatchSuite</className>
          <testName>fixedLengthTest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatchSuite</className>
          <testName>appendRowUntilExceedingCapacity</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.976</duration>
          <className>org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatchSuite</className>
          <testName>appendRowUntilExceedingPageSize</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.ScalaUDFSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.ScalaUDFSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.076000005</duration>
      <cases>
        <case>
          <duration>0.064</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ScalaUDFSuite</className>
          <testName>basic</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.sql.catalyst.expressions.ScalaUDFSuite</className>
          <testName>better error message for NPE</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.StringExpressionsSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.StringExpressionsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>3.520001</duration>
      <cases>
        <case>
          <duration>0.126</duration>
          <className>org.apache.spark.sql.catalyst.expressions.StringExpressionsSuite</className>
          <testName>concat</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.141</duration>
          <className>org.apache.spark.sql.catalyst.expressions.StringExpressionsSuite</className>
          <testName>concat_ws</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.09</duration>
          <className>org.apache.spark.sql.catalyst.expressions.StringExpressionsSuite</className>
          <testName>elt</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.153</duration>
          <className>org.apache.spark.sql.catalyst.expressions.StringExpressionsSuite</className>
          <testName>StringComparison</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.572</duration>
          <className>org.apache.spark.sql.catalyst.expressions.StringExpressionsSuite</className>
          <testName>Substring</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.162</duration>
          <className>org.apache.spark.sql.catalyst.expressions.StringExpressionsSuite</className>
          <testName>string substring_index function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.062</duration>
          <className>org.apache.spark.sql.catalyst.expressions.StringExpressionsSuite</className>
          <testName>ascii for string</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.214</duration>
          <className>org.apache.spark.sql.catalyst.expressions.StringExpressionsSuite</className>
          <testName>base64/unbase64 for string</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.224</duration>
          <className>org.apache.spark.sql.catalyst.expressions.StringExpressionsSuite</className>
          <testName>encode/decode for string</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.049</duration>
          <className>org.apache.spark.sql.catalyst.expressions.StringExpressionsSuite</className>
          <testName>initcap unit test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.095</duration>
          <className>org.apache.spark.sql.catalyst.expressions.StringExpressionsSuite</className>
          <testName>Levenshtein distance</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.08</duration>
          <className>org.apache.spark.sql.catalyst.expressions.StringExpressionsSuite</className>
          <testName>soundex unit test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.sql.catalyst.expressions.StringExpressionsSuite</className>
          <testName>translate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.186</duration>
          <className>org.apache.spark.sql.catalyst.expressions.StringExpressionsSuite</className>
          <testName>TRIM/LTRIM/RTRIM</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.153</duration>
          <className>org.apache.spark.sql.catalyst.expressions.StringExpressionsSuite</className>
          <testName>FORMAT</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.091</duration>
          <className>org.apache.spark.sql.catalyst.expressions.StringExpressionsSuite</className>
          <testName>INSTR</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.167</duration>
          <className>org.apache.spark.sql.catalyst.expressions.StringExpressionsSuite</className>
          <testName>LOCATE</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.121</duration>
          <className>org.apache.spark.sql.catalyst.expressions.StringExpressionsSuite</className>
          <testName>LPAD/RPAD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.066</duration>
          <className>org.apache.spark.sql.catalyst.expressions.StringExpressionsSuite</className>
          <testName>REPEAT</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.052</duration>
          <className>org.apache.spark.sql.catalyst.expressions.StringExpressionsSuite</className>
          <testName>REVERSE</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.079</duration>
          <className>org.apache.spark.sql.catalyst.expressions.StringExpressionsSuite</className>
          <testName>SPACE</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.099</duration>
          <className>org.apache.spark.sql.catalyst.expressions.StringExpressionsSuite</className>
          <testName>length for string / binary</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.272</duration>
          <className>org.apache.spark.sql.catalyst.expressions.StringExpressionsSuite</className>
          <testName>format_number / FormatNumber</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.081</duration>
          <className>org.apache.spark.sql.catalyst.expressions.StringExpressionsSuite</className>
          <testName>find in set</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.051</duration>
          <className>org.apache.spark.sql.catalyst.expressions.StringExpressionsSuite</className>
          <testName>ParseUrl</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.091</duration>
          <className>org.apache.spark.sql.catalyst.expressions.StringExpressionsSuite</className>
          <testName>Sentences</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.SubexpressionEliminationSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.SubexpressionEliminationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.033</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.expressions.SubexpressionEliminationSuite</className>
          <testName>Semantic equals and hash</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.sql.catalyst.expressions.SubexpressionEliminationSuite</className>
          <testName>Expression Equivalence - basic</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.sql.catalyst.expressions.SubexpressionEliminationSuite</className>
          <testName>Expression Equivalence - Trees</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.SubexpressionEliminationSuite</className>
          <testName>Expression equivalence - non deterministic</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.expressions.SubexpressionEliminationSuite</className>
          <testName>Children of CodegenFallback</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.TimeWindowSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.TimeWindowSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.028000003</duration>
      <cases>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.expressions.TimeWindowSuite</className>
          <testName>time window is unevaluable</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.expressions.TimeWindowSuite</className>
          <testName>blank intervals throw exception</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.TimeWindowSuite</className>
          <testName>invalid intervals throw exception</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.TimeWindowSuite</className>
          <testName>intervals greater than a month throws exception</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.expressions.TimeWindowSuite</className>
          <testName>interval strings work with and without &apos;interval&apos; prefix and return microseconds</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.expressions.TimeWindowSuite</className>
          <testName>parse sql expression for duration in microseconds - string</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.TimeWindowSuite</className>
          <testName>parse sql expression for duration in microseconds - integer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.TimeWindowSuite</className>
          <testName>parse sql expression for duration in microseconds - long</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.TimeWindowSuite</className>
          <testName>parse sql expression for duration in microseconds - invalid interval</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.expressions.TimeWindowSuite</className>
          <testName>parse sql expression for duration in microseconds - invalid expression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.expressions.TimeWindowSuite</className>
          <testName>apply equivalent to TimeWindow constructor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.UnsafeRowConverterSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.UnsafeRowConverterSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.091</duration>
      <cases>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.catalyst.expressions.UnsafeRowConverterSuite</className>
          <testName>basic conversion with only primitive types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.catalyst.expressions.UnsafeRowConverterSuite</className>
          <testName>basic conversion with primitive, string and binary types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.expressions.UnsafeRowConverterSuite</className>
          <testName>basic conversion with primitive, string, date and timestamp types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.catalyst.expressions.UnsafeRowConverterSuite</className>
          <testName>null handling</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.expressions.UnsafeRowConverterSuite</className>
          <testName>NaN canonicalization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.expressions.UnsafeRowConverterSuite</className>
          <testName>basic conversion with struct type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.expressions.UnsafeRowConverterSuite</className>
          <testName>basic conversion with array type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.sql.catalyst.expressions.UnsafeRowConverterSuite</className>
          <testName>basic conversion with map type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.expressions.UnsafeRowConverterSuite</className>
          <testName>basic conversion with struct and array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.catalyst.expressions.UnsafeRowConverterSuite</className>
          <testName>basic conversion with struct and map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.sql.catalyst.expressions.UnsafeRowConverterSuite</className>
          <testName>basic conversion with array and map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.XXH64Suite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.XXH64Suite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.259</duration>
      <cases>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.expressions.XXH64Suite</className>
          <testName>testKnownLongInputs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.XXH64Suite</className>
          <testName>testKnownIntegerInputs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.049</duration>
          <className>org.apache.spark.sql.catalyst.expressions.XXH64Suite</className>
          <testName>randomizedStressTest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.expressions.XXH64Suite</className>
          <testName>testKnownByteArrayInputs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.052</duration>
          <className>org.apache.spark.sql.catalyst.expressions.XXH64Suite</className>
          <testName>randomizedStressTestPaddedStrings</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.149</duration>
          <className>org.apache.spark.sql.catalyst.expressions.XXH64Suite</className>
          <testName>randomizedStressTestBytes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentileSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentileSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>3.9799998</duration>
      <cases>
        <case>
          <duration>0.046</duration>
          <className>org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentileSuite</className>
          <testName>serialize and de-serialize</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.363</duration>
          <className>org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentileSuite</className>
          <testName>class PercentileDigest, basic operations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.488</duration>
          <className>org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentileSuite</className>
          <testName>class PercentileDigest, makes sure the memory foot print is bounded</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.041</duration>
          <className>org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentileSuite</className>
          <testName>class ApproximatePercentile, high level interface, update, merge, eval</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentileSuite</className>
          <testName>class ApproximatePercentile, low level interface, update, merge, eval</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentileSuite</className>
          <testName>class ApproximatePercentile, sql string</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentileSuite</className>
          <testName>class ApproximatePercentile, fails analysis if percentage or accuracy is not a constant</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentileSuite</className>
          <testName>class ApproximatePercentile, fails analysis if parameters are invalid</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentileSuite</className>
          <testName>class ApproximatePercentile, automatically add type casting for parameters</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentileSuite</className>
          <testName>class ApproximatePercentile, null handling</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.aggregate.HyperLogLogPlusPlusSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.aggregate.HyperLogLogPlusPlusSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>4.487</duration>
      <cases>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.sql.catalyst.expressions.aggregate.HyperLogLogPlusPlusSuite</className>
          <testName>add nulls</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.936</duration>
          <className>org.apache.spark.sql.catalyst.expressions.aggregate.HyperLogLogPlusPlusSuite</className>
          <testName>deterministic cardinality estimation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.417</duration>
          <className>org.apache.spark.sql.catalyst.expressions.aggregate.HyperLogLogPlusPlusSuite</className>
          <testName>random cardinality estimation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.115</duration>
          <className>org.apache.spark.sql.catalyst.expressions.aggregate.HyperLogLogPlusPlusSuite</className>
          <testName>merging HLL instances</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.aggregate.LastTestSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.aggregate.LastTestSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.021</duration>
      <cases>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.expressions.aggregate.LastTestSuite</className>
          <testName>empty buffer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.expressions.aggregate.LastTestSuite</className>
          <testName>update</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.expressions.aggregate.LastTestSuite</className>
          <testName>update - ignore nulls</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.expressions.aggregate.LastTestSuite</className>
          <testName>merge</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.expressions.aggregate.LastTestSuite</className>
          <testName>merge - ignore nulls</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.expressions.aggregate.LastTestSuite</className>
          <testName>eval</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.expressions.aggregate.LastTestSuite</className>
          <testName>eval - ignore nulls</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.codegen.BufferHolderSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.codegen.BufferHolderSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.001</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.BufferHolderSuite</className>
          <testName>SPARK-16071 Check the size limit to avoid integer overflow</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.codegen.CodeFormatterSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.codegen.CodeFormatterSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.013</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.CodeFormatterSuite</className>
          <testName>removing overlapping comments</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.CodeFormatterSuite</className>
          <testName>basic example</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.CodeFormatterSuite</className>
          <testName>nested example</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.CodeFormatterSuite</className>
          <testName>single line</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.CodeFormatterSuite</className>
          <testName>if else on the same line</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.CodeFormatterSuite</className>
          <testName>function calls</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.CodeFormatterSuite</className>
          <testName>single line comments</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.CodeFormatterSuite</className>
          <testName>single line comments /* */</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.CodeFormatterSuite</className>
          <testName>multi-line comments</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.CodeFormatterSuite</className>
          <testName>reduce empty lines</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.CodeFormatterSuite</className>
          <testName>comment place holder</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.codegen.CodegenExpressionCachingSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.codegen.CodegenExpressionCachingSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.031</duration>
      <cases>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.CodegenExpressionCachingSuite</className>
          <testName>GenerateUnsafeProjection should initialize expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.CodegenExpressionCachingSuite</className>
          <testName>GenerateMutableProjection should initialize expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.CodegenExpressionCachingSuite</className>
          <testName>GeneratePredicate should initialize expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.CodegenExpressionCachingSuite</className>
          <testName>GenerateUnsafeProjection should not share expression instances</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.CodegenExpressionCachingSuite</className>
          <testName>GenerateMutableProjection should not share expression instances</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.CodegenExpressionCachingSuite</className>
          <testName>GeneratePredicate should not share expression instances</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeRowJoinerBitsetSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeRowJoinerBitsetSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>3.791</duration>
      <cases>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeRowJoinerBitsetSuite</className>
          <testName>bitset concat: boundary size 0, 0</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeRowJoinerBitsetSuite</className>
          <testName>bitset concat: boundary size 0, 64</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.018</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeRowJoinerBitsetSuite</className>
          <testName>bitset concat: boundary size 64, 0</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeRowJoinerBitsetSuite</className>
          <testName>bitset concat: boundary size 64, 64</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeRowJoinerBitsetSuite</className>
          <testName>bitset concat: boundary size 0, 128</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeRowJoinerBitsetSuite</className>
          <testName>bitset concat: boundary size 128, 0</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.1</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeRowJoinerBitsetSuite</className>
          <testName>bitset concat: boundary size 128, 128</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeRowJoinerBitsetSuite</className>
          <testName>bitset concat: single word bitsets</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeRowJoinerBitsetSuite</className>
          <testName>bitset concat: first bitset larger than a word</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeRowJoinerBitsetSuite</className>
          <testName>bitset concat: second bitset larger than a word</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeRowJoinerBitsetSuite</className>
          <testName>bitset concat: no reduction in bitset size</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.044</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeRowJoinerBitsetSuite</className>
          <testName>bitset concat: two words</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.038</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeRowJoinerBitsetSuite</className>
          <testName>bitset concat: bitset 65, 128</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.412</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeRowJoinerBitsetSuite</className>
          <testName>bitset concat: randomized tests</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeRowJoinerSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeRowJoinerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.301</duration>
      <cases>
        <case>
          <duration>0.197</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeRowJoinerSuite</className>
          <testName>simple fixed width types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.604</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeRowJoinerSuite</className>
          <testName>randomized fix width types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.11</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeRowJoinerSuite</className>
          <testName>simple variable width types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.39</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeRowJoinerSuite</className>
          <testName>randomized variable width types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.codegen.GeneratedProjectionSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.codegen.GeneratedProjectionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.0399997</duration>
      <cases>
        <case>
          <duration>2.021</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.GeneratedProjectionSuite</className>
          <testName>generated projections on wider table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.GeneratedProjectionSuite</className>
          <testName>generated unsafe projection with array of binary</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.catalyst.expressions.codegen.GeneratedProjectionSuite</className>
          <testName>padding bytes should be zeroed out</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.xml.ReusableStringReaderSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.xml.ReusableStringReaderSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.002</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.xml.ReusableStringReaderSuite</className>
          <testName>empty reader</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.expressions.xml.ReusableStringReaderSuite</className>
          <testName>mark reset</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.expressions.xml.ReusableStringReaderSuite</className>
          <testName>skip</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.xml.UDFXPathUtilSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.xml.UDFXPathUtilSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.031999998</duration>
      <cases>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.expressions.xml.UDFXPathUtilSuite</className>
          <testName>illegal arguments</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.expressions.xml.UDFXPathUtilSuite</className>
          <testName>generic eval</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.expressions.xml.UDFXPathUtilSuite</className>
          <testName>boolean eval</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.expressions.xml.UDFXPathUtilSuite</className>
          <testName>string eval</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.expressions.xml.UDFXPathUtilSuite</className>
          <testName>number eval</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.sql.catalyst.expressions.xml.UDFXPathUtilSuite</className>
          <testName>node eval</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.expressions.xml.UDFXPathUtilSuite</className>
          <testName>node list eval</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.expressions.xml.XPathExpressionSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.expressions.xml.XPathExpressionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.453</duration>
      <cases>
        <case>
          <duration>0.6</duration>
          <className>org.apache.spark.sql.catalyst.expressions.xml.XPathExpressionSuite</className>
          <testName>xpath_boolean</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.115</duration>
          <className>org.apache.spark.sql.catalyst.expressions.xml.XPathExpressionSuite</className>
          <testName>xpath_short</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.103</duration>
          <className>org.apache.spark.sql.catalyst.expressions.xml.XPathExpressionSuite</className>
          <testName>xpath_int</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.095</duration>
          <className>org.apache.spark.sql.catalyst.expressions.xml.XPathExpressionSuite</className>
          <testName>xpath_long</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.097</duration>
          <className>org.apache.spark.sql.catalyst.expressions.xml.XPathExpressionSuite</className>
          <testName>xpath_float</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.145</duration>
          <className>org.apache.spark.sql.catalyst.expressions.xml.XPathExpressionSuite</className>
          <testName>xpath_double</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.132</duration>
          <className>org.apache.spark.sql.catalyst.expressions.xml.XPathExpressionSuite</className>
          <testName>xpath_string</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.155</duration>
          <className>org.apache.spark.sql.catalyst.expressions.xml.XPathExpressionSuite</className>
          <testName>xpath</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.catalyst.expressions.xml.XPathExpressionSuite</className>
          <testName>accept only literal path</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.optimizer.AggregateOptimizeSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.optimizer.AggregateOptimizeSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.014</duration>
      <cases>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.AggregateOptimizeSuite</className>
          <testName>remove literals in grouping expression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.AggregateOptimizeSuite</className>
          <testName>do not remove all grouping expressions if they are all literals</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.AggregateOptimizeSuite</className>
          <testName>Remove aliased literals</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.AggregateOptimizeSuite</className>
          <testName>remove repetition in grouping expression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.optimizer.BinaryComparisonSimplificationSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.optimizer.BinaryComparisonSimplificationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.034</duration>
      <cases>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.BinaryComparisonSimplificationSuite</className>
          <testName>Preserve nullable exprs in general</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.BinaryComparisonSimplificationSuite</className>
          <testName>Preserve non-deterministic exprs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.BinaryComparisonSimplificationSuite</className>
          <testName>Nullable Simplification Primitive: &lt;=&gt;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.BinaryComparisonSimplificationSuite</className>
          <testName>Non-Nullable Simplification Primitive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.BinaryComparisonSimplificationSuite</className>
          <testName>Expression Normalization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.optimizer.BooleanSimplificationSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.optimizer.BooleanSimplificationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.292</duration>
      <cases>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.BooleanSimplificationSuite</className>
          <testName>a &amp;&amp; a =&gt; a</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.BooleanSimplificationSuite</className>
          <testName>a || a =&gt; a</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.07</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.BooleanSimplificationSuite</className>
          <testName>) </testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.BooleanSimplificationSuite</className>
          <testName>) </testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.BooleanSimplificationSuite</className>
          <testName>a &amp;&amp; (!a || b)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.049</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.BooleanSimplificationSuite</className>
          <testName>a &lt; 1 &amp;&amp; (!(a &lt; 1) || b)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.BooleanSimplificationSuite</className>
          <testName>a &lt; 1 &amp;&amp; ((a &gt;= 1) || b)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.BooleanSimplificationSuite</className>
          <testName>DeMorgan&apos;s law</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.BooleanSimplificationSuite</className>
          <testName>(a &amp;&amp; b) || (a &amp;&amp; c) =&gt; a &amp;&amp; (b || c) when case insensitive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.BooleanSimplificationSuite</className>
          <testName>(a || b) &amp;&amp; (a || c) =&gt; a || (b &amp;&amp; c) when case insensitive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.optimizer.CollapseProjectSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.optimizer.CollapseProjectSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.022000002</duration>
      <cases>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.CollapseProjectSuite</className>
          <testName>collapse two deterministic, independent projects into one</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.CollapseProjectSuite</className>
          <testName>collapse two deterministic, dependent projects into one</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.CollapseProjectSuite</className>
          <testName>do not collapse nondeterministic projects</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.CollapseProjectSuite</className>
          <testName>collapse two nondeterministic, independent projects into one</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.CollapseProjectSuite</className>
          <testName>collapse one nondeterministic, one deterministic, independent projects into one</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.CollapseProjectSuite</className>
          <testName>collapse project into aggregate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.CollapseProjectSuite</className>
          <testName>do not collapse common nondeterministic project and aggregate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.optimizer.CollapseRepartitionSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.optimizer.CollapseRepartitionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.019000001</duration>
      <cases>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.CollapseRepartitionSuite</className>
          <testName>collapse two adjacent repartitions into one</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.CollapseRepartitionSuite</className>
          <testName>collapse repartition and repartitionBy into one</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.CollapseRepartitionSuite</className>
          <testName>collapse repartitionBy and repartition into one</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.CollapseRepartitionSuite</className>
          <testName>collapse two adjacent repartitionBys into one</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.optimizer.CollapseWindowSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.optimizer.CollapseWindowSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.015000001</duration>
      <cases>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.CollapseWindowSuite</className>
          <testName>collapse two adjacent windows with the same partition/order</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.CollapseWindowSuite</className>
          <testName>Don&apos;t collapse adjacent windows with different partitions or orders</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.optimizer.ColumnPruningSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.optimizer.ColumnPruningSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.17799999</duration>
      <cases>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.ColumnPruningSuite</className>
          <testName>join = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.ColumnPruningSuite</className>
          <testName>join = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.ColumnPruningSuite</className>
          <testName>join to false if possible</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.ColumnPruningSuite</className>
          <testName>Column pruning for Project on Sort</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.ColumnPruningSuite</className>
          <testName>Column pruning for Expand</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.ColumnPruningSuite</className>
          <testName>Column pruning on Filter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.ColumnPruningSuite</className>
          <testName>Column pruning on except/intersect/distinct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.ColumnPruningSuite</className>
          <testName>Column pruning on Project</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.ColumnPruningSuite</className>
          <testName>Eliminate the Project with an empty projectList</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.ColumnPruningSuite</className>
          <testName>column pruning for group</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.ColumnPruningSuite</className>
          <testName>column pruning for group with alias</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.ColumnPruningSuite</className>
          <testName>column pruning for Project(ne, Limit)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.ColumnPruningSuite</className>
          <testName>push down project past sort</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.ColumnPruningSuite</className>
          <testName>Column pruning on Window with useless aggregate functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.ColumnPruningSuite</className>
          <testName>Column pruning on Window with selected agg expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.ColumnPruningSuite</className>
          <testName>Column pruning on Window in select</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.ColumnPruningSuite</className>
          <testName>Column pruning on Union</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.ColumnPruningSuite</className>
          <testName>Remove redundant projects in column pruning rule</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.ColumnPruningSuite</className>
          <testName>Column pruning on MapPartitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.ColumnPruningSuite</className>
          <testName>push project down into sample</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.optimizer.CombiningLimitsSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.optimizer.CombiningLimitsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.017</duration>
      <cases>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.CombiningLimitsSuite</className>
          <testName>limits: combines two limits</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.CombiningLimitsSuite</className>
          <testName>limits: combines three limits</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.CombiningLimitsSuite</className>
          <testName>limits: combines two limits after ColumnPruning</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.optimizer.ComputeCurrentTimeSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.optimizer.ComputeCurrentTimeSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.006</duration>
      <cases>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.ComputeCurrentTimeSuite</className>
          <testName>analyzer should replace current_timestamp with literals</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.ComputeCurrentTimeSuite</className>
          <testName>analyzer should replace current_date with literals</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.optimizer.ConstantFoldingSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.optimizer.ConstantFoldingSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.036</duration>
      <cases>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.ConstantFoldingSuite</className>
          <testName>eliminate subqueries</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.ConstantFoldingSuite</className>
          <testName>Constant folding test: expressions only have literals</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.ConstantFoldingSuite</className>
          <testName>Constant folding test: expressions have attribute references and literals in arithmetic operations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.ConstantFoldingSuite</className>
          <testName>Constant folding test: expressions have attribute references and literals in predicates</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.ConstantFoldingSuite</className>
          <testName>Constant folding test: expressions have foldable functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.ConstantFoldingSuite</className>
          <testName>Constant folding test: expressions have nonfoldable functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.ConstantFoldingSuite</className>
          <testName>Constant folding test: expressions have null literals</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.ConstantFoldingSuite</className>
          <testName>Constant folding test: Fold In(v, list) into true or false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelationSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.005</duration>
      <cases>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelationSuite</className>
          <testName>Project on LocalRelation should be turned into a single LocalRelation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.optimizer.DecimalAggregatesSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.optimizer.DecimalAggregatesSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.044000003</duration>
      <cases>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.DecimalAggregatesSuite</className>
          <testName>Decimal Sum Aggregation: Optimized</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.DecimalAggregatesSuite</className>
          <testName>Decimal Sum Aggregation: Not Optimized</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.DecimalAggregatesSuite</className>
          <testName>Decimal Average Aggregation: Optimized</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.DecimalAggregatesSuite</className>
          <testName>Decimal Average Aggregation: Not Optimized</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.DecimalAggregatesSuite</className>
          <testName>Decimal Sum Aggregation over Window: Optimized</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.DecimalAggregatesSuite</className>
          <testName>Decimal Sum Aggregation over Window: Not Optimized</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.DecimalAggregatesSuite</className>
          <testName>Decimal Average Aggregation over Window: Optimized</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.DecimalAggregatesSuite</className>
          <testName>Decimal Average Aggregation over Window: Not Optimized</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.optimizer.EliminateSerializationSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.optimizer.EliminateSerializationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.128</duration>
      <cases>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.EliminateSerializationSuite</className>
          <testName>back to back serialization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.EliminateSerializationSuite</className>
          <testName>back to back serialization with object change</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.EliminateSerializationSuite</className>
          <testName>back to back serialization in AppendColumns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.EliminateSerializationSuite</className>
          <testName>back to back serialization in AppendColumns with object change</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.optimizer.EliminateSortsSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.optimizer.EliminateSortsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.018</duration>
      <cases>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.EliminateSortsSuite</className>
          <testName>Empty order by clause</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.EliminateSortsSuite</className>
          <testName>All the SortOrder are no-op</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.EliminateSortsSuite</className>
          <testName>Partial order-by clauses contain no-op SortOrder</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.EliminateSortsSuite</className>
          <testName>Remove no-op alias</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.optimizer.EliminateSubqueryAliasesSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.optimizer.EliminateSubqueryAliasesSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.0060000005</duration>
      <cases>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.EliminateSubqueryAliasesSuite</className>
          <testName>eliminate top level subquery</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.EliminateSubqueryAliasesSuite</className>
          <testName>eliminate mid-tree subquery</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.EliminateSubqueryAliasesSuite</className>
          <testName>eliminate multiple subqueries</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.677</duration>
      <cases>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>eliminate subqueries</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>simple push down</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>combine redundant filters</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>SPARK-16164: Filter pushdown should keep the ordering in the logical plan</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>SPARK-16994: filter should not be pushed through limit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>can&apos;t push without rewrite</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>nondeterministic: can&apos;t push down filter with nondeterministic condition through project</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>nondeterministic: can&apos;t push down filter through project with nondeterministic field</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>filters: combines filters</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>joins: push to either side</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>joins: push to one side</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>joins: push to one side after transformCondition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>joins: rewrite filter to push to either side</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>joins: push down left semi join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>joins: push down left outer join #1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>joins: push down right outer join #1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>joins: push down left outer join #2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>joins: push down right outer join #2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>joins: push down left outer join #3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>joins: push down right outer join #3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>joins: push down left outer join #4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>joins: push down right outer join #4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>joins: push down left outer join #5</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>joins: push down right outer join #5</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>joins: can&apos;t push down</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>joins: conjunctive predicates</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>joins: conjunctive predicates #2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>joins: conjunctive predicates #3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>generate: predicate referenced no generated column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>generate: non-deterministic predicate referenced no generated column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>generate: part of conjuncts referenced generated column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>generate: all conjuncts referenced generated column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>aggregate: push down filter when filter on group by expression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>aggregate: don&apos;t push down filter when filter not on group by expression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>aggregate: push down filters partially which are subset of group by expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>aggregate: push down filters with alias</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>aggregate: push down filters with literal</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>aggregate: don&apos;t push down filters that are nondeterministic</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>SPARK-17712: aggregate: don&apos;t push down filters that are data-independent</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>broadcast hint</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>union</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>expand</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>predicate subquery: push down simple</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>predicate subquery: push down complex</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>Window: predicate push down -- basic</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>Window: predicate push down -- predicates with compound predicate using only one column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>Window: predicate push down -- multi window expressions with the same window spec</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>Window: predicate push down -- multi window specification - 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>Window: predicate push down -- multi window specification - 2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>Window: predicate push down -- predicates with multiple partitioning columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>Window: predicate push down -- complex predicate with the same expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>Window: no predicate push down -- predicates are not from partitioning keys</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>Window: no predicate push down -- partial compound partition key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>Window: no predicate push down -- complex predicates containing non partitioning columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>Window: no predicate push down -- complex predicate with different expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FilterPushdownSuite</className>
          <testName>join condition pushdown: deterministic and non-deterministic</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.optimizer.FoldablePropagationSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.optimizer.FoldablePropagationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.111</duration>
      <cases>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FoldablePropagationSuite</className>
          <testName>Propagate from subquery</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FoldablePropagationSuite</className>
          <testName>Propagate to select clause</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FoldablePropagationSuite</className>
          <testName>Propagate to where clause</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FoldablePropagationSuite</className>
          <testName>Propagate to orderBy clause</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FoldablePropagationSuite</className>
          <testName>Propagate to groupBy clause</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FoldablePropagationSuite</className>
          <testName>Propagate in a complex query</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FoldablePropagationSuite</className>
          <testName>Propagate in subqueries of Union queries</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.FoldablePropagationSuite</className>
          <testName>Propagate in expand</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraintsSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraintsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.287</duration>
      <cases>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraintsSuite</className>
          <testName>filter: filter out constraints in condition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraintsSuite</className>
          <testName>single inner join: filter out values on either side on equi-join keys</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraintsSuite</className>
          <testName>single inner join: filter out nulls on either side on non equal keys</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraintsSuite</className>
          <testName>single inner join with pre-existing filters: filter out values on either side</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraintsSuite</className>
          <testName>single outer join: no null filters are generated</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraintsSuite</className>
          <testName>multiple inner joins: filter out values on all sides on equi-join keys</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraintsSuite</className>
          <testName>inner join with filter: filter out values on all sides on equi-join keys</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraintsSuite</className>
          <testName>inner join with alias: alias contains multiple attributes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraintsSuite</className>
          <testName>inner join with alias: alias contains single attributes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.132</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraintsSuite</className>
          <testName>inner join with alias: don&apos;t generate constraints for recursive functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraintsSuite</className>
          <testName>generate correct filters for alias that don&apos;t produce recursive constraints</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.optimizer.JoinOptimizationSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.optimizer.JoinOptimizationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.073</duration>
      <cases>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.JoinOptimizationSuite</className>
          <testName>extract filters and joins</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.05</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.JoinOptimizationSuite</className>
          <testName>reorder inner joins</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.JoinOptimizationSuite</className>
          <testName>broadcasthint sets relation statistics to smallest value</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.optimizer.LikeSimplificationSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.optimizer.LikeSimplificationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.015000001</duration>
      <cases>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.LikeSimplificationSuite</className>
          <testName>simplify Like into StartsWith</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.LikeSimplificationSuite</className>
          <testName>simplify Like into EndsWith</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.LikeSimplificationSuite</className>
          <testName>simplify Like into startsWith and EndsWith</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.LikeSimplificationSuite</className>
          <testName>simplify Like into Contains</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.LikeSimplificationSuite</className>
          <testName>simplify Like into EqualTo</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.optimizer.LimitPushdownSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.optimizer.LimitPushdownSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.083000004</duration>
      <cases>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.LimitPushdownSuite</className>
          <testName>Union: limit to each side</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.LimitPushdownSuite</className>
          <testName>Union: limit to each side with constant-foldable limit expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.LimitPushdownSuite</className>
          <testName>Union: limit to each side with the new limit number</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.LimitPushdownSuite</className>
          <testName>Union: no limit to both sides if children having smaller limit values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.LimitPushdownSuite</className>
          <testName>Union: limit to each sides if children having larger limit values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.LimitPushdownSuite</className>
          <testName>left outer join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.LimitPushdownSuite</className>
          <testName>right outer join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.LimitPushdownSuite</className>
          <testName>larger limits are not pushed on top of smaller ones in right outer join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.LimitPushdownSuite</className>
          <testName>full outer join where neither side is limited and both sides have same statistics</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.LimitPushdownSuite</className>
          <testName>full outer join where neither side is limited and left side has larger statistics</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.LimitPushdownSuite</className>
          <testName>full outer join where neither side is limited and right side has larger statistics</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.LimitPushdownSuite</className>
          <testName>full outer join where both sides are limited</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.optimizer.OptimizeCodegenSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.optimizer.OptimizeCodegenSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.046000004</duration>
      <cases>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.OptimizeCodegenSuite</className>
          <testName>Codegen only when the number of branches is small</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.OptimizeCodegenSuite</className>
          <testName>Nested CaseWhen Codegen</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.OptimizeCodegenSuite</className>
          <testName>Multiple CaseWhen in one operator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.OptimizeCodegenSuite</className>
          <testName>Multiple CaseWhen in different operators</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.optimizer.OptimizeInSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.optimizer.OptimizeInSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.085999995</duration>
      <cases>
        <case>
          <duration>0.035</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.OptimizeInSuite</className>
          <testName>OptimizedIn test: Remove deterministic repetitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.OptimizeInSuite</className>
          <testName>OptimizedIn test: In clause not optimized to InSet when less than 10 items</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.OptimizeInSuite</className>
          <testName>OptimizedIn test: In clause optimized to InSet when more than 10 items</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.OptimizeInSuite</className>
          <testName>OptimizedIn test: In clause not optimized in case filter has attributes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.OptimizeInSuite</className>
          <testName>, exprN) gets transformed to Filter(null)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.OptimizeInSuite</className>
          <testName>OptimizedIn test: Inset optimization disabled as list expression contains attribute)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.OptimizeInSuite</className>
          <testName>OptimizedIn test: Inset optimization disabled as list expression contains attribute - select)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.OptimizeInSuite</className>
          <testName>OptimizedIn test: Setting the threshold for turning Set into InSet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.optimizer.OptimizerExtendableSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.optimizer.OptimizerExtendableSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.002</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.OptimizerExtendableSuite</className>
          <testName>Extending batches possible</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.optimizer.OuterJoinEliminationSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.optimizer.OuterJoinEliminationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.143</duration>
      <cases>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.OuterJoinEliminationSuite</className>
          <testName>joins: full outer to inner</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.OuterJoinEliminationSuite</className>
          <testName>joins: full outer to right</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.OuterJoinEliminationSuite</className>
          <testName>joins: full outer to left</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.OuterJoinEliminationSuite</className>
          <testName>joins: right to inner</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.OuterJoinEliminationSuite</className>
          <testName>joins: left to inner</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.OuterJoinEliminationSuite</className>
          <testName>joins: left to inner with complicated filter predicates #1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.OuterJoinEliminationSuite</className>
          <testName>joins: left to inner with complicated filter predicates #2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.OuterJoinEliminationSuite</className>
          <testName>joins: left to inner with complicated filter predicates #3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.OuterJoinEliminationSuite</className>
          <testName>joins: left to inner with complicated filter predicates #4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.OuterJoinEliminationSuite</className>
          <testName>joins: no outer join elimination if the filter is not NULL eliminated</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.OuterJoinEliminationSuite</className>
          <testName>joins: no outer join elimination if the filter&apos;s constraints are not NULL eliminated</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.optimizer.PropagateEmptyRelationSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.optimizer.PropagateEmptyRelationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.081</duration>
      <cases>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.PropagateEmptyRelationSuite</className>
          <testName>propagate empty relation through Union</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.049</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.PropagateEmptyRelationSuite</className>
          <testName>propagate empty relation through Join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.PropagateEmptyRelationSuite</className>
          <testName>propagate empty relation through UnaryNode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.PropagateEmptyRelationSuite</className>
          <testName>don&apos;t propagate non-empty local relation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.PropagateEmptyRelationSuite</className>
          <testName>propagate empty relation through Aggregate without aggregate function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.PropagateEmptyRelationSuite</className>
          <testName>don&apos;t propagate empty relation through Aggregate with aggregate function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.optimizer.PruneFiltersSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.optimizer.PruneFiltersSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.077</duration>
      <cases>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.PruneFiltersSuite</className>
          <testName>Constraints of isNull + LeftOuter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.PruneFiltersSuite</className>
          <testName>Constraints of unionall</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.PruneFiltersSuite</className>
          <testName>Pruning multiple constraints in the same run</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.PruneFiltersSuite</className>
          <testName>Partial pruning</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.PruneFiltersSuite</className>
          <testName>No predicate is pruned</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.PruneFiltersSuite</className>
          <testName>Nondeterministic predicate is not pruned</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.optimizer.RemoveAliasOnlyProjectSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.optimizer.RemoveAliasOnlyProjectSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.011000001</duration>
      <cases>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.RemoveAliasOnlyProjectSuite</className>
          <testName>all expressions in project list are aliased child output</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.RemoveAliasOnlyProjectSuite</className>
          <testName>all expressions in project list are aliased child output but with different order</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.RemoveAliasOnlyProjectSuite</className>
          <testName>some expressions in project list are aliased child output</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.RemoveAliasOnlyProjectSuite</className>
          <testName>some expressions in project list are aliased child output but with different order</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.RemoveAliasOnlyProjectSuite</className>
          <testName>some expressions in project list are not Alias or Attribute</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.RemoveAliasOnlyProjectSuite</className>
          <testName>some expressions in project list are aliased child output but with metadata</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.optimizer.ReorderAssociativeOperatorSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.optimizer.ReorderAssociativeOperatorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.042</duration>
      <cases>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.ReorderAssociativeOperatorSuite</className>
          <testName>Reorder associative operators</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.ReorderAssociativeOperatorSuite</className>
          <testName>nested expression with aggregate operator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.optimizer.ReplaceOperatorSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.optimizer.ReplaceOperatorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.007999999</duration>
      <cases>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.ReplaceOperatorSuite</className>
          <testName>replace Intersect with Left-semi Join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.ReplaceOperatorSuite</className>
          <testName>replace Except with Left-anti Join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.ReplaceOperatorSuite</className>
          <testName>replace Distinct with Aggregate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.optimizer.RewriteDistinctAggregatesSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.optimizer.RewriteDistinctAggregatesSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.042999998</duration>
      <cases>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.RewriteDistinctAggregatesSuite</className>
          <testName>single distinct group</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.RewriteDistinctAggregatesSuite</className>
          <testName>single distinct group with partial aggregates</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.RewriteDistinctAggregatesSuite</className>
          <testName>single distinct group with non-partial aggregates</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.RewriteDistinctAggregatesSuite</className>
          <testName>multiple distinct groups</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.RewriteDistinctAggregatesSuite</className>
          <testName>multiple distinct groups with partial aggregates</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.RewriteDistinctAggregatesSuite</className>
          <testName>multiple distinct groups with non-partial aggregates</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.optimizer.SetOperationSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.optimizer.SetOperationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.047</duration>
      <cases>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.SetOperationSuite</className>
          <testName>union: combine unions into one unions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.SetOperationSuite</className>
          <testName>union: filter to each side</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.SetOperationSuite</className>
          <testName>union: project to each side</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.SetOperationSuite</className>
          <testName>Remove unnecessary distincts in multiple unions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.SetOperationSuite</className>
          <testName>Keep necessary distincts in multiple unions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.optimizer.SimplifyCastsSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.optimizer.SimplifyCastsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.008</duration>
      <cases>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.SimplifyCastsSuite</className>
          <testName>non-nullable element array to nullable element array cast</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.SimplifyCastsSuite</className>
          <testName>nullable element to non-nullable element array cast</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.SimplifyCastsSuite</className>
          <testName>non-nullable value map to nullable value map cast</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.SimplifyCastsSuite</className>
          <testName>nullable value map to non-nullable value map cast</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.optimizer.SimplifyConditionalSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.optimizer.SimplifyConditionalSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.024</duration>
      <cases>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.SimplifyConditionalSuite</className>
          <testName>simplify if</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.SimplifyConditionalSuite</className>
          <testName>remove unreachable branches</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.SimplifyConditionalSuite</className>
          <testName>remove entire CaseWhen if only the else branch is reachable</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.SimplifyConditionalSuite</className>
          <testName>remove entire CaseWhen if the first branch is always true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.optimizer.SimplifyStringCaseConversionSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.optimizer.SimplifyStringCaseConversionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.02</duration>
      <cases>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.SimplifyStringCaseConversionSuite</className>
          <testName>simplify UPPER(UPPER(str))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.SimplifyStringCaseConversionSuite</className>
          <testName>simplify UPPER(LOWER(str))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.SimplifyStringCaseConversionSuite</className>
          <testName>simplify LOWER(UPPER(str))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.SimplifyStringCaseConversionSuite</className>
          <testName>simplify LOWER(LOWER(str))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.optimizer.TypedFilterOptimizationSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.optimizer.TypedFilterOptimizationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.22700001</duration>
      <cases>
        <case>
          <duration>0.058</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.TypedFilterOptimizationSuite</className>
          <testName>filter after serialize with the same object type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.TypedFilterOptimizationSuite</className>
          <testName>filter after serialize with different object types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.056</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.TypedFilterOptimizationSuite</className>
          <testName>filter before deserialize with the same object type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.TypedFilterOptimizationSuite</className>
          <testName>filter before deserialize with different object types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.TypedFilterOptimizationSuite</className>
          <testName>back to back filter with the same object type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.sql.catalyst.optimizer.TypedFilterOptimizationSuite</className>
          <testName>back to back filter with different object types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.parser.DataTypeParserSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.015000001</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</className>
          <testName>parse int</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</className>
          <testName>parse integer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</className>
          <testName>parse BooLean</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</className>
          <testName>parse tinYint</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</className>
          <testName>parse smallINT</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</className>
          <testName>parse INT</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</className>
          <testName>parse INTEGER</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</className>
          <testName>parse bigint</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</className>
          <testName>parse float</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</className>
          <testName>parse dOUBle</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</className>
          <testName>parse decimal(10, 5)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</className>
          <testName>parse decimal</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</className>
          <testName>parse DATE</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</className>
          <testName>parse timestamp</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</className>
          <testName>parse string</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</className>
          <testName>parse ChaR(5)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</className>
          <testName>parse varchAr(20)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</className>
          <testName>parse cHaR(27)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</className>
          <testName>parse BINARY</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</className>
          <testName>parse array&lt;doublE&gt;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</className>
          <testName>parse Array&lt;map&lt;int, tinYint&gt;&gt;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</className>
          <testName>parse array&lt;struct&lt;tinYint:tinyint&gt;&gt;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</className>
          <testName>parse MAP&lt;int, STRING&gt;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</className>
          <testName>parse MAp&lt;int, ARRAY&lt;double&gt;&gt;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</className>
          <testName>parse MAP&lt;int, struct&lt;varchar:string&gt;&gt;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</className>
          <testName>parse struct&lt;intType: int, ts:timestamp&gt;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</className>
          <testName>parse Struct&lt;int: int, timestamp:timestamp&gt;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</className>
          <testName>parse struct&lt;  struct:struct&lt;deciMal:DECimal, anotherDecimal:decimAL(5,2)&gt;,  MAP:Map&lt;timestamp, varchar(10)&gt;,  arrAy:Array&lt;double&gt;,  anotherArray:Array&lt;char(9)&gt;&gt;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</className>
          <testName>345&lt;&gt;:&quot;`:varchar(20)&gt;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</className>
          <testName>parse strUCt&lt;&gt;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</className>
          <testName>it is not a data type is not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</className>
          <testName>1:timestamp&gt; is not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</className>
          <testName>struct&lt;x: int is not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</className>
          <testName>struct&lt;x int, y string&gt; is not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</className>
          <testName>parse Struct&lt;TABLE: string, DATE:boolean&gt;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</className>
          <testName>parse struct&lt;end: long, select: int, from: string&gt;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.parser.DataTypeParserSuite</className>
          <testName>parse Struct&lt;x: INT, y: STRING COMMENT &apos;test&apos;&gt;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.parser.ErrorParserSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.parser.ErrorParserSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.549</duration>
      <cases>
        <case>
          <duration>0.359</duration>
          <className>org.apache.spark.sql.catalyst.parser.ErrorParserSuite</className>
          <testName>no viable input</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.083</duration>
          <className>org.apache.spark.sql.catalyst.parser.ErrorParserSuite</className>
          <testName>extraneous input</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.058</duration>
          <className>org.apache.spark.sql.catalyst.parser.ErrorParserSuite</className>
          <testName>mismatched input</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.049</duration>
          <className>org.apache.spark.sql.catalyst.parser.ErrorParserSuite</className>
          <testName>semantic errors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.parser.ExpressionParserSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.parser.ExpressionParserSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.865</duration>
      <cases>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.catalyst.parser.ExpressionParserSuite</className>
          <testName>star expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.catalyst.parser.ExpressionParserSuite</className>
          <testName>named expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.catalyst.parser.ExpressionParserSuite</className>
          <testName>binary logical expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.436</duration>
          <className>org.apache.spark.sql.catalyst.parser.ExpressionParserSuite</className>
          <testName>long binary logical expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.sql.catalyst.parser.ExpressionParserSuite</className>
          <testName>not expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.sql.catalyst.parser.ExpressionParserSuite</className>
          <testName>exists expression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.parser.ExpressionParserSuite</className>
          <testName>comparison expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.parser.ExpressionParserSuite</className>
          <testName>between expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.parser.ExpressionParserSuite</className>
          <testName>in expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.parser.ExpressionParserSuite</className>
          <testName>in sub-query</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.parser.ExpressionParserSuite</className>
          <testName>like expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.parser.ExpressionParserSuite</className>
          <testName>is null expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.sql.catalyst.parser.ExpressionParserSuite</className>
          <testName>binary arithmetic expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.parser.ExpressionParserSuite</className>
          <testName>unary arithmetic expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.catalyst.parser.ExpressionParserSuite</className>
          <testName>cast expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.sql.catalyst.parser.ExpressionParserSuite</className>
          <testName>function expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.052</duration>
          <className>org.apache.spark.sql.catalyst.parser.ExpressionParserSuite</className>
          <testName>window function expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.parser.ExpressionParserSuite</className>
          <testName>row constructor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.parser.ExpressionParserSuite</className>
          <testName>scalar sub-query</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.04</duration>
          <className>org.apache.spark.sql.catalyst.parser.ExpressionParserSuite</className>
          <testName>case when</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.catalyst.parser.ExpressionParserSuite</className>
          <testName>dereference</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.parser.ExpressionParserSuite</className>
          <testName>reference</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.parser.ExpressionParserSuite</className>
          <testName>subscript</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.parser.ExpressionParserSuite</className>
          <testName>parenthesis</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.catalyst.parser.ExpressionParserSuite</className>
          <testName>type constructors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.sql.catalyst.parser.ExpressionParserSuite</className>
          <testName>literals</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.parser.ExpressionParserSuite</className>
          <testName>strings</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.071</duration>
          <className>org.apache.spark.sql.catalyst.parser.ExpressionParserSuite</className>
          <testName>intervals</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.parser.ExpressionParserSuite</className>
          <testName>composed expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.parser.ExpressionParserSuite</className>
          <testName>current date/timestamp braceless expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.parser.ExpressionParserSuite</className>
          <testName>SPARK-17364, fully qualified column name which starts with number</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.parser.ExpressionParserSuite</className>
          <testName>SPARK-17832 function identifier contains backtick</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.parser.ParserUtilsSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.parser.ParserUtilsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.010000001</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.parser.ParserUtilsSuite</className>
          <testName>unescapeSQLString</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.parser.ParserUtilsSuite</className>
          <testName>command</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.parser.ParserUtilsSuite</className>
          <testName>operationNotAllowed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.parser.ParserUtilsSuite</className>
          <testName>checkDuplicateKeys</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.parser.ParserUtilsSuite</className>
          <testName>source</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.parser.ParserUtilsSuite</className>
          <testName>remainder</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.parser.ParserUtilsSuite</className>
          <testName>string</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.parser.ParserUtilsSuite</className>
          <testName>position</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.parser.ParserUtilsSuite</className>
          <testName>validate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.parser.ParserUtilsSuite</className>
          <testName>withOrigin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.parser.PlanParserSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.parser.PlanParserSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.28199998</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.parser.PlanParserSuite</className>
          <testName>case insensitive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.parser.PlanParserSuite</className>
          <testName>explain</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.catalyst.parser.PlanParserSuite</className>
          <testName>set operations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.parser.PlanParserSuite</className>
          <testName>common table expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.catalyst.parser.PlanParserSuite</className>
          <testName>simple select query</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.sql.catalyst.parser.PlanParserSuite</className>
          <testName>reverse select query</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.catalyst.parser.PlanParserSuite</className>
          <testName>multi select query</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.035</duration>
          <className>org.apache.spark.sql.catalyst.parser.PlanParserSuite</className>
          <testName>query organization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.catalyst.parser.PlanParserSuite</className>
          <testName>insert into</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.parser.PlanParserSuite</className>
          <testName>insert with if not exists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.sql.catalyst.parser.PlanParserSuite</className>
          <testName>aggregation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.parser.PlanParserSuite</className>
          <testName>limit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.sql.catalyst.parser.PlanParserSuite</className>
          <testName>window spec</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.018</duration>
          <className>org.apache.spark.sql.catalyst.parser.PlanParserSuite</className>
          <testName>lateral view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.054</duration>
          <className>org.apache.spark.sql.catalyst.parser.PlanParserSuite</className>
          <testName>joins</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.catalyst.parser.PlanParserSuite</className>
          <testName>sampled relations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.sql.catalyst.parser.PlanParserSuite</className>
          <testName>sub-query</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.sql.catalyst.parser.PlanParserSuite</className>
          <testName>scalar sub-query</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.parser.PlanParserSuite</className>
          <testName>table reference</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.parser.PlanParserSuite</className>
          <testName>table valued function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.catalyst.parser.PlanParserSuite</className>
          <testName>inline table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.parser.PlanParserSuite</className>
          <testName>simple select query with !&gt; and !&lt;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.parser.TableIdentifierParserSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.parser.TableIdentifierParserSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.077999994</duration>
      <cases>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.parser.TableIdentifierParserSuite</className>
          <testName>table identifier</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.parser.TableIdentifierParserSuite</className>
          <testName>quoted identifiers</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.catalyst.parser.TableIdentifierParserSuite</className>
          <testName>table identifier - strict keywords</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.052</duration>
          <className>org.apache.spark.sql.catalyst.parser.TableIdentifierParserSuite</className>
          <testName>table identifier - non reserved keywords</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.parser.TableIdentifierParserSuite</className>
          <testName>SPARK-17364 table identifier - contains number</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.parser.TableIdentifierParserSuite</className>
          <testName>SPARK-17832 table identifier - contains backtick</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.plans.ConstraintPropagationSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.plans.ConstraintPropagationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.188</duration>
      <cases>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.catalyst.plans.ConstraintPropagationSuite</className>
          <testName>propagating constraints in filters</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.sql.catalyst.plans.ConstraintPropagationSuite</className>
          <testName>propagating constraints in aggregate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.sql.catalyst.plans.ConstraintPropagationSuite</className>
          <testName>propagating constraints in expand</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.032</duration>
          <className>org.apache.spark.sql.catalyst.plans.ConstraintPropagationSuite</className>
          <testName>propagating constraints in aliases</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.catalyst.plans.ConstraintPropagationSuite</className>
          <testName>propagating constraints in union</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.plans.ConstraintPropagationSuite</className>
          <testName>propagating constraints in intersect</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.plans.ConstraintPropagationSuite</className>
          <testName>propagating constraints in except</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.plans.ConstraintPropagationSuite</className>
          <testName>propagating constraints in inner join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.plans.ConstraintPropagationSuite</className>
          <testName>propagating constraints in left-semi join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.plans.ConstraintPropagationSuite</className>
          <testName>propagating constraints in left-outer join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.plans.ConstraintPropagationSuite</className>
          <testName>propagating constraints in right-outer join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.plans.ConstraintPropagationSuite</className>
          <testName>propagating constraints in full-outer join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.plans.ConstraintPropagationSuite</className>
          <testName>infer additional constraints in filters</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.plans.ConstraintPropagationSuite</className>
          <testName>infer constraints on cast</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.038</duration>
          <className>org.apache.spark.sql.catalyst.plans.ConstraintPropagationSuite</className>
          <testName>infer isnotnull constraints from compound expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.plans.ConstraintPropagationSuite</className>
          <testName>infer IsNotNull constraints from non-nullable attributes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.plans.ConstraintPropagationSuite</className>
          <testName>not infer non-deterministic constraints</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.plans.LogicalPlanSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.plans.LogicalPlanSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.006</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.plans.LogicalPlanSuite</className>
          <testName>resolveOperator runs on operators</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.plans.LogicalPlanSuite</className>
          <testName>resolveOperator runs on operators recursively</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.plans.LogicalPlanSuite</className>
          <testName>resolveOperator skips all ready resolved plans</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.plans.LogicalPlanSuite</className>
          <testName>resolveOperator skips partially resolved plans</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.plans.LogicalPlanSuite</className>
          <testName>isStreaming</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.plans.SameResultSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.plans.SameResultSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.037</duration>
      <cases>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.plans.SameResultSuite</className>
          <testName>relations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.sql.catalyst.plans.SameResultSuite</className>
          <testName>projections</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.plans.SameResultSuite</className>
          <testName>filters</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.plans.SameResultSuite</className>
          <testName>sorts</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.plans.SameResultSuite</className>
          <testName>union</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.trees.RuleExecutorSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.trees.RuleExecutorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.011</duration>
      <cases>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.trees.RuleExecutorSuite</className>
          <testName>only once</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.trees.RuleExecutorSuite</className>
          <testName>to fixed point</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.trees.RuleExecutorSuite</className>
          <testName>to maxIterations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.trees.TreeNodeSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.trees.TreeNodeSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.47200003</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.trees.TreeNodeSuite</className>
          <testName>top node changed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.trees.TreeNodeSuite</className>
          <testName>one child changed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.trees.TreeNodeSuite</className>
          <testName>no change</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.trees.TreeNodeSuite</className>
          <testName>collect</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.trees.TreeNodeSuite</className>
          <testName>pre-order transform</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.trees.TreeNodeSuite</className>
          <testName>post-order transform</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.trees.TreeNodeSuite</className>
          <testName>transform works on nodes with Option children</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.trees.TreeNodeSuite</className>
          <testName>preserves origin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.trees.TreeNodeSuite</className>
          <testName>foreach up</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.trees.TreeNodeSuite</className>
          <testName>find</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.trees.TreeNodeSuite</className>
          <testName>collectFirst</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.trees.TreeNodeSuite</className>
          <testName>transformExpressions on nested expression sequence</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.catalyst.trees.TreeNodeSuite</className>
          <testName>expressions inside a map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.402</duration>
          <className>org.apache.spark.sql.catalyst.trees.TreeNodeSuite</className>
          <testName>toJSON</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.05</duration>
          <className>org.apache.spark.sql.catalyst.trees.TreeNodeSuite</className>
          <testName>StackOverflowError</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.util.DateTimeUtilsSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.util.DateTimeUtilsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>120.383</duration>
      <cases>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.catalyst.util.DateTimeUtilsSuite</className>
          <testName>timestamp and us</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.util.DateTimeUtilsSuite</className>
          <testName>us and julian day</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.sql.catalyst.util.DateTimeUtilsSuite</className>
          <testName>SPARK-6785: java date conversion before and after epoch</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.util.DateTimeUtilsSuite</className>
          <testName>string to date</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.045</duration>
          <className>org.apache.spark.sql.catalyst.util.DateTimeUtilsSuite</className>
          <testName>string to time</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.util.DateTimeUtilsSuite</className>
          <testName>string to timestamp</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.util.DateTimeUtilsSuite</className>
          <testName>SPARK-15379: special invalid date string</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.util.DateTimeUtilsSuite</className>
          <testName>hours</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.util.DateTimeUtilsSuite</className>
          <testName>minutes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.util.DateTimeUtilsSuite</className>
          <testName>seconds</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.util.DateTimeUtilsSuite</className>
          <testName>hours / minutes / seconds</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.util.DateTimeUtilsSuite</className>
          <testName>get day in year</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.util.DateTimeUtilsSuite</className>
          <testName>get year</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.util.DateTimeUtilsSuite</className>
          <testName>get quarter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.util.DateTimeUtilsSuite</className>
          <testName>get month</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.util.DateTimeUtilsSuite</className>
          <testName>get day of month</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.util.DateTimeUtilsSuite</className>
          <testName>date add months</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.util.DateTimeUtilsSuite</className>
          <testName>timestamp add months</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.util.DateTimeUtilsSuite</className>
          <testName>monthsBetween</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.531</duration>
          <className>org.apache.spark.sql.catalyst.util.DateTimeUtilsSuite</className>
          <testName>from UTC timestamp</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.117</duration>
          <className>org.apache.spark.sql.catalyst.util.DateTimeUtilsSuite</className>
          <testName>to UTC timestamp</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>119.625</duration>
          <className>org.apache.spark.sql.catalyst.util.DateTimeUtilsSuite</className>
          <testName>daysToMillis and millisToDays</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.util.MetadataSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.util.MetadataSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.011</duration>
      <cases>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.util.MetadataSuite</className>
          <testName>metadata builder and getters</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.util.MetadataSuite</className>
          <testName>metadata json conversion</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.util.NumberConverterSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.util.NumberConverterSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.002</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.util.NumberConverterSuite</className>
          <testName>convert</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.util.QuantileSummariesSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.31599998</duration>
      <cases>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>1 and seq=increasing, compression=1000</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>1 and seq=increasing, compression=1000</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>1 and seq=increasing, compression=1000 (interleaved)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>1 and seq=increasing, compression=10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>1 and seq=increasing, compression=10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>1 and seq=increasing, compression=10 (interleaved)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>0E-4 and seq=increasing, compression=1000</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>0E-4 and seq=increasing, compression=1000</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.063</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>0E-4 and seq=increasing, compression=1000 (interleaved)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>0E-4 and seq=increasing, compression=10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>0E-4 and seq=increasing, compression=10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>0E-4 and seq=increasing, compression=10 (interleaved)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>1 and seq=decreasing, compression=1000</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>1 and seq=decreasing, compression=1000</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>1 and seq=decreasing, compression=1000 (interleaved)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>1 and seq=decreasing, compression=10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>1 and seq=decreasing, compression=10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>1 and seq=decreasing, compression=10 (interleaved)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>0E-4 and seq=decreasing, compression=1000</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>0E-4 and seq=decreasing, compression=1000</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>0E-4 and seq=decreasing, compression=1000 (interleaved)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>0E-4 and seq=decreasing, compression=10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>0E-4 and seq=decreasing, compression=10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>0E-4 and seq=decreasing, compression=10 (interleaved)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>1 and seq=random, compression=1000</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>1 and seq=random, compression=1000</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>1 and seq=random, compression=1000 (interleaved)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>1 and seq=random, compression=10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>1 and seq=random, compression=10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>1 and seq=random, compression=10 (interleaved)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>0E-4 and seq=random, compression=1000</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>0E-4 and seq=random, compression=1000</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>0E-4 and seq=random, compression=1000 (interleaved)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>0E-4 and seq=random, compression=10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>0E-4 and seq=random, compression=10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>0E-4 and seq=random, compression=10 (interleaved)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>1 and seq=increasing, compression=1000</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>1 and seq=increasing, compression=1000</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>1 and seq=increasing, compression=10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>1 and seq=increasing, compression=10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>0E-4 and seq=increasing, compression=1000</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>0E-4 and seq=increasing, compression=1000</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>0E-4 and seq=increasing, compression=10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>0E-4 and seq=increasing, compression=10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>1 and seq=decreasing, compression=1000</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>1 and seq=decreasing, compression=1000</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>1 and seq=decreasing, compression=10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>1 and seq=decreasing, compression=10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>0E-4 and seq=decreasing, compression=1000</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>0E-4 and seq=decreasing, compression=1000</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>0E-4 and seq=decreasing, compression=10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>0E-4 and seq=decreasing, compression=10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>1 and seq=random, compression=1000</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>1 and seq=random, compression=1000</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>1 and seq=random, compression=10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>1 and seq=random, compression=10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>0E-4 and seq=random, compression=1000</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>0E-4 and seq=random, compression=1000</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>0E-4 and seq=random, compression=10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.util.QuantileSummariesSuite</className>
          <testName>0E-4 and seq=random, compression=10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.util.StringUtilsSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.util.StringUtilsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.0050000004</duration>
      <cases>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.util.StringUtilsSuite</className>
          <testName>escapeLikeRegex</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.util.StringUtilsSuite</className>
          <testName>filter pattern</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.util.TypeUtilsSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.util.TypeUtilsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.004</duration>
      <cases>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.util.TypeUtilsSuite</className>
          <testName>checkForSameTypeInputExpr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.catalyst.util.UnsafeArraySuite.xml</file>
      <name>org.apache.spark.sql.catalyst.util.UnsafeArraySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.85800004</duration>
      <cases>
        <case>
          <duration>0.842</duration>
          <className>org.apache.spark.sql.catalyst.util.UnsafeArraySuite</className>
          <testName>read array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.util.UnsafeArraySuite</className>
          <testName>from primitive array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.sql.catalyst.util.UnsafeArraySuite</className>
          <testName>to primitive array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.streaming.JavaOutputModeSuite.xml</file>
      <name>org.apache.spark.sql.streaming.JavaOutputModeSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.001</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.streaming.JavaOutputModeSuite</className>
          <testName>testOutputModes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.types.DataTypeSuite.xml</file>
      <name>org.apache.spark.sql.types.DataTypeSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.077999964</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>construct an ArrayType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>construct an MapType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>construct with add</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>construct with add from StructField</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>construct with add from StructField with comments</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>construct with String DataType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>extract fields from a StructType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>extract field index from a StructType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>fieldsMap returns map of name to StructField</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>merge where right is empty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>merge where left is empty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>merge where both are non-empty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>merge where right contains type conflict</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>existsRecursively</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>JSON - NullType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>JSON - BooleanType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>JSON - ByteType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>JSON - ShortType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>JSON - IntegerType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>JSON - LongType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>JSON - FloatType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>JSON - DoubleType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>JSON - DecimalType(10,5)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>JSON - DecimalType(38,18)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>JSON - DateType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>JSON - TimestampType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>JSON - StringType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>JSON - BinaryType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>JSON - ArrayType(DoubleType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>JSON - ArrayType(StringType,false)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>JSON - MapType(IntegerType,StringType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>JSON - MapType(IntegerType,ArrayType(DoubleType,true),false)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>JSON - StructType(StructField(a,IntegerType,true), StructField(b,ArrayType(DoubleType,true),false), StructField(c,DoubleType,false))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>Check the default size of NullType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>Check the default size of BooleanType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>Check the default size of ByteType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>Check the default size of ShortType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>Check the default size of IntegerType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>Check the default size of LongType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>Check the default size of FloatType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>Check the default size of DoubleType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>Check the default size of DecimalType(10,5)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>Check the default size of DecimalType(38,18)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>Check the default size of DateType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>Check the default size of TimestampType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>Check the default size of StringType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>Check the default size of BinaryType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>Check the default size of ArrayType(DoubleType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>Check the default size of ArrayType(StringType,false)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>Check the default size of MapType(IntegerType,StringType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>Check the default size of MapType(IntegerType,ArrayType(DoubleType,true),false)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>Check the default size of StructType(StructField(a,IntegerType,true), StructField(b,ArrayType(DoubleType,true),false), StructField(c,DoubleType,false))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>equalsIgnoreCompatibleNullability: (from: ArrayType(DoubleType,true), to: ArrayType(DoubleType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>equalsIgnoreCompatibleNullability: (from: ArrayType(DoubleType,false), to: ArrayType(DoubleType,false))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>equalsIgnoreCompatibleNullability: (from: ArrayType(DoubleType,false), to: ArrayType(DoubleType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>equalsIgnoreCompatibleNullability: (from: ArrayType(DoubleType,true), to: ArrayType(DoubleType,false))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>equalsIgnoreCompatibleNullability: (from: ArrayType(DoubleType,false), to: ArrayType(StringType,false))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>equalsIgnoreCompatibleNullability: (from: MapType(StringType,DoubleType,true), to: MapType(StringType,DoubleType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>equalsIgnoreCompatibleNullability: (from: MapType(StringType,DoubleType,false), to: MapType(StringType,DoubleType,false))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>equalsIgnoreCompatibleNullability: (from: MapType(StringType,DoubleType,false), to: MapType(StringType,DoubleType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>equalsIgnoreCompatibleNullability: (from: MapType(StringType,DoubleType,true), to: MapType(StringType,DoubleType,false))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>equalsIgnoreCompatibleNullability: (from: MapType(StringType,ArrayType(IntegerType,true),true), to: MapType(StringType,ArrayType(IntegerType,false),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>equalsIgnoreCompatibleNullability: (from: MapType(StringType,ArrayType(IntegerType,false),true), to: MapType(StringType,ArrayType(IntegerType,true),true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>equalsIgnoreCompatibleNullability: (from: StructType(StructField(a,StringType,true)), to: StructType(StructField(a,StringType,true)))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>equalsIgnoreCompatibleNullability: (from: StructType(StructField(a,StringType,false)), to: StructType(StructField(a,StringType,false)))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>equalsIgnoreCompatibleNullability: (from: StructType(StructField(a,StringType,false)), to: StructType(StructField(a,StringType,true)))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>equalsIgnoreCompatibleNullability: (from: StructType(StructField(a,StringType,true)), to: StructType(StructField(a,StringType,false)))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>equalsIgnoreCompatibleNullability: (from: StructType(StructField(a,StringType,false), StructField(b,StringType,true)), to: StructType(StructField(a,StringType,false), StructField(b,StringType,false)))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>catalogString: BooleanType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>catalogString: ByteType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>catalogString: ShortType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>catalogString: IntegerType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>catalogString: LongType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>catalogString: FloatType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>catalogString: DoubleType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>catalogString: DecimalType(10,5)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>catalogString: BinaryType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>catalogString: StringType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>catalogString: DateType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>catalogString: TimestampType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>catalogString: StructType(StructField(col0,IntegerType,true), StructField(col1,IntegerType,true), StructField(col2,IntegerType,true), StructField(col3,IntegerType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>catalogString: StructType(StructField(col0,IntegerType,true), StructField(col1,IntegerType,true), StructField(col2,IntegerType,true), StructField(col3,IntegerType,true), StructField(col4,IntegerType,true), StructField(col5,IntegerType,true), StructField(col6,IntegerType,true), StructField(col7,IntegerType,true), StructField(col8,IntegerType,true), StructField(col9,IntegerType,true), StructField(col10,IntegerType,true), StructField(col11,IntegerType,true), StructField(col12,IntegerType,true), StructField(col13,IntegerType,true), StructField(col14,IntegerType,true), StructField(col15,IntegerType,true), StructField(col16,IntegerType,true), StructField(col17,IntegerType,true), StructField(col18,IntegerType,true), StructField(col19,IntegerType,true), StructField(col20,IntegerType,true), StructField(col21,IntegerType,true), StructField(col22,IntegerType,true), StructField(col23,IntegerType,true), StructField(col24,IntegerType,true), StructField(col25,IntegerType,true), StructField(col26,IntegerType,true), StructField(col27,IntegerType,true), StructField(col28,IntegerType,true), StructField(col29,IntegerType,true), StructField(col30,IntegerType,true), StructField(col31,IntegerType,true), StructField(col32,IntegerType,true), StructField(col33,IntegerType,true), StructField(col34,IntegerType,true), StructField(col35,IntegerType,true), StructField(col36,IntegerType,true), StructField(col37,IntegerType,true), StructField(col38,IntegerType,true), StructField(col39,IntegerType,true))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>catalogString: ArrayType(IntegerType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>catalogString: ArrayType(StructType(StructField(col0,IntegerType,true), StructField(col1,IntegerType,true), StructField(col2,IntegerType,true), StructField(col3,IntegerType,true), StructField(col4,IntegerType,true), StructField(col5,IntegerType,true), StructField(col6,IntegerType,true), StructField(col7,IntegerType,true), StructField(col8,IntegerType,true), StructField(col9,IntegerType,true), StructField(col10,IntegerType,true), StructField(col11,IntegerType,true), StructField(col12,IntegerType,true), StructField(col13,IntegerType,true), StructField(col14,IntegerType,true), StructField(col15,IntegerType,true), StructField(col16,IntegerType,true), StructField(col17,IntegerType,true), StructField(col18,IntegerType,true), StructField(col19,IntegerType,true), StructField(col20,IntegerType,true), StructField(col21,IntegerType,true), StructField(col22,IntegerType,true), StructField(col23,IntegerType,true), StructField(col24,IntegerType,true), StructField(col25,IntegerType,true), StructField(col26,IntegerType,true), StructField(col27,IntegerType,true), StructField(col28,IntegerType,true), StructField(col29,IntegerType,true), StructField(col30,IntegerType,true), StructField(col31,IntegerType,true), StructField(col32,IntegerType,true), StructField(col33,IntegerType,true), StructField(col34,IntegerType,true), StructField(col35,IntegerType,true), StructField(col36,IntegerType,true), StructField(col37,IntegerType,true), StructField(col38,IntegerType,true), StructField(col39,IntegerType,true)),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>catalogString: MapType(IntegerType,StringType,true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.types.DataTypeSuite</className>
          <testName>catalogString: MapType(IntegerType,StructType(StructField(col0,IntegerType,true), StructField(col1,IntegerType,true), StructField(col2,IntegerType,true), StructField(col3,IntegerType,true), StructField(col4,IntegerType,true), StructField(col5,IntegerType,true), StructField(col6,IntegerType,true), StructField(col7,IntegerType,true), StructField(col8,IntegerType,true), StructField(col9,IntegerType,true), StructField(col10,IntegerType,true), StructField(col11,IntegerType,true), StructField(col12,IntegerType,true), StructField(col13,IntegerType,true), StructField(col14,IntegerType,true), StructField(col15,IntegerType,true), StructField(col16,IntegerType,true), StructField(col17,IntegerType,true), StructField(col18,IntegerType,true), StructField(col19,IntegerType,true), StructField(col20,IntegerType,true), StructField(col21,IntegerType,true), StructField(col22,IntegerType,true), StructField(col23,IntegerType,true), StructField(col24,IntegerType,true), StructField(col25,IntegerType,true), StructField(col26,IntegerType,true), StructField(col27,IntegerType,true), StructField(col28,IntegerType,true), StructField(col29,IntegerType,true), StructField(col30,IntegerType,true), StructField(col31,IntegerType,true), StructField(col32,IntegerType,true), StructField(col33,IntegerType,true), StructField(col34,IntegerType,true), StructField(col35,IntegerType,true), StructField(col36,IntegerType,true), StructField(col37,IntegerType,true), StructField(col38,IntegerType,true), StructField(col39,IntegerType,true)),true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/catalyst/target/test-reports/org.apache.spark.sql.types.DecimalSuite.xml</file>
      <name>org.apache.spark.sql.types.DecimalSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.021</duration>
      <cases>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.types.DecimalSuite</className>
          <testName>creating decimals</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DecimalSuite</className>
          <testName>creating decimals with negative scale</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DecimalSuite</className>
          <testName>double and long values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.types.DecimalSuite</className>
          <testName>small decimals represented as unscaled long</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DecimalSuite</className>
          <testName>hash code</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DecimalSuite</className>
          <testName>equals</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DecimalSuite</className>
          <testName>isZero</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.types.DecimalSuite</className>
          <testName>arithmetic</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DecimalSuite</className>
          <testName>accurate precision after multiplication</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DecimalSuite</className>
          <testName>fix non-terminating decimal expansion problem</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DecimalSuite</className>
          <testName>fix loss of precision/scale when doing division operation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.types.DecimalSuite</className>
          <testName>set/setOrNull</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.sql.types.DecimalSuite</className>
          <testName>changePrecision() on compact decimal should respect rounding mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.ApproximatePercentileQuerySuite.xml</file>
      <name>org.apache.spark.sql.ApproximatePercentileQuerySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.606</duration>
      <cases>
        <case>
          <duration>0.366</duration>
          <className>org.apache.spark.sql.ApproximatePercentileQuerySuite</className>
          <testName>percentile_approx, single percentile value</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.306</duration>
          <className>org.apache.spark.sql.ApproximatePercentileQuerySuite</className>
          <testName>percentile_approx, array of percentile value</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.127</duration>
          <className>org.apache.spark.sql.ApproximatePercentileQuerySuite</className>
          <testName>percentile_approx, multiple records with the minimum value in a partition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.321</duration>
          <className>org.apache.spark.sql.ApproximatePercentileQuerySuite</className>
          <testName>percentile_approx, with different accuracies</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.134</duration>
          <className>org.apache.spark.sql.ApproximatePercentileQuerySuite</className>
          <testName>percentile_approx, supports constant folding for parameter accuracy and percentages</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.114</duration>
          <className>org.apache.spark.sql.ApproximatePercentileQuerySuite</className>
          <testName>percentile_approx(), aggregation on empty input table, no group by</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.112</duration>
          <className>org.apache.spark.sql.ApproximatePercentileQuerySuite</className>
          <testName>percentile_approx(), aggregation on empty input table, with group by</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.232</duration>
          <className>org.apache.spark.sql.ApproximatePercentileQuerySuite</className>
          <testName>percentile_approx(null), aggregation with group by</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.178</duration>
          <className>org.apache.spark.sql.ApproximatePercentileQuerySuite</className>
          <testName>percentile_approx(null), aggregation without group by</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.171</duration>
          <className>org.apache.spark.sql.ApproximatePercentileQuerySuite</className>
          <testName>), input rows contains null, with out group by</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.355</duration>
          <className>org.apache.spark.sql.ApproximatePercentileQuerySuite</className>
          <testName>), input rows contains null, with group by</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.19</duration>
          <className>org.apache.spark.sql.ApproximatePercentileQuerySuite</className>
          <testName>) works in window function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.CachedTableSuite.xml</file>
      <name>org.apache.spark.sql.CachedTableSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>9.726001</duration>
      <cases>
        <case>
          <duration>0.133</duration>
          <className>org.apache.spark.sql.CachedTableSuite</className>
          <testName>withColumn doesn&apos;t invalidate cached dataframe</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.sql.CachedTableSuite</className>
          <testName>cache temp table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.sql.CachedTableSuite</className>
          <testName>unpersist an uncached table will not raise exception</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.076</duration>
          <className>org.apache.spark.sql.CachedTableSuite</className>
          <testName>cache table as select</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.039</duration>
          <className>org.apache.spark.sql.CachedTableSuite</className>
          <testName>uncaching temp table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.822</duration>
          <className>org.apache.spark.sql.CachedTableSuite</className>
          <testName>too big for memory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.sql.CachedTableSuite</className>
          <testName>cache() should use in-memory columnar caching</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.sql.CachedTableSuite</className>
          <testName>unpersist() should drop in-memory columnar cache</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.CachedTableSuite</className>
          <testName>isCached</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.CachedTableSuite</className>
          <testName>SPARK-1669: cacheTable should be idempotent</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.102</duration>
          <className>org.apache.spark.sql.CachedTableSuite</className>
          <testName>read from cached table and uncache</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.069</duration>
          <className>org.apache.spark.sql.CachedTableSuite</className>
          <testName>SELECT star from cached table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.429</duration>
          <className>org.apache.spark.sql.CachedTableSuite</className>
          <testName>Self-join cached</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.049</duration>
          <className>org.apache.spark.sql.CachedTableSuite</className>
          <testName>&apos;CACHE TABLE&apos; and &apos;UNCACHE TABLE&apos; SQL statement</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.052</duration>
          <className>org.apache.spark.sql.CachedTableSuite</className>
          <testName>CACHE TABLE tableName AS SELECT * FROM anotherTable</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.061</duration>
          <className>org.apache.spark.sql.CachedTableSuite</className>
          <testName>CACHE TABLE tableName AS SELECT </testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.057</duration>
          <className>org.apache.spark.sql.CachedTableSuite</className>
          <testName>CACHE LAZY TABLE tableName</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.sql.CachedTableSuite</className>
          <testName>InMemoryRelation statistics</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.CachedTableSuite</className>
          <testName>Drops temporary table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.sql.CachedTableSuite</className>
          <testName>Drops cached temporary table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.045</duration>
          <className>org.apache.spark.sql.CachedTableSuite</className>
          <testName>Clear all cache</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.725</duration>
          <className>org.apache.spark.sql.CachedTableSuite</className>
          <testName>Ensure accumulators to be cleared after GC when uncacheTable</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.051</duration>
          <className>org.apache.spark.sql.CachedTableSuite</className>
          <testName>SPARK-10327 Cache Table is not working while subquery has alias in its project list</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>5.721</duration>
          <className>org.apache.spark.sql.CachedTableSuite</className>
          <testName>A cached table preserves the partitioning and ordering of its cached SparkPlan</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.085</duration>
          <className>org.apache.spark.sql.CachedTableSuite</className>
          <testName>SPARK-15870 DataFrame can&apos;t execute after uncacheTable</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.sql.CachedTableSuite</className>
          <testName>SPARK-15915 Logical plans should use canonicalized plan when override sameResult</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.ColumnExpressionSuite.xml</file>
      <name>org.apache.spark.sql.ColumnExpressionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>6.414</duration>
      <cases>
        <case>
          <duration>0.156</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>column names with space</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.246</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>column names with dot</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.018</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>alias and name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>as propagates metadata</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.06</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>collect on column produced by a binary operator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.093</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>star</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.091</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>star qualified by data frame object</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.056</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>star qualified by table name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.168</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>+</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.13</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>-</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.17</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>*</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.136</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>/</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.127</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>%</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.064</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>unary -</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.192</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>unary !</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.191</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>isNull</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.131</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>isNotNull</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.159</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>isNaN</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.171</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>nanvl</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.165</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>===</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.102</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>&lt;=&gt;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.22</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>=!=</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.135</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>&gt;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.154</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>&gt;=</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.125</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>&lt;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.135</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>&lt;=</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.108</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>between</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.351</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>in</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.143</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>&amp;&amp;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.12</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>||</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.054</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>SPARK-7321 when conditional statements</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.461</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>sqrt</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.248</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>upper</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.193</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>lower</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.067</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>monotonically_increasing_id</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.092</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>spark_partition_id</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.279</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>input_file_name - FileScanRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.209</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>input_file_name - HadoopRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.175</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>input_file_name - NewHadoopRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>columns can be compared</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>alias with metadata</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.09</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>rand</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.059</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>randn</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.123</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>bitwiseAND</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.119</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>bitwiseOR</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.123</duration>
          <className>org.apache.spark.sql.ColumnExpressionSuite</className>
          <testName>bitwiseXOR</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.DataFrameAggregateSuite.xml</file>
      <name>org.apache.spark.sql.DataFrameAggregateSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>14.798002</duration>
      <cases>
        <case>
          <duration>1.96</duration>
          <className>org.apache.spark.sql.DataFrameAggregateSuite</className>
          <testName>groupBy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.238</duration>
          <className>org.apache.spark.sql.DataFrameAggregateSuite</className>
          <testName>SPARK-17124 agg should be ordering preserving</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.379</duration>
          <className>org.apache.spark.sql.DataFrameAggregateSuite</className>
          <testName>rollup</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.744</duration>
          <className>org.apache.spark.sql.DataFrameAggregateSuite</className>
          <testName>cube</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.256</duration>
          <className>org.apache.spark.sql.DataFrameAggregateSuite</className>
          <testName>grouping and grouping_id</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.565</duration>
          <className>org.apache.spark.sql.DataFrameAggregateSuite</className>
          <testName>grouping/grouping_id inside window function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.499</duration>
          <className>org.apache.spark.sql.DataFrameAggregateSuite</className>
          <testName>rollup overlapping columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.948</duration>
          <className>org.apache.spark.sql.DataFrameAggregateSuite</className>
          <testName>cube overlapping columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.339</duration>
          <className>org.apache.spark.sql.DataFrameAggregateSuite</className>
          <testName>retainGroupColumns config</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.104</duration>
          <className>org.apache.spark.sql.DataFrameAggregateSuite</className>
          <testName>agg without groups</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.096</duration>
          <className>org.apache.spark.sql.DataFrameAggregateSuite</className>
          <testName>agg without groups and functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.331</duration>
          <className>org.apache.spark.sql.DataFrameAggregateSuite</className>
          <testName>average</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.735</duration>
          <className>org.apache.spark.sql.DataFrameAggregateSuite</className>
          <testName>null average</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.346</duration>
          <className>org.apache.spark.sql.DataFrameAggregateSuite</className>
          <testName>zero average</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.388</duration>
          <className>org.apache.spark.sql.DataFrameAggregateSuite</className>
          <testName>count</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.036</duration>
          <className>org.apache.spark.sql.DataFrameAggregateSuite</className>
          <testName>null count</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.898</duration>
          <className>org.apache.spark.sql.DataFrameAggregateSuite</className>
          <testName>multiple column distinct count</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.259</duration>
          <className>org.apache.spark.sql.DataFrameAggregateSuite</className>
          <testName>zero count</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.296</duration>
          <className>org.apache.spark.sql.DataFrameAggregateSuite</className>
          <testName>stddev</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.137</duration>
          <className>org.apache.spark.sql.DataFrameAggregateSuite</className>
          <testName>zero stddev</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.094</duration>
          <className>org.apache.spark.sql.DataFrameAggregateSuite</className>
          <testName>zero sum</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.171</duration>
          <className>org.apache.spark.sql.DataFrameAggregateSuite</className>
          <testName>zero sum distinct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.368</duration>
          <className>org.apache.spark.sql.DataFrameAggregateSuite</className>
          <testName>moments</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.522</duration>
          <className>org.apache.spark.sql.DataFrameAggregateSuite</className>
          <testName>zero moments</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.392</duration>
          <className>org.apache.spark.sql.DataFrameAggregateSuite</className>
          <testName>null moments</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.241</duration>
          <className>org.apache.spark.sql.DataFrameAggregateSuite</className>
          <testName>collect functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.253</duration>
          <className>org.apache.spark.sql.DataFrameAggregateSuite</className>
          <testName>collect functions structs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.DataFrameAggregateSuite</className>
          <testName>collect_set functions cannot have maps</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.218</duration>
          <className>org.apache.spark.sql.DataFrameAggregateSuite</className>
          <testName>SPARK-17641: collect functions should not collect null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.288</duration>
          <className>org.apache.spark.sql.DataFrameAggregateSuite</className>
          <testName>SPARK-14664: Decimal sum/avg over window should work</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.246</duration>
          <className>org.apache.spark.sql.DataFrameAggregateSuite</className>
          <testName>SQL decimal test (used for catching certain demical handling bugs in aggregates)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.43</duration>
          <className>org.apache.spark.sql.DataFrameAggregateSuite</className>
          <testName>SPARK-17616: distinct aggregate combined with a non-partial aggregate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.DataFrameComplexTypeSuite.xml</file>
      <name>org.apache.spark.sql.DataFrameComplexTypeSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.883</duration>
      <cases>
        <case>
          <duration>0.088</duration>
          <className>org.apache.spark.sql.DataFrameComplexTypeSuite</className>
          <testName>UDF on struct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.038</duration>
          <className>org.apache.spark.sql.DataFrameComplexTypeSuite</className>
          <testName>UDF on named_struct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.058</duration>
          <className>org.apache.spark.sql.DataFrameComplexTypeSuite</className>
          <testName>UDF on array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.sql.DataFrameComplexTypeSuite</className>
          <testName>UDF on map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.098</duration>
          <className>org.apache.spark.sql.DataFrameComplexTypeSuite</className>
          <testName>SPARK-12477 accessing null element in array field</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.579</duration>
          <className>org.apache.spark.sql.DataFrameComplexTypeSuite</className>
          <testName>apply method grows beyond 64KB</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.DataFrameFunctionsSuite.xml</file>
      <name>org.apache.spark.sql.DataFrameFunctionsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>5.2250004</duration>
      <cases>
        <case>
          <duration>0.08</duration>
          <className>org.apache.spark.sql.DataFrameFunctionsSuite</className>
          <testName>array with column name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.045</duration>
          <className>org.apache.spark.sql.DataFrameFunctionsSuite</className>
          <testName>array with column expression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.068</duration>
          <className>org.apache.spark.sql.DataFrameFunctionsSuite</className>
          <testName>map with column expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.054</duration>
          <className>org.apache.spark.sql.DataFrameFunctionsSuite</className>
          <testName>struct with column name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.sql.DataFrameFunctionsSuite</className>
          <testName>struct with column expression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.089</duration>
          <className>org.apache.spark.sql.DataFrameFunctionsSuite</className>
          <testName>struct with column expression to be automatically named</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.103</duration>
          <className>org.apache.spark.sql.DataFrameFunctionsSuite</className>
          <testName>struct with literal columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.097</duration>
          <className>org.apache.spark.sql.DataFrameFunctionsSuite</className>
          <testName>struct with all literal columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.139</duration>
          <className>org.apache.spark.sql.DataFrameFunctionsSuite</className>
          <testName>constant functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.122</duration>
          <className>org.apache.spark.sql.DataFrameFunctionsSuite</className>
          <testName>bitwiseNOT</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.099</duration>
          <className>org.apache.spark.sql.DataFrameFunctionsSuite</className>
          <testName>bin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.062</duration>
          <className>org.apache.spark.sql.DataFrameFunctionsSuite</className>
          <testName>if function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.091</duration>
          <className>org.apache.spark.sql.DataFrameFunctionsSuite</className>
          <testName>misc md5 function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.085</duration>
          <className>org.apache.spark.sql.DataFrameFunctionsSuite</className>
          <testName>misc sha1 function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.075</duration>
          <className>org.apache.spark.sql.DataFrameFunctionsSuite</className>
          <testName>misc sha2 function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.076</duration>
          <className>org.apache.spark.sql.DataFrameFunctionsSuite</className>
          <testName>misc crc32 function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.064</duration>
          <className>org.apache.spark.sql.DataFrameFunctionsSuite</className>
          <testName>string function find_in_set</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.289</duration>
          <className>org.apache.spark.sql.DataFrameFunctionsSuite</className>
          <testName>conditional function: least</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.23</duration>
          <className>org.apache.spark.sql.DataFrameFunctionsSuite</className>
          <testName>conditional function: greatest</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.233</duration>
          <className>org.apache.spark.sql.DataFrameFunctionsSuite</className>
          <testName>pmod</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.441</duration>
          <className>org.apache.spark.sql.DataFrameFunctionsSuite</className>
          <testName>sort_array function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.094</duration>
          <className>org.apache.spark.sql.DataFrameFunctionsSuite</className>
          <testName>array size function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.108</duration>
          <className>org.apache.spark.sql.DataFrameFunctionsSuite</className>
          <testName>map size function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.15</duration>
          <className>org.apache.spark.sql.DataFrameFunctionsSuite</className>
          <testName>map_keys/map_values function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.175</duration>
          <className>org.apache.spark.sql.DataFrameFunctionsSuite</className>
          <testName>array contains function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.119</duration>
          <className>org.apache.spark.sql.DataFrameFunctionsSuite</className>
          <testName>SPARK-14393: values generated by non-deterministic functions shouldn&apos;t change after coalesce or union</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.DataFrameImplicitsSuite.xml</file>
      <name>org.apache.spark.sql.DataFrameImplicitsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.311</duration>
      <cases>
        <case>
          <duration>0.096</duration>
          <className>org.apache.spark.sql.DataFrameImplicitsSuite</className>
          <testName>RDD of tuples</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.052</duration>
          <className>org.apache.spark.sql.DataFrameImplicitsSuite</className>
          <testName>Seq of tuples</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.049</duration>
          <className>org.apache.spark.sql.DataFrameImplicitsSuite</className>
          <testName>RDD[Int]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.058</duration>
          <className>org.apache.spark.sql.DataFrameImplicitsSuite</className>
          <testName>RDD[Long]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.056</duration>
          <className>org.apache.spark.sql.DataFrameImplicitsSuite</className>
          <testName>RDD[String]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.DataFrameJoinSuite.xml</file>
      <name>org.apache.spark.sql.DataFrameJoinSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>5.5629997</duration>
      <cases>
        <case>
          <duration>0.126</duration>
          <className>org.apache.spark.sql.DataFrameJoinSuite</className>
          <testName>join - join using</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.146</duration>
          <className>org.apache.spark.sql.DataFrameJoinSuite</className>
          <testName>join - join using multiple columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.64</duration>
          <className>org.apache.spark.sql.DataFrameJoinSuite</className>
          <testName>join - sorted columns not in join&apos;s outputSet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.026</duration>
          <className>org.apache.spark.sql.DataFrameJoinSuite</className>
          <testName>join - join using multiple columns and specifying join type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.06</duration>
          <className>org.apache.spark.sql.DataFrameJoinSuite</className>
          <testName>join - join using self join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.399</duration>
          <className>org.apache.spark.sql.DataFrameJoinSuite</className>
          <testName>join - self join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.108</duration>
          <className>org.apache.spark.sql.DataFrameJoinSuite</className>
          <testName>join - cross join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.524</duration>
          <className>org.apache.spark.sql.DataFrameJoinSuite</className>
          <testName>join - using aliases after self join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.954</duration>
          <className>org.apache.spark.sql.DataFrameJoinSuite</className>
          <testName>[SPARK-6231] join - self join auto resolve ambiguity</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.246</duration>
          <className>org.apache.spark.sql.DataFrameJoinSuite</className>
          <testName>broadcast join hint</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.577</duration>
          <className>org.apache.spark.sql.DataFrameJoinSuite</className>
          <testName>join - outer join conversion</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.474</duration>
          <className>org.apache.spark.sql.DataFrameJoinSuite</className>
          <testName>process outer join results using the non-nullable columns in the join input</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.283</duration>
          <className>org.apache.spark.sql.DataFrameJoinSuite</className>
          <testName>SPARK-16991: Full outer join followed by inner join produces wrong results</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.DataFrameNaFunctionsSuite.xml</file>
      <name>org.apache.spark.sql.DataFrameNaFunctionsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.98300004</duration>
      <cases>
        <case>
          <duration>0.302</duration>
          <className>org.apache.spark.sql.DataFrameNaFunctionsSuite</className>
          <testName>drop</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.198</duration>
          <className>org.apache.spark.sql.DataFrameNaFunctionsSuite</className>
          <testName>drop with how</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.093</duration>
          <className>org.apache.spark.sql.DataFrameNaFunctionsSuite</className>
          <testName>drop with threshold</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.17</duration>
          <className>org.apache.spark.sql.DataFrameNaFunctionsSuite</className>
          <testName>fill</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.187</duration>
          <className>org.apache.spark.sql.DataFrameNaFunctionsSuite</className>
          <testName>fill with map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.033</duration>
          <className>org.apache.spark.sql.DataFrameNaFunctionsSuite</className>
          <testName>replace</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.DataFramePivotSuite.xml</file>
      <name>org.apache.spark.sql.DataFramePivotSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>5.26</duration>
      <cases>
        <case>
          <duration>0.43</duration>
          <className>org.apache.spark.sql.DataFramePivotSuite</className>
          <testName>pivot courses</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.382</duration>
          <className>org.apache.spark.sql.DataFramePivotSuite</className>
          <testName>pivot year</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.396</duration>
          <className>org.apache.spark.sql.DataFramePivotSuite</className>
          <testName>pivot courses with multiple aggregations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.271</duration>
          <className>org.apache.spark.sql.DataFramePivotSuite</className>
          <testName>pivot year with string values (cast)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.281</duration>
          <className>org.apache.spark.sql.DataFramePivotSuite</className>
          <testName>pivot year with int values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.459</duration>
          <className>org.apache.spark.sql.DataFramePivotSuite</className>
          <testName>pivot courses with no values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.442</duration>
          <className>org.apache.spark.sql.DataFramePivotSuite</className>
          <testName>pivot year with no values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.124</duration>
          <className>org.apache.spark.sql.DataFramePivotSuite</className>
          <testName>pivot max values enforced</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.275</duration>
          <className>org.apache.spark.sql.DataFramePivotSuite</className>
          <testName>pivot with UnresolvedFunction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.DataFramePivotSuite</className>
          <testName>optimized pivot planned</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.301</duration>
          <className>org.apache.spark.sql.DataFramePivotSuite</className>
          <testName>optimized pivot courses with literals</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.287</duration>
          <className>org.apache.spark.sql.DataFramePivotSuite</className>
          <testName>optimized pivot year with literals</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.285</duration>
          <className>org.apache.spark.sql.DataFramePivotSuite</className>
          <testName>optimized pivot year with string values (cast)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.406</duration>
          <className>org.apache.spark.sql.DataFramePivotSuite</className>
          <testName>optimized pivot DecimalType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.DataFramePivotSuite</className>
          <testName>PivotFirst supported datatypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.399</duration>
          <className>org.apache.spark.sql.DataFramePivotSuite</className>
          <testName>optimized pivot with multiple aggregations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.222</duration>
          <className>org.apache.spark.sql.DataFramePivotSuite</className>
          <testName>pivot with datatype not supported by PivotFirst</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.269</duration>
          <className>org.apache.spark.sql.DataFramePivotSuite</className>
          <testName>pivot with datatype not supported by PivotFirst 2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.DataFramePivotSuite</className>
          <testName>pivot preserves aliases if given</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.DataFrameStatPerfSuite.xml</file>
      <name>org.apache.spark.sql.DataFrameStatPerfSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>-0.001</duration>
      <cases>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.DataFrameStatPerfSuite</className>
          <testName>computing quantiles should not take much longer than describe()</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.DataFrameStatSuite.xml</file>
      <name>org.apache.spark.sql.DataFrameStatSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>5.054</duration>
      <cases>
        <case>
          <duration>0.075</duration>
          <className>org.apache.spark.sql.DataFrameStatSuite</className>
          <testName>sample with replacement</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.049</duration>
          <className>org.apache.spark.sql.DataFrameStatSuite</className>
          <testName>sample without replacement</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.023</duration>
          <className>org.apache.spark.sql.DataFrameStatSuite</className>
          <testName>randomSplit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.439</duration>
          <className>org.apache.spark.sql.DataFrameStatSuite</className>
          <testName>randomSplit on reordered partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.103</duration>
          <className>org.apache.spark.sql.DataFrameStatSuite</className>
          <testName>pearson correlation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.1</duration>
          <className>org.apache.spark.sql.DataFrameStatSuite</className>
          <testName>covariance</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.544</duration>
          <className>org.apache.spark.sql.DataFrameStatSuite</className>
          <testName>approximate quantile</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.233</duration>
          <className>org.apache.spark.sql.DataFrameStatSuite</className>
          <testName>crosstab</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.696</duration>
          <className>org.apache.spark.sql.DataFrameStatSuite</className>
          <testName>, &apos;&apos;, null, ``)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.178</duration>
          <className>org.apache.spark.sql.DataFrameStatSuite</className>
          <testName>Frequent Items</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.064</duration>
          <className>org.apache.spark.sql.DataFrameStatSuite</className>
          <testName>Frequent Items 2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.DataFrameStatSuite</className>
          <testName>min` in `freqItems`</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.328</duration>
          <className>org.apache.spark.sql.DataFrameStatSuite</className>
          <testName>sampleBy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.112</duration>
          <className>org.apache.spark.sql.DataFrameStatSuite</className>
          <testName>countMinSketch</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.105</duration>
          <className>org.apache.spark.sql.DataFrameStatSuite</className>
          <testName>Bloom filter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.DataFrameSuite.xml</file>
      <name>org.apache.spark.sql.DataFrameSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>29.202997</duration>
      <cases>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>analysis error should be eagerly reported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>dataframe toString</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.215</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>rename nested groupby</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.202</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>access complex data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.074</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>table scan</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.18</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>union all</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.185</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>union should union DataFrames with UDTs (SPARK-13410)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>empty data frame</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.095</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>head and take</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>dataframe alias</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.071</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>simple explode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.47</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>explode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.455</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>Star Expansion - CreateStruct and CreateArray</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.687</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>Star Expansion - hash</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.083</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>Star Expansion - explode should fail with a meaningful message if it takes a star</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.106</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>Star Expansion - explode alias and star</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.114</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>sort after generate with join=true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.071</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>selectExpr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.094</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>selectExpr with alias</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.092</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>selectExpr with udtf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.305</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>filterExpr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.065</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>filterExpr using where</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.122</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>repartition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.076</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>coalesce</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.056</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>convert $&quot;attribute name&quot; into unresolved attribute</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>convert Scala Symbol &apos;attrname into unresolved attribute</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>select *</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.035</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>simple select</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.371</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>select with functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.376</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>sorting with null ordering</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.81</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>global sorting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.5</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>limit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.329</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>except</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.18</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>except distinct - SQL compliance</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.907</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>except - nullability</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.475</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>intersect</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.892</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>intersect - nullability</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.104</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>udf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.041</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>callUDF without Hive Support</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.099</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>withColumn</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.046</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>replace column using withColumn</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.073</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>drop column using drop</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>drop columns using drop</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>drop unknown column (no-op)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.058</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>drop column using drop with column reference</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.053</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>drop unknown column (no-op) with column reference</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.065</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>drop unknown column with same name with column reference</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.391</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>drop column after join with duplicate columns using column reference</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.073</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>drop top level columns that contains dot</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.166</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>drop(name: String) search and drop all top level columns that matchs the name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.061</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>withColumnRenamed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.787</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>describe</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.064</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>apply on query results (SPARK-5462)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.224</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>inputFiles</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>show</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.06</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>showString: truncate = [0, 20]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.052</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>showString: truncate = [3, 17]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>showString(negative)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>showString(0)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.075</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>showString: array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>showString: binary</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>showString: minimum column width</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-7319 showString</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.039</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-7327 show with empty dataFrame</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.038</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>createDataFrame(RDD[Row], StructType) should convert UDTs (SPARK-6672)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.103</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-6899: type should match when using codegen</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.288</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-7133: Implement struct, array, and map field accessor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.137</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-7551: support backticks for DataFrame attribute resolution</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.339</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-7324 dropDuplicates</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.536</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-7150 range api</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-8621: support empty string column name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.167</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-8797: sort by float column containing NaN should not crash</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.146</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-8797: sort by double column containing NaN should not crash</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.146</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>NaN is greater than all other non-NaN numeric values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.054</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-8072: Better Exception for Duplicate Columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.326</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-6941: Better error message for inserting into RDD-based Table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.06</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-8608: call `show` on local DataFrame with random columns should return same value</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.587</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-8609: local DataFrame with random columns should return same value after sort</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.081</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-9083: sort with non-deterministic expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.211</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>Sorting columns are not in Filter and Project</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.131</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>orderBy should support nested column name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.262</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-9950: correctly analyze grouping/aggregating on struct fields</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.119</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-10093: Avoid transformations on executors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.321</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-10185: Read multiple Hadoop Filesystem paths and paths with a comma in it</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.62</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>Alias uses internally generated names &apos;aggOrder&apos; and &apos;havingCondition&apos;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.223</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-10316: respect non-deterministic expressions in PhysicalOperation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.289</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-10539: Project should not be pushed down through Intersect or Except</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.637</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-10740: handle nondeterministic expressions correctly for set operations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-10743: keep the name of expression if possible when do cast</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.258</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-11301: fix case sensitivity for filter on partitioned columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.509</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>distributeBy and localSort</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.183</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>fix case sensitivity of partition by</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.211</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-11633: LogicalRDD throws TreeNode Exception: Failed to Copy Node</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.053</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-10656: completely support special chars</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.209</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-11725: correctly handle null inputs for ScalaUDF</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.36</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-12398 truncated toString</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.094</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>reuse exchange</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>sameResult() on aggregate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.111</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>` in column name for withColumn()</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.055</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-12841: cast in filter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-12982: Add table name validation in temp table registration</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>assertAnalyzed shouldn&apos;t replace original stack trace</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-13774: Check error message for non existent path without globbed paths</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-13774: Check error message for not existent globbed paths</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.104</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-15230: distinct() does not handle column name with dot properly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.203</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-16181: outer join with isNull filter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.188</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-16664: persist with more than 200 columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.248</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-17409: Do Not Optimize Query in CTAS (Data source tables) More Than Once</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>copy results for sampling with replacement</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.096</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-17625: data source table in InMemoryCatalog should guarantee output consistency</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-17957: no change on nullability in FilterExec output</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.05</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-17957: set nullability to false in FilterExec output</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.319</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>fill</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.287</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-17123: Performing set operations that combine non-scala native types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.1</duration>
          <className>org.apache.spark.sql.DataFrameSuite</className>
          <testName>SPARK-18070 binary operator should not consider nullability when comparing input types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.DataFrameTimeWindowingSuite.xml</file>
      <name>org.apache.spark.sql.DataFrameTimeWindowingSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>4.0239997</duration>
      <cases>
        <case>
          <duration>0.444</duration>
          <className>org.apache.spark.sql.DataFrameTimeWindowingSuite</className>
          <testName>tumbling window groupBy statement</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.359</duration>
          <className>org.apache.spark.sql.DataFrameTimeWindowingSuite</className>
          <testName>tumbling window groupBy statement with startTime</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.242</duration>
          <className>org.apache.spark.sql.DataFrameTimeWindowingSuite</className>
          <testName>tumbling window with multi-column projection</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.508</duration>
          <className>org.apache.spark.sql.DataFrameTimeWindowingSuite</className>
          <testName>sliding window grouping</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.3</duration>
          <className>org.apache.spark.sql.DataFrameTimeWindowingSuite</className>
          <testName>sliding window projection</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.262</duration>
          <className>org.apache.spark.sql.DataFrameTimeWindowingSuite</className>
          <testName>windowing combined with explode expression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.143</duration>
          <className>org.apache.spark.sql.DataFrameTimeWindowingSuite</className>
          <testName>null timestamps</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.526</duration>
          <className>org.apache.spark.sql.DataFrameTimeWindowingSuite</className>
          <testName>time window joins</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.192</duration>
          <className>org.apache.spark.sql.DataFrameTimeWindowingSuite</className>
          <testName>negative timestamps</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.sql.DataFrameTimeWindowingSuite</className>
          <testName>multiple time windows in a single operator throws nice exception</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.209</duration>
          <className>org.apache.spark.sql.DataFrameTimeWindowingSuite</className>
          <testName>aliased windows</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.521</duration>
          <className>org.apache.spark.sql.DataFrameTimeWindowingSuite</className>
          <testName>millisecond precision sliding windows</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.105</duration>
          <className>org.apache.spark.sql.DataFrameTimeWindowingSuite</className>
          <testName>time window in SQL with single string expression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.089</duration>
          <className>org.apache.spark.sql.DataFrameTimeWindowingSuite</className>
          <testName>time window in SQL with with two expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.111</duration>
          <className>org.apache.spark.sql.DataFrameTimeWindowingSuite</className>
          <testName>time window in SQL with with three expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.DataFrameTungstenSuite.xml</file>
      <name>org.apache.spark.sql.DataFrameTungstenSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.286</duration>
      <cases>
        <case>
          <duration>0.09</duration>
          <className>org.apache.spark.sql.DataFrameTungstenSuite</className>
          <testName>test simple types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.088</duration>
          <className>org.apache.spark.sql.DataFrameTungstenSuite</className>
          <testName>test struct type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.108</duration>
          <className>org.apache.spark.sql.DataFrameTungstenSuite</className>
          <testName>test nested struct type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.DataFrameWindowFunctionsSuite.xml</file>
      <name>org.apache.spark.sql.DataFrameWindowFunctionsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>5.1750007</duration>
      <cases>
        <case>
          <duration>0.275</duration>
          <className>org.apache.spark.sql.DataFrameWindowFunctionsSuite</className>
          <testName>reuse window partitionBy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.141</duration>
          <className>org.apache.spark.sql.DataFrameWindowFunctionsSuite</className>
          <testName>reuse window orderBy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.166</duration>
          <className>org.apache.spark.sql.DataFrameWindowFunctionsSuite</className>
          <testName>rowsBetween</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.187</duration>
          <className>org.apache.spark.sql.DataFrameWindowFunctionsSuite</className>
          <testName>lead</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.165</duration>
          <className>org.apache.spark.sql.DataFrameWindowFunctionsSuite</className>
          <testName>lag</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.137</duration>
          <className>org.apache.spark.sql.DataFrameWindowFunctionsSuite</className>
          <testName>lead with default value</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.131</duration>
          <className>org.apache.spark.sql.DataFrameWindowFunctionsSuite</className>
          <testName>lag with default value</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.367</duration>
          <className>org.apache.spark.sql.DataFrameWindowFunctionsSuite</className>
          <testName>rank functions in unspecific window</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.DataFrameWindowFunctionsSuite</className>
          <testName>window function should fail if order by clause is not specified</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.17</duration>
          <className>org.apache.spark.sql.DataFrameWindowFunctionsSuite</className>
          <testName>aggregation and rows between</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.179</duration>
          <className>org.apache.spark.sql.DataFrameWindowFunctionsSuite</className>
          <testName>aggregation and range between</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.202</duration>
          <className>org.apache.spark.sql.DataFrameWindowFunctionsSuite</className>
          <testName>aggregation and rows between with unbounded</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.267</duration>
          <className>org.apache.spark.sql.DataFrameWindowFunctionsSuite</className>
          <testName>aggregation and range between with unbounded</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.259</duration>
          <className>org.apache.spark.sql.DataFrameWindowFunctionsSuite</className>
          <testName>reverse sliding range frame</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.165</duration>
          <className>org.apache.spark.sql.DataFrameWindowFunctionsSuite</className>
          <testName>reverse unbounded range frame</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.305</duration>
          <className>org.apache.spark.sql.DataFrameWindowFunctionsSuite</className>
          <testName>statistical functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.342</duration>
          <className>org.apache.spark.sql.DataFrameWindowFunctionsSuite</className>
          <testName>window function with aggregates</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.267</duration>
          <className>org.apache.spark.sql.DataFrameWindowFunctionsSuite</className>
          <testName>SPARK-16195 empty over spec</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.271</duration>
          <className>org.apache.spark.sql.DataFrameWindowFunctionsSuite</className>
          <testName>window function with udaf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.124</duration>
          <className>org.apache.spark.sql.DataFrameWindowFunctionsSuite</className>
          <testName>null inputs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.273</duration>
          <className>org.apache.spark.sql.DataFrameWindowFunctionsSuite</className>
          <testName>last/first with ignoreNulls</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.241</duration>
          <className>org.apache.spark.sql.DataFrameWindowFunctionsSuite</className>
          <testName>SPARK-12989 ExtractWindowExpressions treats alias as regular attribute</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.261</duration>
          <className>org.apache.spark.sql.DataFrameWindowFunctionsSuite</className>
          <testName>aggregation and rows between with unbounded + predicate pushdown</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.266</duration>
          <className>org.apache.spark.sql.DataFrameWindowFunctionsSuite</className>
          <testName>aggregation and range between with unbounded + predicate pushdown</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.DatasetAggregatorSuite.xml</file>
      <name>org.apache.spark.sql.DatasetAggregatorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>4.097</duration>
      <cases>
        <case>
          <duration>0.259</duration>
          <className>org.apache.spark.sql.DatasetAggregatorSuite</className>
          <testName>typed aggregation: TypedAggregator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.175</duration>
          <className>org.apache.spark.sql.DatasetAggregatorSuite</className>
          <testName>typed aggregation: TypedAggregator, expr, expr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.213</duration>
          <className>org.apache.spark.sql.DatasetAggregatorSuite</className>
          <testName>typed aggregation: complex result type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.159</duration>
          <className>org.apache.spark.sql.DatasetAggregatorSuite</className>
          <testName>typed aggregation: in project list</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.09</duration>
          <className>org.apache.spark.sql.DatasetAggregatorSuite</className>
          <testName>typed aggregation: class input</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.288</duration>
          <className>org.apache.spark.sql.DatasetAggregatorSuite</className>
          <testName>typed aggregation: class input with reordering</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.134</duration>
          <className>org.apache.spark.sql.DatasetAggregatorSuite</className>
          <testName>Typed aggregation using aggregator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.583</duration>
          <className>org.apache.spark.sql.DatasetAggregatorSuite</className>
          <testName>typed aggregation: complex input</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.24</duration>
          <className>org.apache.spark.sql.DatasetAggregatorSuite</className>
          <testName>typed aggregate: avg, count, sum</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.253</duration>
          <className>org.apache.spark.sql.DatasetAggregatorSuite</className>
          <testName>generic typed sum</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.231</duration>
          <className>org.apache.spark.sql.DatasetAggregatorSuite</className>
          <testName>SPARK-12555 - result should not be corrupted after input columns are reordered</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.208</duration>
          <className>org.apache.spark.sql.DatasetAggregatorSuite</className>
          <testName>aggregator in DataFrame/Dataset[Row]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.209</duration>
          <className>org.apache.spark.sql.DatasetAggregatorSuite</className>
          <testName>SPARK-14675: ClassFormatError when use Seq as Aggregator buffer type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.219</duration>
          <className>org.apache.spark.sql.DatasetAggregatorSuite</className>
          <testName>spark-15051 alias of aggregator in DataFrame/Dataset[Row]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.046</duration>
          <className>org.apache.spark.sql.DatasetAggregatorSuite</className>
          <testName>spark-15114 shorter system generated alias names</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.253</duration>
          <className>org.apache.spark.sql.DatasetAggregatorSuite</className>
          <testName>SPARK-15814 Aggregator can return null result</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.188</duration>
          <className>org.apache.spark.sql.DatasetAggregatorSuite</className>
          <testName>SPARK-16100: use Map as the buffer type of Aggregator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.sql.DatasetAggregatorSuite</className>
          <testName>SPARK-15204 improve nullability inference for Aggregator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.306</duration>
          <className>org.apache.spark.sql.DatasetAggregatorSuite</className>
          <testName>SPARK-18147: very complex aggregator result type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.DatasetCacheSuite.xml</file>
      <name>org.apache.spark.sql.DatasetCacheSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.597</duration>
      <cases>
        <case>
          <duration>0.058</duration>
          <className>org.apache.spark.sql.DatasetCacheSuite</className>
          <testName>get storage level</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.12</duration>
          <className>org.apache.spark.sql.DatasetCacheSuite</className>
          <testName>persist and unpersist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.222</duration>
          <className>org.apache.spark.sql.DatasetCacheSuite</className>
          <testName>persist and then rebind right encoder when join 2 datasets</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.197</duration>
          <className>org.apache.spark.sql.DatasetCacheSuite</className>
          <testName>persist and then groupBy columns asKey, map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.DatasetPrimitiveSuite.xml</file>
      <name>org.apache.spark.sql.DatasetPrimitiveSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.265</duration>
      <cases>
        <case>
          <duration>0.041</duration>
          <className>org.apache.spark.sql.DatasetPrimitiveSuite</className>
          <testName>toDS</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.018</duration>
          <className>org.apache.spark.sql.DatasetPrimitiveSuite</className>
          <testName>as case class / collect</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.045</duration>
          <className>org.apache.spark.sql.DatasetPrimitiveSuite</className>
          <testName>map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.sql.DatasetPrimitiveSuite</className>
          <testName>filter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.sql.DatasetPrimitiveSuite</className>
          <testName>foreach</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.sql.DatasetPrimitiveSuite</className>
          <testName>foreachPartition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.sql.DatasetPrimitiveSuite</className>
          <testName>reduce</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.125</duration>
          <className>org.apache.spark.sql.DatasetPrimitiveSuite</className>
          <testName>groupBy function, keys</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.105</duration>
          <className>org.apache.spark.sql.DatasetPrimitiveSuite</className>
          <testName>groupBy function, map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.119</duration>
          <className>org.apache.spark.sql.DatasetPrimitiveSuite</className>
          <testName>groupBy function, flatMap</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.694</duration>
          <className>org.apache.spark.sql.DatasetPrimitiveSuite</className>
          <testName>Arrays and Lists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.sql.DatasetPrimitiveSuite</className>
          <testName>package objects</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.DatasetSerializerRegistratorSuite.xml</file>
      <name>org.apache.spark.sql.DatasetSerializerRegistratorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.048</duration>
      <cases>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.sql.DatasetSerializerRegistratorSuite</className>
          <testName>Kryo registrator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.DatasetSuite.xml</file>
      <name>org.apache.spark.sql.DatasetSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>12.751999</duration>
      <cases>
        <case>
          <duration>0.128</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>checkAnswer should compare map correctly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.032</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>toDS</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.055</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>toDS with RDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.072</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>emptyDataset</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.189</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>range</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.068</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>SPARK-12404: Datatype Helper Serializability</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.063</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>collect, first, and take should use encoders for serialization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.154</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>coalesce, repartition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.039</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>as tuple</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>as case class / collect</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.045</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>as case class - reordered fields by name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>as case class - take</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.085</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.14</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>map with type change with the exact matched number of attributes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.118</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>map with type change with less attributes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.223</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>map and group by with class data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>select</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.183</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>SPARK-16853: select, case class and tuple</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>select 2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.055</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>select 2, primitive and tuple</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.061</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>select 2, primitive and class</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.06</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>select 2, primitive and class, fields reordered</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>filter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.059</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>filter and then select</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>SPARK-15632: typed filter should preserve the underlying logical schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>foreach</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>foreachPartition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>reduce</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.061</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>joinWith, flat schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.088</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>joinWith tuple with primitive, expression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.1</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>joinWith class with primitive, toDF</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.134</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>multi-level joinWith</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.144</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>groupBy function, keys</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.21</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>groupBy function, map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.104</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>groupBy function, flatMap</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.221</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>groupBy function, mapValues, flatMap</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.216</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>groupBy function, reduce</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.176</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>groupBy single field class, count</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.17</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>typed aggregation: expr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.184</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>typed aggregation: expr, expr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.189</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>typed aggregation: expr, expr, expr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.195</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>typed aggregation: expr, expr, expr, expr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.205</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>cogroup</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.197</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>cogroup with complex data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.088</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>sample with replacement</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.035</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>sample without replacement</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.182</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>sample with seed results shouldn&apos;t depend on downstream usage</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.063</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>SPARK-11436: we should rebind right encoder when join 2 datasets</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>self join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>toString</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.326</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>Kryo encoder</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.075</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>Kryo encoder self join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>Kryo encoder: check the schema mismatch when converting DataFrame to Dataset</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.129</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>Java encoder</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.041</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>Java encoder self join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.061</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>SPARK-14696: implicit encoders for boxed types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.108</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>SPARK-11894: Incorrect results are returned when using null</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.04</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>change encoder with compatible schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>verify mismatching field names fail with a good error</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.173</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>runtime nullability check</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.166</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>SPARK-12478: top level null field</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.087</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>support inner class in Dataset</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.162</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>grouping key and grouped value has field with same name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.152</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>cogroup&apos;s left and right side has field with same name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>give nice error message when the real number of fields doesn&apos;t match encoder schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.045</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>SPARK-13440: Resolving option fields</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.039</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>SPARK-13540 Dataset of nested class defined in Scala object</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.065</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>SPARK-14000: case class with tuple type field</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>isStreaming returns false for static Dataset</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>isStreaming returns true for streaming Dataset</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>isStreaming returns true after static and streaming Dataset join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.299</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>map may generate wrong java code for wide table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.232</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>SPARK-14838: estimating sizeInBytes in operators with ObjectProducer shouldn&apos;t fail</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.04</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>SPARK-15097: implicits on dataset&apos;s spark can be imported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.1</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>rdd with generic case class</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.039</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>runtime null check for RowEncoder</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.032</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>row nullability mismatch</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>createTempView</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.084</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>SPARK-15381: physical object operator should define `reference` correctly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.05</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>show() should show contents of the underlying logical plan</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>show() should show inner nested products as rows</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.078</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>SPARK-15112: EmbedDeserializerInFilter should not optimize plan fragment that changes schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.371</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>mapped dataset should resolve duplicated attributes for self join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.094</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>SPARK-15441: Dataset outer join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>better error message when use java reserved keyword as field name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>Dataset should support flat input object to be null</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>Dataset should throw RuntimeException if non-flat input object is null</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.396</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>dropDuplicates</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.168</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>dropDuplicates: columns with same column name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.084</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>dropDuplicates should not change child plan output</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>tuple should handle null object correctly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.278</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>SPARK-16995: flat mapping on Dataset containing a column created with lit/expr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.582</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>SPARK-18125: Spark generated code causes CompileException</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.133</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>SPARK-18189: Fix serialization issue in KeyValueGroupedDataset</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.333</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>checkpoint() - basic (eager = true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.493</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>checkpoint() - should preserve partitioning information (eager = true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.202</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>checkpoint() - basic (eager = false)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.415</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>checkpoint() - should preserve partitioning information (eager = false)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.244</duration>
          <className>org.apache.spark.sql.DatasetSuite</className>
          <testName>identity map for primitive arrays</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.DateFunctionsSuite.xml</file>
      <name>org.apache.spark.sql.DateFunctionsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.887</duration>
      <cases>
        <case>
          <duration>0.062</duration>
          <className>org.apache.spark.sql.DateFunctionsSuite</className>
          <testName>function current_date</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.306</duration>
          <className>org.apache.spark.sql.DateFunctionsSuite</className>
          <testName>function current_timestamp and now</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.142</duration>
          <className>org.apache.spark.sql.DateFunctionsSuite</className>
          <testName>timestamp comparison with date strings</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.158</duration>
          <className>org.apache.spark.sql.DateFunctionsSuite</className>
          <testName>date comparison with date strings</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.091</duration>
          <className>org.apache.spark.sql.DateFunctionsSuite</className>
          <testName>date format</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.072</duration>
          <className>org.apache.spark.sql.DateFunctionsSuite</className>
          <testName>year</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.054</duration>
          <className>org.apache.spark.sql.DateFunctionsSuite</className>
          <testName>quarter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.054</duration>
          <className>org.apache.spark.sql.DateFunctionsSuite</className>
          <testName>month</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.052</duration>
          <className>org.apache.spark.sql.DateFunctionsSuite</className>
          <testName>dayofmonth</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.052</duration>
          <className>org.apache.spark.sql.DateFunctionsSuite</className>
          <testName>dayofyear</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.054</duration>
          <className>org.apache.spark.sql.DateFunctionsSuite</className>
          <testName>hour</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.053</duration>
          <className>org.apache.spark.sql.DateFunctionsSuite</className>
          <testName>minute</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.061</duration>
          <className>org.apache.spark.sql.DateFunctionsSuite</className>
          <testName>second</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.06</duration>
          <className>org.apache.spark.sql.DateFunctionsSuite</className>
          <testName>weekofyear</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.169</duration>
          <className>org.apache.spark.sql.DateFunctionsSuite</className>
          <testName>function date_add</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.177</duration>
          <className>org.apache.spark.sql.DateFunctionsSuite</className>
          <testName>function date_sub</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.076</duration>
          <className>org.apache.spark.sql.DateFunctionsSuite</className>
          <testName>time_add</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.055</duration>
          <className>org.apache.spark.sql.DateFunctionsSuite</className>
          <testName>time_sub</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.049</duration>
          <className>org.apache.spark.sql.DateFunctionsSuite</className>
          <testName>function add_months</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.081</duration>
          <className>org.apache.spark.sql.DateFunctionsSuite</className>
          <testName>function months_between</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.066</duration>
          <className>org.apache.spark.sql.DateFunctionsSuite</className>
          <testName>function last_day</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.068</duration>
          <className>org.apache.spark.sql.DateFunctionsSuite</className>
          <testName>function next_day</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.145</duration>
          <className>org.apache.spark.sql.DateFunctionsSuite</className>
          <testName>function to_date</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.046</duration>
          <className>org.apache.spark.sql.DateFunctionsSuite</className>
          <testName>function trunc</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.125</duration>
          <className>org.apache.spark.sql.DateFunctionsSuite</className>
          <testName>from_unixtime</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.245</duration>
          <className>org.apache.spark.sql.DateFunctionsSuite</className>
          <testName>unix_timestamp</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.091</duration>
          <className>org.apache.spark.sql.DateFunctionsSuite</className>
          <testName>to_unix_timestamp</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.112</duration>
          <className>org.apache.spark.sql.DateFunctionsSuite</className>
          <testName>datediff</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.061</duration>
          <className>org.apache.spark.sql.DateFunctionsSuite</className>
          <testName>from_utc_timestamp</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.05</duration>
          <className>org.apache.spark.sql.DateFunctionsSuite</className>
          <testName>to_utc_timestamp</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.ExtraStrategiesSuite.xml</file>
      <name>org.apache.spark.sql.ExtraStrategiesSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.128</duration>
      <cases>
        <case>
          <duration>0.128</duration>
          <className>org.apache.spark.sql.ExtraStrategiesSuite</className>
          <testName>insert an extraStrategy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.GeneratorFunctionSuite.xml</file>
      <name>org.apache.spark.sql.GeneratorFunctionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.667</duration>
      <cases>
        <case>
          <duration>0.501</duration>
          <className>org.apache.spark.sql.GeneratorFunctionSuite</className>
          <testName>stack</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.077</duration>
          <className>org.apache.spark.sql.GeneratorFunctionSuite</className>
          <testName>single explode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.059</duration>
          <className>org.apache.spark.sql.GeneratorFunctionSuite</className>
          <testName>single posexplode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.131</duration>
          <className>org.apache.spark.sql.GeneratorFunctionSuite</className>
          <testName>explode and other columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.144</duration>
          <className>org.apache.spark.sql.GeneratorFunctionSuite</className>
          <testName>aliased explode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.114</duration>
          <className>org.apache.spark.sql.GeneratorFunctionSuite</className>
          <testName>explode on map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.07</duration>
          <className>org.apache.spark.sql.GeneratorFunctionSuite</className>
          <testName>explode on map with aliases</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.152</duration>
          <className>org.apache.spark.sql.GeneratorFunctionSuite</className>
          <testName>self join explode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.GeneratorFunctionSuite</className>
          <testName>inline raises exception on array of null type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.057</duration>
          <className>org.apache.spark.sql.GeneratorFunctionSuite</className>
          <testName>inline with empty table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.sql.GeneratorFunctionSuite</className>
          <testName>inline on literal</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.308</duration>
          <className>org.apache.spark.sql.GeneratorFunctionSuite</className>
          <testName>inline on column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.JoinSuite.xml</file>
      <name>org.apache.spark.sql.JoinSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>8.929999</duration>
      <cases>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.JoinSuite</className>
          <testName>equi-join is hash-join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.165</duration>
          <className>org.apache.spark.sql.JoinSuite</className>
          <testName>join operator selection</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.115</duration>
          <className>org.apache.spark.sql.JoinSuite</className>
          <testName>broadcasted hash join operator selection</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.111</duration>
          <className>org.apache.spark.sql.JoinSuite</className>
          <testName>broadcasted hash outer join operator selection</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.JoinSuite</className>
          <testName>multiple-key equi-join is hash-join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.283</duration>
          <className>org.apache.spark.sql.JoinSuite</className>
          <testName>inner join where, one match per row</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.205</duration>
          <className>org.apache.spark.sql.JoinSuite</className>
          <testName>inner join ON, one match per row</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.139</duration>
          <className>org.apache.spark.sql.JoinSuite</className>
          <testName>inner join, where, multiple matches</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.141</duration>
          <className>org.apache.spark.sql.JoinSuite</className>
          <testName>inner join, no matches</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.421</duration>
          <className>org.apache.spark.sql.JoinSuite</className>
          <testName>big inner join, 4 matches per row</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.117</duration>
          <className>org.apache.spark.sql.JoinSuite</className>
          <testName>cartesian product join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.846</duration>
          <className>org.apache.spark.sql.JoinSuite</className>
          <testName>left outer join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.707</duration>
          <className>org.apache.spark.sql.JoinSuite</className>
          <testName>right outer join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.642</duration>
          <className>org.apache.spark.sql.JoinSuite</className>
          <testName>full outer join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.082</duration>
          <className>org.apache.spark.sql.JoinSuite</className>
          <testName>broadcasted existence join operator selection</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.535</duration>
          <className>org.apache.spark.sql.JoinSuite</className>
          <testName>cross join with broadcast</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.273</duration>
          <className>org.apache.spark.sql.JoinSuite</className>
          <testName>left semi join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.123</duration>
          <className>org.apache.spark.sql.JoinSuite</className>
          <testName>cross join detection</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.JsonFunctionsSuite.xml</file>
      <name>org.apache.spark.sql.JsonFunctionsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.806</duration>
      <cases>
        <case>
          <duration>0.083</duration>
          <className>org.apache.spark.sql.JsonFunctionsSuite</className>
          <testName>function get_json_object</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.065</duration>
          <className>org.apache.spark.sql.JsonFunctionsSuite</className>
          <testName>function get_json_object - null</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.139</duration>
          <className>org.apache.spark.sql.JsonFunctionsSuite</className>
          <testName>json_tuple select</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.244</duration>
          <className>org.apache.spark.sql.JsonFunctionsSuite</className>
          <testName>json_tuple filter and group</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.052</duration>
          <className>org.apache.spark.sql.JsonFunctionsSuite</className>
          <testName>from_json</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.sql.JsonFunctionsSuite</className>
          <testName>from_json missing columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.sql.JsonFunctionsSuite</className>
          <testName>from_json invalid json</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.046</duration>
          <className>org.apache.spark.sql.JsonFunctionsSuite</className>
          <testName>to_json</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.sql.JsonFunctionsSuite</className>
          <testName>to_json unsupported type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.098</duration>
          <className>org.apache.spark.sql.JsonFunctionsSuite</className>
          <testName>roundtrip in to_json and from_json</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.MathFunctionsSuite.xml</file>
      <name>org.apache.spark.sql.MathFunctionsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>4.2140007</duration>
      <cases>
        <case>
          <duration>0.115</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>sin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.049</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>asin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>sinh</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>cos</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.072</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>acos</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.052</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>cosh</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>tan</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>atan</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.044</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>tanh</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.125</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>degrees</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.105</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>radians</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.056</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>cbrt</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.143</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>ceil and ceiling</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.178</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>conv</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.051</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>floor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.049</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>factorial</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.051</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>rint</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.209</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>round/bround</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.059</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>exp</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.059</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>expm1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.11</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>signum / sign</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.386</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>pow / power</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.216</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>hex</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.154</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>unhex</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.239</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>hypot</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.204</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>atan2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.135</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>log / ln</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>log10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.054</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>log1p</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.105</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>shift left</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.078</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>shift right</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.073</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>shift right unsigned</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.068</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>binary log</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.445</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>abs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.076</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>log2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.089</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>sqrt</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.051</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>negative</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.09</duration>
          <className>org.apache.spark.sql.MathFunctionsSuite</className>
          <testName>positive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.MetadataCacheSuite.xml</file>
      <name>org.apache.spark.sql.MetadataCacheSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.98599994</duration>
      <cases>
        <case>
          <duration>0.38</duration>
          <className>org.apache.spark.sql.MetadataCacheSuite</className>
          <testName>SPARK-16336 Suggest doing table refresh when encountering FileNotFoundException</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.341</duration>
          <className>org.apache.spark.sql.MetadataCacheSuite</className>
          <testName>SPARK-16337 temporary view refresh</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.265</duration>
          <className>org.apache.spark.sql.MetadataCacheSuite</className>
          <testName>case sensitivity support in temporary view refresh</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.MiscFunctionsSuite.xml</file>
      <name>org.apache.spark.sql.MiscFunctionsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.062</duration>
      <cases>
        <case>
          <duration>0.062</duration>
          <className>org.apache.spark.sql.MiscFunctionsSuite</className>
          <testName>reflect and java_method</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.ProcessingTimeSuite.xml</file>
      <name>org.apache.spark.sql.ProcessingTimeSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.002</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.ProcessingTimeSuite</className>
          <testName>create</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.QueryTestSuite.xml</file>
      <name>org.apache.spark.sql.QueryTestSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.071</duration>
      <cases>
        <case>
          <duration>0.071</duration>
          <className>org.apache.spark.sql.QueryTestSuite</className>
          <testName>SPARK-16940: checkAnswer should raise TestFailedException for wrong results</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.RowSuite.xml</file>
      <name>org.apache.spark.sql.RowSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.074</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RowSuite</className>
          <testName>create row</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RowSuite</className>
          <testName>update with null</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.071</duration>
          <className>org.apache.spark.sql.RowSuite</className>
          <testName>toDF</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RowSuite</className>
          <testName>float NaN == NaN</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RowSuite</className>
          <testName>double NaN == NaN</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RowSuite</className>
          <testName>equals and hashCode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.RuntimeConfigSuite.xml</file>
      <name>org.apache.spark.sql.RuntimeConfigSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.002</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RuntimeConfigSuite</className>
          <testName>set and get</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.RuntimeConfigSuite</className>
          <testName>getOption</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.RuntimeConfigSuite</className>
          <testName>unset</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.SQLContextSuite.xml</file>
      <name>org.apache.spark.sql.SQLContextSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.478</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.SQLContextSuite</className>
          <testName>getOrCreate instantiates SQLContext</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.sql.SQLContextSuite</className>
          <testName>getOrCreate return the original SQLContext</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.083</duration>
          <className>org.apache.spark.sql.SQLContextSuite</className>
          <testName>Sessions of SQLContext</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.SQLContextSuite</className>
          <testName>Catalyst optimization passes are modifiable at runtime</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.127</duration>
          <className>org.apache.spark.sql.SQLContextSuite</className>
          <testName>get all tables</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.085</duration>
          <className>org.apache.spark.sql.SQLContextSuite</className>
          <testName>getting all tables with a database name has no impact on returned table names</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.165</duration>
          <className>org.apache.spark.sql.SQLContextSuite</className>
          <testName>query the returned DataFrame of tables</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.SQLQuerySuite.xml</file>
      <name>org.apache.spark.sql.SQLQuerySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>40.391</duration>
      <cases>
        <case>
          <duration>0.076</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-8010: promote numeric to string</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.112</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>show functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>describe functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.697</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-14415: All functions should have own descriptions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.244</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-6743: no columns from cache</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.232</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>self join with aliases</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.28</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>star</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.448</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>self join with alias in agg</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.208</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-8668 expr function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.068</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-4625 support SORT BY in SimpleSQLParser &amp; DSL</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.08</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-7158 collect and take return different results</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.354</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>grouping on nested fields</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.068</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-6201 IN type conversion</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.069</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-11226 Skip empty line in json file</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.082</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-8828 sum should return null if all input values are null</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.103</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>aggregation with codegen</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.153</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>Add Parser of SQL COALESCE()</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.076</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-3176 Added Parser of SQL LAST()</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.045</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-2041 column name equals tablename</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.038</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SQRT</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.044</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SQRT with automatic string casts</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.162</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-2407 Added Parser of SQL SUBSTR()</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.374</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-3173 Timestamp support in the parser</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.119</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>left semi greater than predicate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.513</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>left semi greater than predicate and equal operator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.059</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>select *</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.051</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>simple select</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.587</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>external sorting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>negative in LIMIT or TABLESAMPLE</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.19</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>CTE feature</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>Allow only a single WITH clause per query</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.087</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>date row</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.232</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>from follow multiple brackets</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.077</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>average</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.182</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>average overflow</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.109</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>count</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.176</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>count distinct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.165</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>approximate count distinct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.179</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>approximate count distinct with user provided standard deviation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.48</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>null count</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.088</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>count of empty table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.256</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>inner join where, one match per row</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.16</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>inner join ON, one match per row</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.175</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>inner join, where, multiple matches</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.098</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>inner join, no matches</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.362</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>big inner join, 4 matches per row</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.107</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>cartesian product join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.234</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>left outer join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.188</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>right outer join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.257</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>full outer join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.263</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-11111 null-safe join should not use cartesian product</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.52</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-3349 partitioning after limit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.23</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>mixed-case keywords</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.066</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>select with table name as qualifier</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.19</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>inner join ON with table name as qualifier</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.206</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>qualified select with inner join ON with table name as qualifier</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.126</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>system function upper()</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.099</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>system function lower()</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.469</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>UNION</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.473</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>UNION with column mismatches</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.982</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>EXCEPT</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.907</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>MINUS</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.58</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>INTERSECT</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.097</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SET commands semantics using sql()</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SET commands with illegal or inappropriate argument</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.39</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>apply schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.171</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-3423 BETWEEN</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.3</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-17863: SELECT distinct does not work correctly if order by missing attribute</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.074</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>cast boolean to string</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.035</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>metadata is propagated correctly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.163</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-3371 Renaming a function expression with group by gives error</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.153</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-3813 CASE a WHEN b THEN c [WHEN d THEN e]* [ELSE f] END</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.126</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-3813 CASE WHEN a THEN b [WHEN c THEN d]* [ELSE e] END</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.059</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-16748: SparkExceptions during planning should not wrapped in TreeNodeException</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.344</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>Multiple join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-3483 Special chars in column names</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.045</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-3814 Support Bitwise &amp; operator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.045</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-3814 Support Bitwise | operator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-3814 Support Bitwise ^ operator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-3814 Support Bitwise ~ operator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.31</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-4120 Join of multiple tables does not work in SparkSQL</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.159</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-4154 Query does not work if it has &apos;not between&apos; in Spark SQL and HQL</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.175</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-4207 Query which has syntax like &apos;not like&apos; is not working in Spark SQL</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.299</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-4322 Grouping field with struct field as sub expression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.135</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-4432 Fix attribute reference resolution error when using ORDER BY</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.16</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>oder by asc by default when not specify ascending and descending</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.258</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>Supporting relational operator &apos;&lt;=&gt;&apos; in Spark SQL</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.193</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.073</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-4699 case sensitivity SQL query</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.843</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-6145: ORDER BY test for nested fields</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.262</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-6145: special cases</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.099</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-6898: complete support for special chars in column names</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.34</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-6583 order by aggregated function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.101</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-7952: fix the equality check between boolean and numeric types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.178</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-7067: order by queries for complex ExtractValue chain</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.059</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-8782: ORDER BY NULL</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.119</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-8837: use keyword in column name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.053</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-8753: add interval type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.127</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-8945: add and subtract expressions for interval type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.217</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>aggregation with codegen updates peak execution memory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.584</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>decimal precision with multiply/division</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.179</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-10215 Div of Decimal returns null</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.302</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>precision smaller than scale</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.105</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>external sorting updates peak execution memory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.11</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-9511: error with table starting with number</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.241</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>specifying database name for a temporary table is not allowed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-10130 type coercion for IF should have children resolved first</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.597</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-10389: order by non-attribute grouping expression on Aggregate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.482</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>run sql directly on files</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.646</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SortMergeJoin returns wrong results when using UnsafeRows</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.286</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-11303: filter should not be pushed down into sample</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.822</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>Struct Star Expansion</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.146</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>Struct Star Expansion - Name conflict</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.238</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>Star Expansion - group by</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.117</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>Star Expansion - table with zero column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.944</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>Common subexpression elimination</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.094</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-10707: nullability should be correctly propagated through set operations (1)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.073</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-10707: nullability should be correctly propagated through set operations (2)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.166</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>filter on a grouping column that is not presented in SELECT</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.152</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-13056: Null in map value causes NPE</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.061</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>hash function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.499</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>join with using clause</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.577</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-15327: fail to compile generated code with complex data structure</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.052</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-14986: Outer lateral view with empty generate expression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.344</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>data source table created in InMemoryCatalog should be able to read/write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>Eliminate noop ordinal ORDER BY</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.413</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>check code injection is prevented</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.598</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-15752 optimize metadata only query for datasource table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.222</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-16975: Column-partition path starting &apos;_&apos; should be handled correctly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.164</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-16644: Aggregate should not put aggregate expressions to constraints</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.249</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-16674: field names containing dots for both fields and partitioned fields</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.054</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>execute() should perform per-partition limits</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.113</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>CREATE TABLE USING should not fail if a same-name temp view exists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.149</duration>
          <className>org.apache.spark.sql.SQLQuerySuite</className>
          <testName>SPARK-18053: ARRAY equality is broken</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.SQLQueryTestSuite.xml</file>
      <name>org.apache.spark.sql.SQLQueryTestSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>34.275</duration>
      <cases>
        <case>
          <duration>0.395</duration>
          <className>org.apache.spark.sql.SQLQueryTestSuite</className>
          <testName>sql</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.107</duration>
          <className>org.apache.spark.sql.SQLQueryTestSuite</className>
          <testName>sql</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>6.014</duration>
          <className>org.apache.spark.sql.SQLQueryTestSuite</className>
          <testName>sql</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.847</duration>
          <className>org.apache.spark.sql.SQLQueryTestSuite</className>
          <testName>sql</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.233</duration>
          <className>org.apache.spark.sql.SQLQueryTestSuite</className>
          <testName>sql</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.169</duration>
          <className>org.apache.spark.sql.SQLQueryTestSuite</className>
          <testName>sql</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.159</duration>
          <className>org.apache.spark.sql.SQLQueryTestSuite</className>
          <testName>sql</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.109</duration>
          <className>org.apache.spark.sql.SQLQueryTestSuite</className>
          <testName>sql</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>5.683</duration>
          <className>org.apache.spark.sql.SQLQueryTestSuite</className>
          <testName>sql</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.769</duration>
          <className>org.apache.spark.sql.SQLQueryTestSuite</className>
          <testName>sql</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.365</duration>
          <className>org.apache.spark.sql.SQLQueryTestSuite</className>
          <testName>sql</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.SQLQueryTestSuite</className>
          <testName>sql</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.695</duration>
          <className>org.apache.spark.sql.SQLQueryTestSuite</className>
          <testName>sql</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.238</duration>
          <className>org.apache.spark.sql.SQLQueryTestSuite</className>
          <testName>sql</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.119</duration>
          <className>org.apache.spark.sql.SQLQueryTestSuite</className>
          <testName>sql</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.388</duration>
          <className>org.apache.spark.sql.SQLQueryTestSuite</className>
          <testName>sql</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.17</duration>
          <className>org.apache.spark.sql.SQLQueryTestSuite</className>
          <testName>sql</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.1</duration>
          <className>org.apache.spark.sql.SQLQueryTestSuite</className>
          <testName>sql</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.505</duration>
          <className>org.apache.spark.sql.SQLQueryTestSuite</className>
          <testName>sql</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.19</duration>
          <className>org.apache.spark.sql.SQLQueryTestSuite</className>
          <testName>sql</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.325</duration>
          <className>org.apache.spark.sql.SQLQueryTestSuite</className>
          <testName>sql</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.253</duration>
          <className>org.apache.spark.sql.SQLQueryTestSuite</className>
          <testName>sql</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.098</duration>
          <className>org.apache.spark.sql.SQLQueryTestSuite</className>
          <testName>sql</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>9.559</duration>
          <className>org.apache.spark.sql.SQLQueryTestSuite</className>
          <testName>sql</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.786</duration>
          <className>org.apache.spark.sql.SQLQueryTestSuite</className>
          <testName>sql</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.ScalaReflectionRelationSuite.xml</file>
      <name>org.apache.spark.sql.ScalaReflectionRelationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.496</duration>
      <cases>
        <case>
          <duration>0.118</duration>
          <className>org.apache.spark.sql.ScalaReflectionRelationSuite</className>
          <testName>query case class RDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.057</duration>
          <className>org.apache.spark.sql.ScalaReflectionRelationSuite</className>
          <testName>query case class RDD with nulls</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.sql.ScalaReflectionRelationSuite</className>
          <testName>query case class RDD with Nones</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.sql.ScalaReflectionRelationSuite</className>
          <testName>query binary data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.242</duration>
          <className>org.apache.spark.sql.ScalaReflectionRelationSuite</className>
          <testName>query complex data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.SerializationSuite.xml</file>
      <name>org.apache.spark.sql.SerializationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.009</duration>
      <cases>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.SerializationSuite</className>
          <testName>[SPARK-5235] SQLContext should be serializable</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.SingleLevelAggregateHashMapSuite.xml</file>
      <name>org.apache.spark.sql.SingleLevelAggregateHashMapSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>11.058998</duration>
      <cases>
        <case>
          <duration>1.712</duration>
          <className>org.apache.spark.sql.SingleLevelAggregateHashMapSuite</className>
          <testName>groupBy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.193</duration>
          <className>org.apache.spark.sql.SingleLevelAggregateHashMapSuite</className>
          <testName>SPARK-17124 agg should be ordering preserving</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.231</duration>
          <className>org.apache.spark.sql.SingleLevelAggregateHashMapSuite</className>
          <testName>rollup</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.381</duration>
          <className>org.apache.spark.sql.SingleLevelAggregateHashMapSuite</className>
          <testName>cube</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.201</duration>
          <className>org.apache.spark.sql.SingleLevelAggregateHashMapSuite</className>
          <testName>grouping and grouping_id</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.425</duration>
          <className>org.apache.spark.sql.SingleLevelAggregateHashMapSuite</className>
          <testName>grouping/grouping_id inside window function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.382</duration>
          <className>org.apache.spark.sql.SingleLevelAggregateHashMapSuite</className>
          <testName>rollup overlapping columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.346</duration>
          <className>org.apache.spark.sql.SingleLevelAggregateHashMapSuite</className>
          <testName>cube overlapping columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.294</duration>
          <className>org.apache.spark.sql.SingleLevelAggregateHashMapSuite</className>
          <testName>retainGroupColumns config</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.08</duration>
          <className>org.apache.spark.sql.SingleLevelAggregateHashMapSuite</className>
          <testName>agg without groups</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.074</duration>
          <className>org.apache.spark.sql.SingleLevelAggregateHashMapSuite</className>
          <testName>agg without groups and functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.032</duration>
          <className>org.apache.spark.sql.SingleLevelAggregateHashMapSuite</className>
          <testName>average</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.48</duration>
          <className>org.apache.spark.sql.SingleLevelAggregateHashMapSuite</className>
          <testName>null average</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.237</duration>
          <className>org.apache.spark.sql.SingleLevelAggregateHashMapSuite</className>
          <testName>zero average</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.268</duration>
          <className>org.apache.spark.sql.SingleLevelAggregateHashMapSuite</className>
          <testName>count</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.818</duration>
          <className>org.apache.spark.sql.SingleLevelAggregateHashMapSuite</className>
          <testName>null count</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.771</duration>
          <className>org.apache.spark.sql.SingleLevelAggregateHashMapSuite</className>
          <testName>multiple column distinct count</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.14</duration>
          <className>org.apache.spark.sql.SingleLevelAggregateHashMapSuite</className>
          <testName>zero count</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.215</duration>
          <className>org.apache.spark.sql.SingleLevelAggregateHashMapSuite</className>
          <testName>stddev</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.097</duration>
          <className>org.apache.spark.sql.SingleLevelAggregateHashMapSuite</className>
          <testName>zero stddev</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.088</duration>
          <className>org.apache.spark.sql.SingleLevelAggregateHashMapSuite</className>
          <testName>zero sum</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.138</duration>
          <className>org.apache.spark.sql.SingleLevelAggregateHashMapSuite</className>
          <testName>zero sum distinct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.318</duration>
          <className>org.apache.spark.sql.SingleLevelAggregateHashMapSuite</className>
          <testName>moments</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.405</duration>
          <className>org.apache.spark.sql.SingleLevelAggregateHashMapSuite</className>
          <testName>zero moments</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.308</duration>
          <className>org.apache.spark.sql.SingleLevelAggregateHashMapSuite</className>
          <testName>null moments</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.192</duration>
          <className>org.apache.spark.sql.SingleLevelAggregateHashMapSuite</className>
          <testName>collect functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.2</duration>
          <className>org.apache.spark.sql.SingleLevelAggregateHashMapSuite</className>
          <testName>collect functions structs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.SingleLevelAggregateHashMapSuite</className>
          <testName>collect_set functions cannot have maps</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.181</duration>
          <className>org.apache.spark.sql.SingleLevelAggregateHashMapSuite</className>
          <testName>SPARK-17641: collect functions should not collect null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.256</duration>
          <className>org.apache.spark.sql.SingleLevelAggregateHashMapSuite</className>
          <testName>SPARK-14664: Decimal sum/avg over window should work</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.203</duration>
          <className>org.apache.spark.sql.SingleLevelAggregateHashMapSuite</className>
          <testName>SQL decimal test (used for catching certain demical handling bugs in aggregates)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.379</duration>
          <className>org.apache.spark.sql.SingleLevelAggregateHashMapSuite</className>
          <testName>SPARK-17616: distinct aggregate combined with a non-partial aggregate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.SparkSessionBuilderSuite.xml</file>
      <name>org.apache.spark.sql.SparkSessionBuilderSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.505</duration>
      <cases>
        <case>
          <duration>0.064</duration>
          <className>org.apache.spark.sql.SparkSessionBuilderSuite</className>
          <testName>create with config options and propagate them to SparkContext and SparkSession</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.SparkSessionBuilderSuite</className>
          <testName>use global default session</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.sql.SparkSessionBuilderSuite</className>
          <testName>config options are propagated to existing SparkSession</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.sql.SparkSessionBuilderSuite</className>
          <testName>use session from active thread session and propagate config options</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.073</duration>
          <className>org.apache.spark.sql.SparkSessionBuilderSuite</className>
          <testName>create a new session if the default session has been stopped</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.127</duration>
          <className>org.apache.spark.sql.SparkSessionBuilderSuite</className>
          <testName>create a new session if the active thread session has been stopped</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.067</duration>
          <className>org.apache.spark.sql.SparkSessionBuilderSuite</className>
          <testName>create SparkContext first then SparkSession</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.068</duration>
          <className>org.apache.spark.sql.SparkSessionBuilderSuite</className>
          <testName>xml should be loaded</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.066</duration>
          <className>org.apache.spark.sql.SparkSessionBuilderSuite</className>
          <testName>SPARK-15991: Set global Hadoop conf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.StatisticsCollectionSuite.xml</file>
      <name>org.apache.spark.sql.StatisticsCollectionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.3100001</duration>
      <cases>
        <case>
          <duration>0.093</duration>
          <className>org.apache.spark.sql.StatisticsCollectionSuite</className>
          <testName>column stats round trip serialization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.848</duration>
          <className>org.apache.spark.sql.StatisticsCollectionSuite</className>
          <testName>analyze column command - result verification</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.032</duration>
          <className>org.apache.spark.sql.StatisticsCollectionSuite</className>
          <testName>estimates the size of a limit 0 on outer join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.121</duration>
          <className>org.apache.spark.sql.StatisticsCollectionSuite</className>
          <testName>analyze column command - unsupported types and invalid columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.172</duration>
          <className>org.apache.spark.sql.StatisticsCollectionSuite</className>
          <testName>test table-level statistics for data source table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.StatisticsCollectionSuite</className>
          <testName>SPARK-15392: DataFrame created from RDD should not be broadcasted</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.sql.StatisticsCollectionSuite</className>
          <testName>estimates the size of limit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.StringFunctionsSuite.xml</file>
      <name>org.apache.spark.sql.StringFunctionsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.8469996</duration>
      <cases>
        <case>
          <duration>0.123</duration>
          <className>org.apache.spark.sql.StringFunctionsSuite</className>
          <testName>string concat</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.068</duration>
          <className>org.apache.spark.sql.StringFunctionsSuite</className>
          <testName>string concat_ws</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.112</duration>
          <className>org.apache.spark.sql.StringFunctionsSuite</className>
          <testName>string elt</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.064</duration>
          <className>org.apache.spark.sql.StringFunctionsSuite</className>
          <testName>string Levenshtein distance</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.119</duration>
          <className>org.apache.spark.sql.StringFunctionsSuite</className>
          <testName>string regex_replace / regex_extract</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.076</duration>
          <className>org.apache.spark.sql.StringFunctionsSuite</className>
          <testName>non-matching optional group</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.059</duration>
          <className>org.apache.spark.sql.StringFunctionsSuite</className>
          <testName>string ascii function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.08</duration>
          <className>org.apache.spark.sql.StringFunctionsSuite</className>
          <testName>string base64/unbase64 function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.097</duration>
          <className>org.apache.spark.sql.StringFunctionsSuite</className>
          <testName>string / binary substring function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.085</duration>
          <className>org.apache.spark.sql.StringFunctionsSuite</className>
          <testName>string encode/decode function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.057</duration>
          <className>org.apache.spark.sql.StringFunctionsSuite</className>
          <testName>string translate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.057</duration>
          <className>org.apache.spark.sql.StringFunctionsSuite</className>
          <testName>string trim functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.068</duration>
          <className>org.apache.spark.sql.StringFunctionsSuite</className>
          <testName>string formatString function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.05</duration>
          <className>org.apache.spark.sql.StringFunctionsSuite</className>
          <testName>soundex function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.053</duration>
          <className>org.apache.spark.sql.StringFunctionsSuite</className>
          <testName>string instr function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.05</duration>
          <className>org.apache.spark.sql.StringFunctionsSuite</className>
          <testName>string substring_index function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.089</duration>
          <className>org.apache.spark.sql.StringFunctionsSuite</className>
          <testName>string locate function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.082</duration>
          <className>org.apache.spark.sql.StringFunctionsSuite</className>
          <testName>string padding functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.309</duration>
          <className>org.apache.spark.sql.StringFunctionsSuite</className>
          <testName>string parse_url function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.072</duration>
          <className>org.apache.spark.sql.StringFunctionsSuite</className>
          <testName>string repeat function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.056</duration>
          <className>org.apache.spark.sql.StringFunctionsSuite</className>
          <testName>string reverse function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.033</duration>
          <className>org.apache.spark.sql.StringFunctionsSuite</className>
          <testName>string space function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.086</duration>
          <className>org.apache.spark.sql.StringFunctionsSuite</className>
          <testName>string split function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.14</duration>
          <className>org.apache.spark.sql.StringFunctionsSuite</className>
          <testName>string / binary length function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.065</duration>
          <className>org.apache.spark.sql.StringFunctionsSuite</className>
          <testName>initcap function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.412</duration>
          <className>org.apache.spark.sql.StringFunctionsSuite</className>
          <testName>number format function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.155</duration>
          <className>org.apache.spark.sql.StringFunctionsSuite</className>
          <testName>string sentences function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.13</duration>
          <className>org.apache.spark.sql.StringFunctionsSuite</className>
          <testName>str_to_map function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.SubquerySuite.xml</file>
      <name>org.apache.spark.sql.SubquerySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>23.448002</duration>
      <cases>
        <case>
          <duration>0.726</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>rdd deserialization does not crash [SPARK-15791]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.014</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>simple uncorrelated scalar subquery</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.627</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>define CTE in CTE subquery</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.295</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>uncorrelated scalar subquery in CTE</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.11</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>uncorrelated scalar subquery should return null if there is 0 rows</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.085</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>runtime error when the number of rows is greater than 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.18</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>uncorrelated scalar subquery on a DataFrame generated query</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.447</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>SPARK-15677: Queries against local relations with scalar subquery in Select list</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.674</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>SPARK-14791: scalar subquery inside broadcast join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.265</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>EXISTS predicate subquery</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.242</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>NOT EXISTS predicate subquery</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.258</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>EXISTS predicate subquery within OR</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.308</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>IN predicate subquery</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.56</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>NOT IN predicate subquery</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.14</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>IN predicate subquery within OR</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.278</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>complex IN predicate subquery</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.331</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>same column in subquery and outer table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.377</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>having with function in subquery</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.987</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>SPARK-15832: Test embedded existential predicate sub-queries</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.356</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>correlated scalar subquery in where</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.328</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>correlated scalar subquery in select</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.292</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>correlated scalar subquery in select (null safe)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.472</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>correlated scalar subquery in aggregate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.041</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>SPARK-18504 extra GROUP BY column in correlated scalar subquery is not permitted</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>non-aggregated correlated scalar subquery</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>non-equal correlated scalar subquery</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.636</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>disjunctive correlated scalar subquery</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.888</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>SPARK-15370: COUNT bug in WHERE clause (Filter)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.284</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>SPARK-15370: COUNT bug in SELECT clause (Project)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.56</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>SPARK-15370: COUNT bug in HAVING clause (Filter)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.663</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>SPARK-15370: COUNT bug in Aggregate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.917</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>SPARK-15370: COUNT bug negative examples</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.387</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>SPARK-15370: COUNT bug in subquery in subquery in subquery</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.374</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>SPARK-15370: COUNT bug with nasty predicate expr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.34</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>SPARK-15370: COUNT bug with attribute ref in subquery input and output</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.21</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>SPARK-16804: Correlated subqueries containing LIMIT - 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.192</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>SPARK-16804: Correlated subqueries containing LIMIT - 2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.131</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>SPARK-17337: Incorrect column resolution leads to incorrect results</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.266</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>SPARK-17348: Correlated subqueries with non-equality predicate (good case)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.173</duration>
          <className>org.apache.spark.sql.SubquerySuite</className>
          <testName>SPARK-17348: Correlated subqueries with non-equality predicate (error case)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.TwoLevelAggregateHashMapSuite.xml</file>
      <name>org.apache.spark.sql.TwoLevelAggregateHashMapSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>12.320002</duration>
      <cases>
        <case>
          <duration>1.731</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapSuite</className>
          <testName>groupBy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.21</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapSuite</className>
          <testName>SPARK-17124 agg should be ordering preserving</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.326</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapSuite</className>
          <testName>rollup</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.447</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapSuite</className>
          <testName>cube</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.22</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapSuite</className>
          <testName>grouping and grouping_id</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.515</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapSuite</className>
          <testName>grouping/grouping_id inside window function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.453</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapSuite</className>
          <testName>rollup overlapping columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.434</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapSuite</className>
          <testName>cube overlapping columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.313</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapSuite</className>
          <testName>retainGroupColumns config</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.076</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapSuite</className>
          <testName>agg without groups</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.087</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapSuite</className>
          <testName>agg without groups and functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.157</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapSuite</className>
          <testName>average</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.665</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapSuite</className>
          <testName>null average</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.302</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapSuite</className>
          <testName>zero average</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.359</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapSuite</className>
          <testName>count</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.9</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapSuite</className>
          <testName>null count</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.824</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapSuite</className>
          <testName>multiple column distinct count</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.234</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapSuite</className>
          <testName>zero count</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.219</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapSuite</className>
          <testName>stddev</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.1</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapSuite</className>
          <testName>zero stddev</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.095</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapSuite</className>
          <testName>zero sum</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.145</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapSuite</className>
          <testName>zero sum distinct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.304</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapSuite</className>
          <testName>moments</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.395</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapSuite</className>
          <testName>zero moments</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.279</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapSuite</className>
          <testName>null moments</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.219</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapSuite</className>
          <testName>collect functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.207</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapSuite</className>
          <testName>collect functions structs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapSuite</className>
          <testName>collect_set functions cannot have maps</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.181</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapSuite</className>
          <testName>SPARK-17641: collect functions should not collect null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.257</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapSuite</className>
          <testName>SPARK-14664: Decimal sum/avg over window should work</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.249</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapSuite</className>
          <testName>SQL decimal test (used for catching certain demical handling bugs in aggregates)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.403</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapSuite</className>
          <testName>SPARK-17616: distinct aggregate combined with a non-partial aggregate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite.xml</file>
      <name>org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>11.595999</duration>
      <cases>
        <case>
          <duration>1.771</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite</className>
          <testName>groupBy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.198</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite</className>
          <testName>SPARK-17124 agg should be ordering preserving</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.305</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite</className>
          <testName>rollup</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.413</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite</className>
          <testName>cube</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.232</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite</className>
          <testName>grouping and grouping_id</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.475</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite</className>
          <testName>grouping/grouping_id inside window function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.477</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite</className>
          <testName>rollup overlapping columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.413</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite</className>
          <testName>cube overlapping columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.243</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite</className>
          <testName>retainGroupColumns config</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.07</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite</className>
          <testName>agg without groups</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.072</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite</className>
          <testName>agg without groups and functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.042</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite</className>
          <testName>average</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.583</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite</className>
          <testName>null average</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.25</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite</className>
          <testName>zero average</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.289</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite</className>
          <testName>count</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.827</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite</className>
          <testName>null count</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.739</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite</className>
          <testName>multiple column distinct count</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.191</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite</className>
          <testName>zero count</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.189</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite</className>
          <testName>stddev</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.092</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite</className>
          <testName>zero stddev</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.105</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite</className>
          <testName>zero sum</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.139</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite</className>
          <testName>zero sum distinct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.27</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite</className>
          <testName>moments</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.4</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite</className>
          <testName>zero moments</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.309</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite</className>
          <testName>null moments</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.183</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite</className>
          <testName>collect functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.206</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite</className>
          <testName>collect functions structs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite</className>
          <testName>collect_set functions cannot have maps</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.226</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite</className>
          <testName>SPARK-17641: collect functions should not collect null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.242</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite</className>
          <testName>SPARK-14664: Decimal sum/avg over window should work</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.218</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite</className>
          <testName>SQL decimal test (used for catching certain demical handling bugs in aggregates)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.414</duration>
          <className>org.apache.spark.sql.TwoLevelAggregateHashMapWithVectorizedMapSuite</className>
          <testName>SPARK-17616: distinct aggregate combined with a non-partial aggregate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.TypedImperativeAggregateSuite.xml</file>
      <name>org.apache.spark.sql.TypedImperativeAggregateSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.4280002</duration>
      <cases>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.TypedImperativeAggregateSuite</className>
          <testName>aggregate with object aggregate buffer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.TypedImperativeAggregateSuite</className>
          <testName>supports SpecificMutableRow as mutable row</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.054</duration>
          <className>org.apache.spark.sql.TypedImperativeAggregateSuite</className>
          <testName>dataframe aggregate with object aggregate buffer, should not use HashAggregate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.202</duration>
          <className>org.apache.spark.sql.TypedImperativeAggregateSuite</className>
          <testName>dataframe aggregate with object aggregate buffer, no group by</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.125</duration>
          <className>org.apache.spark.sql.TypedImperativeAggregateSuite</className>
          <testName>dataframe aggregate with object aggregate buffer, non-nullable aggregator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.127</duration>
          <className>org.apache.spark.sql.TypedImperativeAggregateSuite</className>
          <testName>dataframe aggregate with object aggregate buffer, nullable aggregator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.138</duration>
          <className>org.apache.spark.sql.TypedImperativeAggregateSuite</className>
          <testName>dataframe aggregation with object aggregate buffer, input row contains null</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.317</duration>
          <className>org.apache.spark.sql.TypedImperativeAggregateSuite</className>
          <testName>dataframe aggregate with object aggregate buffer, with group by</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.085</duration>
          <className>org.apache.spark.sql.TypedImperativeAggregateSuite</className>
          <testName>dataframe aggregate with object aggregate buffer, empty inputs, no group by</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.088</duration>
          <className>org.apache.spark.sql.TypedImperativeAggregateSuite</className>
          <testName>dataframe aggregate with object aggregate buffer, empty inputs, with group by</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.279</duration>
          <className>org.apache.spark.sql.TypedImperativeAggregateSuite</className>
          <testName>TypedImperativeAggregate should not break Window function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.UDFSuite.xml</file>
      <name>org.apache.spark.sql.UDFSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>3.441</duration>
      <cases>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.sql.UDFSuite</className>
          <testName>built-in fixed arity expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.UDFSuite</className>
          <testName>built-in vararg expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.sql.UDFSuite</className>
          <testName>built-in expressions with multiple constructors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.sql.UDFSuite</className>
          <testName>count</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.sql.UDFSuite</className>
          <testName>count distinct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.072</duration>
          <className>org.apache.spark.sql.UDFSuite</className>
          <testName>SPARK-8003 spark_partition_id</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.446</duration>
          <className>org.apache.spark.sql.UDFSuite</className>
          <testName>SPARK-8005 input_file_name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.UDFSuite</className>
          <testName>error reporting for incorrect number of arguments</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.UDFSuite</className>
          <testName>error reporting for undefined functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.093</duration>
          <className>org.apache.spark.sql.UDFSuite</className>
          <testName>Simple UDF</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.054</duration>
          <className>org.apache.spark.sql.UDFSuite</className>
          <testName>ZeroArgument UDF</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.049</duration>
          <className>org.apache.spark.sql.UDFSuite</className>
          <testName>TwoArgument UDF</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.121</duration>
          <className>org.apache.spark.sql.UDFSuite</className>
          <testName>UDF in a WHERE</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.237</duration>
          <className>org.apache.spark.sql.UDFSuite</className>
          <testName>UDF in a HAVING</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.173</duration>
          <className>org.apache.spark.sql.UDFSuite</className>
          <testName>UDF in a GROUP BY</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.245</duration>
          <className>org.apache.spark.sql.UDFSuite</className>
          <testName>UDFs everywhere</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.062</duration>
          <className>org.apache.spark.sql.UDFSuite</className>
          <testName>struct UDF</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.066</duration>
          <className>org.apache.spark.sql.UDFSuite</className>
          <testName>udf that is transformed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.046</duration>
          <className>org.apache.spark.sql.UDFSuite</className>
          <testName>type coercion for udf inputs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.372</duration>
          <className>org.apache.spark.sql.UDFSuite</className>
          <testName>udf in different types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.264</duration>
          <className>org.apache.spark.sql.UDFSuite</className>
          <testName>SPARK-11716 UDFRegistration does not include the input data type in returned UDF</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.UDTRegistrationSuite.xml</file>
      <name>org.apache.spark.sql.UDTRegistrationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.0050000004</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.UDTRegistrationSuite</className>
          <testName>register non-UserDefinedType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.UDTRegistrationSuite</className>
          <testName>default UDTs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.UDTRegistrationSuite</className>
          <testName>query registered user class</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.UDTRegistrationSuite</className>
          <testName>query unregistered user class</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.UnsafeRowSuite.xml</file>
      <name>org.apache.spark.sql.UnsafeRowSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.052</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.UnsafeRowSuite</className>
          <testName>UnsafeRow Java serialization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.UnsafeRowSuite</className>
          <testName>UnsafeRow Kryo serialization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.UnsafeRowSuite</className>
          <testName>bitset width calculation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.UnsafeRowSuite</className>
          <testName>writeToStream</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.sql.UnsafeRowSuite</className>
          <testName>calling getDouble() and getFloat() on null columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.UnsafeRowSuite</className>
          <testName>calling get(ordinal, datatype) on null columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.UnsafeRowSuite</className>
          <testName>createFromByteArray and copyFrom</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.UnsafeRowSuite</className>
          <testName>calling hashCode on unsafe array returned by getArray(ordinal)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.UserDefinedTypeSuite.xml</file>
      <name>org.apache.spark.sql.UserDefinedTypeSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.494</duration>
      <cases>
        <case>
          <duration>0.131</duration>
          <className>org.apache.spark.sql.UserDefinedTypeSuite</className>
          <testName>register user type: MyDenseVector for MyLabeledPoint</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.039</duration>
          <className>org.apache.spark.sql.UserDefinedTypeSuite</className>
          <testName>UDTs and UDFs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.27</duration>
          <className>org.apache.spark.sql.UserDefinedTypeSuite</className>
          <testName>Standard mode - UDTs with Parquet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.22</duration>
          <className>org.apache.spark.sql.UserDefinedTypeSuite</className>
          <testName>Legacy mode - UDTs with Parquet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.206</duration>
          <className>org.apache.spark.sql.UserDefinedTypeSuite</className>
          <testName>Standard mode - Repartition UDTs with Parquet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.204</duration>
          <className>org.apache.spark.sql.UserDefinedTypeSuite</className>
          <testName>Legacy mode - Repartition UDTs with Parquet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.266</duration>
          <className>org.apache.spark.sql.UserDefinedTypeSuite</className>
          <testName>Local UDTs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.061</duration>
          <className>org.apache.spark.sql.UserDefinedTypeSuite</className>
          <testName>UDTs with JSON</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.sql.UserDefinedTypeSuite</className>
          <testName>UDTs with JSON and Dataset</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.UserDefinedTypeSuite</className>
          <testName>typeName</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.UserDefinedTypeSuite</className>
          <testName>Catalyst type converter null handling for UDTs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.063</duration>
          <className>org.apache.spark.sql.UserDefinedTypeSuite</className>
          <testName>map returns UDT object</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.WideSchemaBenchmark.xml</file>
      <name>org.apache.spark.sql.WideSchemaBenchmark</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>-0.0070000007</duration>
      <cases>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.WideSchemaBenchmark</className>
          <testName>parsing large select expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.WideSchemaBenchmark</className>
          <testName>many column field read and write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.WideSchemaBenchmark</className>
          <testName>wide shallowly nested struct field read and write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.WideSchemaBenchmark</className>
          <testName>deeply nested struct field read and write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.WideSchemaBenchmark</className>
          <testName>bushy struct field read and write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.WideSchemaBenchmark</className>
          <testName>wide array field read and write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.WideSchemaBenchmark</className>
          <testName>wide map field read and write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.XPathFunctionsSuite.xml</file>
      <name>org.apache.spark.sql.XPathFunctionsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.391</duration>
      <cases>
        <case>
          <duration>0.224</duration>
          <className>org.apache.spark.sql.XPathFunctionsSuite</className>
          <testName>xpath_boolean</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.05</duration>
          <className>org.apache.spark.sql.XPathFunctionsSuite</className>
          <testName>xpath_short, xpath_int, xpath_long</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.045</duration>
          <className>org.apache.spark.sql.XPathFunctionsSuite</className>
          <testName>xpath_float, xpath_double, xpath_number</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.sql.XPathFunctionsSuite</className>
          <testName>xpath_string</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.sql.XPathFunctionsSuite</className>
          <testName>xpath</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.api.r.SQLUtilsSuite.xml</file>
      <name>org.apache.spark.sql.api.r.SQLUtilsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.055</duration>
      <cases>
        <case>
          <duration>0.055</duration>
          <className>org.apache.spark.sql.api.r.SQLUtilsSuite</className>
          <testName>dfToCols should collect and transpose a data frame</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.CoGroupedIteratorSuite.xml</file>
      <name>org.apache.spark.sql.execution.CoGroupedIteratorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.003</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.CoGroupedIteratorSuite</className>
          <testName>basic</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.CoGroupedIteratorSuite</className>
          <testName>hasNext is not idempotent</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.ExchangeCoordinatorSuite.xml</file>
      <name>org.apache.spark.sql.execution.ExchangeCoordinatorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>3.5650003</duration>
      <cases>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.execution.ExchangeCoordinatorSuite</className>
          <testName>test estimatePartitionStartIndices - 1 Exchange</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.ExchangeCoordinatorSuite</className>
          <testName>test estimatePartitionStartIndices - 2 Exchanges</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.ExchangeCoordinatorSuite</className>
          <testName>test estimatePartitionStartIndices and enforce minimal number of reducers</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.37</duration>
          <className>org.apache.spark.sql.execution.ExchangeCoordinatorSuite</className>
          <testName>determining the number of reducers: aggregate operator(minNumPostShufflePartitions: 3)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.428</duration>
          <className>org.apache.spark.sql.execution.ExchangeCoordinatorSuite</className>
          <testName>determining the number of reducers: join operator(minNumPostShufflePartitions: 3)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.558</duration>
          <className>org.apache.spark.sql.execution.ExchangeCoordinatorSuite</className>
          <testName>determining the number of reducers: complex query 1(minNumPostShufflePartitions: 3)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.49</duration>
          <className>org.apache.spark.sql.execution.ExchangeCoordinatorSuite</className>
          <testName>determining the number of reducers: complex query 2(minNumPostShufflePartitions: 3)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.256</duration>
          <className>org.apache.spark.sql.execution.ExchangeCoordinatorSuite</className>
          <testName>determining the number of reducers: aggregate operator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.398</duration>
          <className>org.apache.spark.sql.execution.ExchangeCoordinatorSuite</className>
          <testName>determining the number of reducers: join operator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.584</duration>
          <className>org.apache.spark.sql.execution.ExchangeCoordinatorSuite</className>
          <testName>determining the number of reducers: complex query 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.476</duration>
          <className>org.apache.spark.sql.execution.ExchangeCoordinatorSuite</className>
          <testName>determining the number of reducers: complex query 2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.ExchangeSuite.xml</file>
      <name>org.apache.spark.sql.execution.ExchangeSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.085</duration>
      <cases>
        <case>
          <duration>0.076</duration>
          <className>org.apache.spark.sql.execution.ExchangeSuite</className>
          <testName>shuffling UnsafeRows in exchange</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.ExchangeSuite</className>
          <testName>compatible BroadcastMode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.execution.ExchangeSuite</className>
          <testName>BroadcastExchange same result</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.execution.ExchangeSuite</className>
          <testName>ShuffleExchange same result</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.GlobalTempViewSuite.xml</file>
      <name>org.apache.spark.sql.execution.GlobalTempViewSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.0970001</duration>
      <cases>
        <case>
          <duration>0.274</duration>
          <className>org.apache.spark.sql.execution.GlobalTempViewSuite</className>
          <testName>basic semantic</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.099</duration>
          <className>org.apache.spark.sql.execution.GlobalTempViewSuite</className>
          <testName>global temp view is shared among all sessions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.execution.GlobalTempViewSuite</className>
          <testName>global temp view database should be preserved</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.396</duration>
          <className>org.apache.spark.sql.execution.GlobalTempViewSuite</className>
          <testName>CREATE GLOBAL TEMP VIEW USING</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.sql.execution.GlobalTempViewSuite</className>
          <testName>CREATE TABLE LIKE should work for global temp view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.172</duration>
          <className>org.apache.spark.sql.execution.GlobalTempViewSuite</className>
          <testName>list global temp views</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.111</duration>
          <className>org.apache.spark.sql.execution.GlobalTempViewSuite</className>
          <testName>should lookup global temp view if and only if global temp db is specified</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.execution.GlobalTempViewSuite</className>
          <testName>public Catalog should recognize global temp view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.GroupedIteratorSuite.xml</file>
      <name>org.apache.spark.sql.execution.GroupedIteratorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.065</duration>
      <cases>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.sql.execution.GroupedIteratorSuite</className>
          <testName>basic</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.035</duration>
          <className>org.apache.spark.sql.execution.GroupedIteratorSuite</className>
          <testName>group by 2 columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.execution.GroupedIteratorSuite</className>
          <testName>do nothing to the value iterator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.OptimizeMetadataOnlyQuerySuite.xml</file>
      <name>org.apache.spark.sql.execution.OptimizeMetadataOnlyQuerySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.48799998</duration>
      <cases>
        <case>
          <duration>0.078</duration>
          <className>org.apache.spark.sql.execution.OptimizeMetadataOnlyQuerySuite</className>
          <testName>Aggregate expression is partition columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.sql.execution.OptimizeMetadataOnlyQuerySuite</className>
          <testName>Distinct aggregate function on partition columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.sql.execution.OptimizeMetadataOnlyQuerySuite</className>
          <testName>Distinct on partition columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.124</duration>
          <className>org.apache.spark.sql.execution.OptimizeMetadataOnlyQuerySuite</className>
          <testName>Aggregate function on partition columns which have same result w or w/o DISTINCT keyword</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.077</duration>
          <className>org.apache.spark.sql.execution.OptimizeMetadataOnlyQuerySuite</className>
          <testName>Don&apos;t optimize metadata only query for non-partition columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.sql.execution.OptimizeMetadataOnlyQuerySuite</className>
          <testName>Don&apos;t optimize metadata only query for non-distinct aggregate function on partition columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.072</duration>
          <className>org.apache.spark.sql.execution.OptimizeMetadataOnlyQuerySuite</className>
          <testName>Don&apos;t optimize metadata only query for GroupingSet/Union operator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.PlannerSuite.xml</file>
      <name>org.apache.spark.sql.execution.PlannerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.43699992</duration>
      <cases>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.execution.PlannerSuite</className>
          <testName>count is partially aggregated</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.execution.PlannerSuite</className>
          <testName>count distinct is partially aggregated</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.execution.PlannerSuite</className>
          <testName>mixed aggregates are partially aggregated</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.sql.execution.PlannerSuite</className>
          <testName>sizeInBytes estimation of limit operator for broadcast hash join optimization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.109</duration>
          <className>org.apache.spark.sql.execution.PlannerSuite</className>
          <testName>InMemoryRelation statistics propagation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.149</duration>
          <className>org.apache.spark.sql.execution.PlannerSuite</className>
          <testName>SPARK-11390 explain should print PushedFilters of PhysicalRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.execution.PlannerSuite</className>
          <testName>efficient terminal limit -&gt; sort should use TakeOrderedAndProject</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.execution.PlannerSuite</className>
          <testName>terminal limit -&gt; project -&gt; sort should use TakeOrderedAndProject</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.execution.PlannerSuite</className>
          <testName>terminal limits that are not handled by TakeOrderedAndProject should use CollectLimit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.execution.PlannerSuite</className>
          <testName>TakeOrderedAndProject can appear in the middle of plans</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.execution.PlannerSuite</className>
          <testName>CollectLimit can appear in the middle of a plan when caching is used</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.058</duration>
          <className>org.apache.spark.sql.execution.PlannerSuite</className>
          <testName>PartitioningCollection</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.execution.PlannerSuite</className>
          <testName>collapse adjacent repartitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.execution.PlannerSuite</className>
          <testName>EnsureRequirements with incompatible child partitionings which satisfy distribution</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.PlannerSuite</className>
          <testName>EnsureRequirements with child partitionings with different numbers of output partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.PlannerSuite</className>
          <testName>EnsureRequirements with compatible child partitionings that do not satisfy distribution</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.PlannerSuite</className>
          <testName>EnsureRequirements with compatible child partitionings that satisfy distribution</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.PlannerSuite</className>
          <testName>EnsureRequirements should not repartition if only ordering requirement is unsatisfied</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.PlannerSuite</className>
          <testName>EnsureRequirements adds sort when there is no existing ordering</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.PlannerSuite</className>
          <testName>EnsureRequirements skips sort when required ordering is prefix of existing ordering</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.PlannerSuite</className>
          <testName>EnsureRequirements skips sort when required ordering is semantically equal to existing ordering</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.PlannerSuite</className>
          <testName>EnsureRequirements adds sort when required ordering isn&apos;t a prefix of existing ordering</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.PlannerSuite</className>
          <testName>EnsureRequirements eliminates Exchange if child has Exchange with same partitioning</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.PlannerSuite</className>
          <testName>EnsureRequirements does not eliminate Exchange with different partitioning</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.PlannerSuite</className>
          <testName>Reuse exchanges</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.QueryExecutionSuite.xml</file>
      <name>org.apache.spark.sql.execution.QueryExecutionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.022</duration>
      <cases>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.sql.execution.QueryExecutionSuite</className>
          <testName>toString() exception/error handling</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.SQLExecutionSuite.xml</file>
      <name>org.apache.spark.sql.execution.SQLExecutionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.867</duration>
      <cases>
        <case>
          <duration>0.257</duration>
          <className>org.apache.spark.sql.execution.SQLExecutionSuite</className>
          <testName>concurrent query execution (SPARK-10548)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.61</duration>
          <className>org.apache.spark.sql.execution.SQLExecutionSuite</className>
          <testName>concurrent query execution with fork-join pool (SPARK-13747)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.SQLWindowFunctionSuite.xml</file>
      <name>org.apache.spark.sql.execution.SQLWindowFunctionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>5.348</duration>
      <cases>
        <case>
          <duration>1.392</duration>
          <className>org.apache.spark.sql.execution.SQLWindowFunctionSuite</className>
          <testName>window function: udaf with aggregate expression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.201</duration>
          <className>org.apache.spark.sql.execution.SQLWindowFunctionSuite</className>
          <testName>window function: refer column in inner select block</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.43</duration>
          <className>org.apache.spark.sql.execution.SQLWindowFunctionSuite</className>
          <testName>window function: partition and order expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.sql.execution.SQLWindowFunctionSuite</className>
          <testName>window function: distinct should not be silently ignored</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.197</duration>
          <className>org.apache.spark.sql.execution.SQLWindowFunctionSuite</className>
          <testName>window function: expressions in arguments of a window functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.929</duration>
          <className>org.apache.spark.sql.execution.SQLWindowFunctionSuite</className>
          <testName>window function: Sorting columns are not in Project</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.execution.SQLWindowFunctionSuite</className>
          <testName>window function: Pushing aggregate Expressions in Sort to Aggregate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.337</duration>
          <className>org.apache.spark.sql.execution.SQLWindowFunctionSuite</className>
          <testName>window function: multiple window expressions in a single expression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.291</duration>
          <className>org.apache.spark.sql.execution.SQLWindowFunctionSuite</className>
          <testName>SPARK-7595: Window will cause resolve failed with self join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.215</duration>
          <className>org.apache.spark.sql.execution.SQLWindowFunctionSuite</className>
          <testName>SPARK-16633: lead/lag should return the default value if the offset row does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.34</duration>
          <className>org.apache.spark.sql.execution.SQLWindowFunctionSuite</className>
          <testName>lead/lag should respect null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.SortSuite.xml</file>
      <name>org.apache.spark.sql.execution.SortSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>45.072994</duration>
      <cases>
        <case>
          <duration>0.206</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>basic sorting using ExternalSort</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.191</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting all nulls</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.166</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sort followed by limit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.56</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting does not crash for large inputs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.121</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting updates peak execution memory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.575</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on StringType with nullable=true, sortOrder=List(&apos;a ASC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.589</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on StringType with nullable=true, sortOrder=List(&apos;a ASC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.555</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on StringType with nullable=true, sortOrder=List(&apos;a DESC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.513</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on StringType with nullable=true, sortOrder=List(&apos;a DESC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.538</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on StringType with nullable=false, sortOrder=List(&apos;a ASC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.501</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on StringType with nullable=false, sortOrder=List(&apos;a ASC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.533</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on StringType with nullable=false, sortOrder=List(&apos;a DESC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.532</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on StringType with nullable=false, sortOrder=List(&apos;a DESC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.463</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on NullType with nullable=true, sortOrder=List(&apos;a ASC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.425</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on NullType with nullable=true, sortOrder=List(&apos;a ASC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.455</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on NullType with nullable=true, sortOrder=List(&apos;a DESC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.446</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on NullType with nullable=true, sortOrder=List(&apos;a DESC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.438</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on NullType with nullable=false, sortOrder=List(&apos;a ASC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.443</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on NullType with nullable=false, sortOrder=List(&apos;a ASC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.483</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on NullType with nullable=false, sortOrder=List(&apos;a DESC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.46</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on NullType with nullable=false, sortOrder=List(&apos;a DESC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.386</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on LongType with nullable=true, sortOrder=List(&apos;a ASC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.377</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on LongType with nullable=true, sortOrder=List(&apos;a ASC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.377</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on LongType with nullable=true, sortOrder=List(&apos;a DESC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.347</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on LongType with nullable=true, sortOrder=List(&apos;a DESC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.344</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on LongType with nullable=false, sortOrder=List(&apos;a ASC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.344</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on LongType with nullable=false, sortOrder=List(&apos;a ASC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.349</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on LongType with nullable=false, sortOrder=List(&apos;a DESC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.345</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on LongType with nullable=false, sortOrder=List(&apos;a DESC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.365</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on IntegerType with nullable=true, sortOrder=List(&apos;a ASC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.341</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on IntegerType with nullable=true, sortOrder=List(&apos;a ASC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.365</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on IntegerType with nullable=true, sortOrder=List(&apos;a DESC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.346</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on IntegerType with nullable=true, sortOrder=List(&apos;a DESC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.361</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on IntegerType with nullable=false, sortOrder=List(&apos;a ASC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.375</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on IntegerType with nullable=false, sortOrder=List(&apos;a ASC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.382</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on IntegerType with nullable=false, sortOrder=List(&apos;a DESC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.319</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on IntegerType with nullable=false, sortOrder=List(&apos;a DESC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.424</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DecimalType(20,5) with nullable=true, sortOrder=List(&apos;a ASC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.371</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DecimalType(20,5) with nullable=true, sortOrder=List(&apos;a ASC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.348</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DecimalType(20,5) with nullable=true, sortOrder=List(&apos;a DESC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.328</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DecimalType(20,5) with nullable=true, sortOrder=List(&apos;a DESC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.332</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DecimalType(20,5) with nullable=false, sortOrder=List(&apos;a ASC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.339</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DecimalType(20,5) with nullable=false, sortOrder=List(&apos;a ASC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.331</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DecimalType(20,5) with nullable=false, sortOrder=List(&apos;a DESC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.328</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DecimalType(20,5) with nullable=false, sortOrder=List(&apos;a DESC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.317</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on TimestampType with nullable=true, sortOrder=List(&apos;a ASC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.352</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on TimestampType with nullable=true, sortOrder=List(&apos;a ASC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.333</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on TimestampType with nullable=true, sortOrder=List(&apos;a DESC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.352</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on TimestampType with nullable=true, sortOrder=List(&apos;a DESC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.323</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on TimestampType with nullable=false, sortOrder=List(&apos;a ASC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.342</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on TimestampType with nullable=false, sortOrder=List(&apos;a ASC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.291</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on TimestampType with nullable=false, sortOrder=List(&apos;a DESC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.331</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on TimestampType with nullable=false, sortOrder=List(&apos;a DESC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.36</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DoubleType with nullable=true, sortOrder=List(&apos;a ASC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.322</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DoubleType with nullable=true, sortOrder=List(&apos;a ASC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.328</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DoubleType with nullable=true, sortOrder=List(&apos;a DESC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.34</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DoubleType with nullable=true, sortOrder=List(&apos;a DESC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.288</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DoubleType with nullable=false, sortOrder=List(&apos;a ASC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.294</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DoubleType with nullable=false, sortOrder=List(&apos;a ASC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.336</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DoubleType with nullable=false, sortOrder=List(&apos;a DESC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.322</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DoubleType with nullable=false, sortOrder=List(&apos;a DESC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.452</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DateType with nullable=true, sortOrder=List(&apos;a ASC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.351</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DateType with nullable=true, sortOrder=List(&apos;a ASC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.334</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DateType with nullable=true, sortOrder=List(&apos;a DESC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.348</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DateType with nullable=true, sortOrder=List(&apos;a DESC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.34</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DateType with nullable=false, sortOrder=List(&apos;a ASC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.319</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DateType with nullable=false, sortOrder=List(&apos;a ASC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.362</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DateType with nullable=false, sortOrder=List(&apos;a DESC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.328</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DateType with nullable=false, sortOrder=List(&apos;a DESC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.363</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DecimalType(10,0) with nullable=true, sortOrder=List(&apos;a ASC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.34</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DecimalType(10,0) with nullable=true, sortOrder=List(&apos;a ASC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.351</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DecimalType(10,0) with nullable=true, sortOrder=List(&apos;a DESC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.338</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DecimalType(10,0) with nullable=true, sortOrder=List(&apos;a DESC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.32</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DecimalType(10,0) with nullable=false, sortOrder=List(&apos;a ASC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.343</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DecimalType(10,0) with nullable=false, sortOrder=List(&apos;a ASC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.349</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DecimalType(10,0) with nullable=false, sortOrder=List(&apos;a DESC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.315</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DecimalType(10,0) with nullable=false, sortOrder=List(&apos;a DESC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.428</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on BinaryType with nullable=true, sortOrder=List(&apos;a ASC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.397</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on BinaryType with nullable=true, sortOrder=List(&apos;a ASC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.396</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on BinaryType with nullable=true, sortOrder=List(&apos;a DESC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.387</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on BinaryType with nullable=true, sortOrder=List(&apos;a DESC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.364</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on BinaryType with nullable=false, sortOrder=List(&apos;a ASC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.391</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on BinaryType with nullable=false, sortOrder=List(&apos;a ASC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.349</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on BinaryType with nullable=false, sortOrder=List(&apos;a DESC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.372</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on BinaryType with nullable=false, sortOrder=List(&apos;a DESC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.311</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on BooleanType with nullable=true, sortOrder=List(&apos;a ASC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.321</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on BooleanType with nullable=true, sortOrder=List(&apos;a ASC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.293</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on BooleanType with nullable=true, sortOrder=List(&apos;a DESC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.293</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on BooleanType with nullable=true, sortOrder=List(&apos;a DESC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.313</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on BooleanType with nullable=false, sortOrder=List(&apos;a ASC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.291</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on BooleanType with nullable=false, sortOrder=List(&apos;a ASC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.313</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on BooleanType with nullable=false, sortOrder=List(&apos;a DESC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.29</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on BooleanType with nullable=false, sortOrder=List(&apos;a DESC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.392</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DecimalType(38,18) with nullable=true, sortOrder=List(&apos;a ASC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.376</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DecimalType(38,18) with nullable=true, sortOrder=List(&apos;a ASC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.393</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DecimalType(38,18) with nullable=true, sortOrder=List(&apos;a DESC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.369</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DecimalType(38,18) with nullable=true, sortOrder=List(&apos;a DESC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.334</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DecimalType(38,18) with nullable=false, sortOrder=List(&apos;a ASC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.348</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DecimalType(38,18) with nullable=false, sortOrder=List(&apos;a ASC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.359</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DecimalType(38,18) with nullable=false, sortOrder=List(&apos;a DESC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.342</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on DecimalType(38,18) with nullable=false, sortOrder=List(&apos;a DESC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.343</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on ByteType with nullable=true, sortOrder=List(&apos;a ASC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.33</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on ByteType with nullable=true, sortOrder=List(&apos;a ASC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.317</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on ByteType with nullable=true, sortOrder=List(&apos;a DESC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.338</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on ByteType with nullable=true, sortOrder=List(&apos;a DESC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.34</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on ByteType with nullable=false, sortOrder=List(&apos;a ASC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.316</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on ByteType with nullable=false, sortOrder=List(&apos;a ASC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.321</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on ByteType with nullable=false, sortOrder=List(&apos;a DESC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.342</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on ByteType with nullable=false, sortOrder=List(&apos;a DESC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.392</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on FloatType with nullable=true, sortOrder=List(&apos;a ASC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.312</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on FloatType with nullable=true, sortOrder=List(&apos;a ASC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.383</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on FloatType with nullable=true, sortOrder=List(&apos;a DESC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.377</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on FloatType with nullable=true, sortOrder=List(&apos;a DESC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.329</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on FloatType with nullable=false, sortOrder=List(&apos;a ASC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.336</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on FloatType with nullable=false, sortOrder=List(&apos;a ASC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.345</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on FloatType with nullable=false, sortOrder=List(&apos;a DESC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.325</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on FloatType with nullable=false, sortOrder=List(&apos;a DESC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.375</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on ShortType with nullable=true, sortOrder=List(&apos;a ASC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.336</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on ShortType with nullable=true, sortOrder=List(&apos;a ASC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.327</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on ShortType with nullable=true, sortOrder=List(&apos;a DESC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.36</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on ShortType with nullable=true, sortOrder=List(&apos;a DESC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.369</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on ShortType with nullable=false, sortOrder=List(&apos;a ASC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.304</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on ShortType with nullable=false, sortOrder=List(&apos;a ASC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.315</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on ShortType with nullable=false, sortOrder=List(&apos;a DESC NULLS LAST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.318</duration>
          <className>org.apache.spark.sql.execution.SortSuite</className>
          <testName>sorting on ShortType with nullable=false, sortOrder=List(&apos;a DESC NULLS FIRST)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.SparkPlannerSuite.xml</file>
      <name>org.apache.spark.sql.execution.SparkPlannerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.073</duration>
      <cases>
        <case>
          <duration>0.073</duration>
          <className>org.apache.spark.sql.execution.SparkPlannerSuite</className>
          <testName>Ensure to go down only the first branch, not any other possible branches</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.SparkSqlParserSuite.xml</file>
      <name>org.apache.spark.sql.execution.SparkSqlParserSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.017</duration>
      <cases>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.execution.SparkSqlParserSuite</className>
          <testName>show functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.SparkSqlParserSuite</className>
          <testName>describe function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.execution.SparkSqlParserSuite</className>
          <testName>create table - schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.SparkSqlParserSuite</className>
          <testName>create table using - schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.execution.SparkSqlParserSuite</className>
          <testName>SPARK-17328 Fix NPE with EXPLAIN DESCRIBE TABLE</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.SparkSqlParserSuite</className>
          <testName>analyze table statistics</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.SparkSqlParserSuite</className>
          <testName>analyze table column statistics</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.TakeOrderedAndProjectSuite.xml</file>
      <name>org.apache.spark.sql.execution.TakeOrderedAndProjectSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.65900004</duration>
      <cases>
        <case>
          <duration>0.366</duration>
          <className>org.apache.spark.sql.execution.TakeOrderedAndProjectSuite</className>
          <testName>doExecute without project</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.293</duration>
          <className>org.apache.spark.sql.execution.TakeOrderedAndProjectSuite</className>
          <testName>doExecute with project</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMapSuite.xml</file>
      <name>org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMapSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.399</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMapSuite</className>
          <testName>supported schemas</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMapSuite</className>
          <testName>empty map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.059</duration>
          <className>org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMapSuite</className>
          <testName>updating values for a single key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.114</duration>
          <className>org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMapSuite</className>
          <testName>inserting large random keys</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.213</duration>
          <className>org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMapSuite</className>
          <testName>test external sorting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.129</duration>
          <className>org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMapSuite</className>
          <testName>test external sorting with an empty map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.86</duration>
          <className>org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMapSuite</className>
          <testName>test external sorting with empty records</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMapSuite</className>
          <testName>convert to external sorter under memory pressure (SPARK-10474)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite.xml</file>
      <name>org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.5769999</duration>
      <cases>
        <case>
          <duration>0.074</duration>
          <className>org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite</className>
          <testName>kv sorting key schema [] and value schema []</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.095</duration>
          <className>org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite</className>
          <testName>kv sorting key schema [int] and value schema []</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.062</duration>
          <className>org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite</className>
          <testName>kv sorting key schema [] and value schema [int]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite</className>
          <testName>kv sorting key schema [int] and value schema [float,float,double,string,float]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.376</duration>
          <className>org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite</className>
          <testName>kv sorting key schema [double,string,string,int,float,string,string] and value schema [double,int,string,int,double,string,double]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.21</duration>
          <className>org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite</className>
          <testName>kv sorting key schema [int,string,float,int,int,string] and value schema [double,float,float,string,string,double,float,float,float,float]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.299</duration>
          <className>org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite</className>
          <testName>kv sorting key schema [string,double,int,int,string,string] and value schema [int,int,string,float,float,double,double]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.095</duration>
          <className>org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite</className>
          <testName>kv sorting key schema [int,float,float,int,float,float,int,int,float,int] and value schema [double,float,float,double]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.175</duration>
          <className>org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite</className>
          <testName>kv sorting key schema [double,int,string,double,float,float] and value schema [float,string]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.16</duration>
          <className>org.apache.spark.sql.execution.UnsafeKVExternalSorterSuite</className>
          <testName>kv sorting with records that exceed page size</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.UnsafeRowSerializerSuite.xml</file>
      <name>org.apache.spark.sql.execution.UnsafeRowSerializerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.39600003</duration>
      <cases>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.execution.UnsafeRowSerializerSuite</className>
          <testName>toUnsafeRow() test helper method</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.execution.UnsafeRowSerializerSuite</className>
          <testName>basic row serialization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.UnsafeRowSerializerSuite</className>
          <testName>close empty input stream</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.312</duration>
          <className>org.apache.spark.sql.execution.UnsafeRowSerializerSuite</className>
          <testName>SPARK-10466: external sorter spilling with unsafe row serializer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.071</duration>
          <className>org.apache.spark.sql.execution.UnsafeRowSerializerSuite</className>
          <testName>SPARK-10403: unsafe row serializer with SortShuffleManager</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.WholeStageCodegenSuite.xml</file>
      <name>org.apache.spark.sql.execution.WholeStageCodegenSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.79999995</duration>
      <cases>
        <case>
          <duration>0.057</duration>
          <className>org.apache.spark.sql.execution.WholeStageCodegenSuite</className>
          <testName>range/filter should be combined</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.08</duration>
          <className>org.apache.spark.sql.execution.WholeStageCodegenSuite</className>
          <testName>Aggregate should be included in WholeStageCodegen</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.219</duration>
          <className>org.apache.spark.sql.execution.WholeStageCodegenSuite</className>
          <testName>Aggregate with grouping keys should be included in WholeStageCodegen</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.088</duration>
          <className>org.apache.spark.sql.execution.WholeStageCodegenSuite</className>
          <testName>BroadcastHashJoin should be included in WholeStageCodegen</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.062</duration>
          <className>org.apache.spark.sql.execution.WholeStageCodegenSuite</className>
          <testName>Sort should be included in WholeStageCodegen</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.sql.execution.WholeStageCodegenSuite</className>
          <testName>MapElements should be included in WholeStageCodegen</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.039</duration>
          <className>org.apache.spark.sql.execution.WholeStageCodegenSuite</className>
          <testName>typed filter should be included in WholeStageCodegen</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.033</duration>
          <className>org.apache.spark.sql.execution.WholeStageCodegenSuite</className>
          <testName>back-to-back typed filter should be included in WholeStageCodegen</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.18</duration>
          <className>org.apache.spark.sql.execution.WholeStageCodegenSuite</className>
          <testName>simple typed UDAF should be included in WholeStageCodegen</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.benchmark.AggregateBenchmark.xml</file>
      <name>org.apache.spark.sql.execution.benchmark.AggregateBenchmark</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>-0.009000001</duration>
      <cases>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.execution.benchmark.AggregateBenchmark</className>
          <testName>aggregate without grouping</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.execution.benchmark.AggregateBenchmark</className>
          <testName>stat functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.execution.benchmark.AggregateBenchmark</className>
          <testName>aggregate with linear keys</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.execution.benchmark.AggregateBenchmark</className>
          <testName>aggregate with randomized keys</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.execution.benchmark.AggregateBenchmark</className>
          <testName>aggregate with string key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.execution.benchmark.AggregateBenchmark</className>
          <testName>aggregate with decimal key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.execution.benchmark.AggregateBenchmark</className>
          <testName>aggregate with multiple key types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.execution.benchmark.AggregateBenchmark</className>
          <testName>cube</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.execution.benchmark.AggregateBenchmark</className>
          <testName>hash and BytesToBytesMap</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.benchmark.BenchmarkWideTable.xml</file>
      <name>org.apache.spark.sql.execution.benchmark.BenchmarkWideTable</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>-0.001</duration>
      <cases>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.execution.benchmark.BenchmarkWideTable</className>
          <testName>project on wide table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.benchmark.JoinBenchmark.xml</file>
      <name>org.apache.spark.sql.execution.benchmark.JoinBenchmark</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>-0.010000001</duration>
      <cases>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.execution.benchmark.JoinBenchmark</className>
          <testName>broadcast hash join, long key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.execution.benchmark.JoinBenchmark</className>
          <testName>broadcast hash join, long key with duplicates</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.execution.benchmark.JoinBenchmark</className>
          <testName>broadcast hash join, two int key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.execution.benchmark.JoinBenchmark</className>
          <testName>broadcast hash join, two long key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.execution.benchmark.JoinBenchmark</className>
          <testName>broadcast hash join, two long key with duplicates</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.execution.benchmark.JoinBenchmark</className>
          <testName>broadcast hash join, outer join long key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.execution.benchmark.JoinBenchmark</className>
          <testName>broadcast hash join, semi join long key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.execution.benchmark.JoinBenchmark</className>
          <testName>sort merge join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.execution.benchmark.JoinBenchmark</className>
          <testName>sort merge join with duplicates</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.execution.benchmark.JoinBenchmark</className>
          <testName>shuffle hash join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.benchmark.MiscBenchmark.xml</file>
      <name>org.apache.spark.sql.execution.benchmark.MiscBenchmark</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>-0.0050000004</duration>
      <cases>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.execution.benchmark.MiscBenchmark</className>
          <testName>filter &amp; aggregate without group</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.execution.benchmark.MiscBenchmark</className>
          <testName>range/limit/sum</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.execution.benchmark.MiscBenchmark</className>
          <testName>sample</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.execution.benchmark.MiscBenchmark</className>
          <testName>collect</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.execution.benchmark.MiscBenchmark</className>
          <testName>collect limit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.benchmark.PrimitiveArrayBenchmark.xml</file>
      <name>org.apache.spark.sql.execution.benchmark.PrimitiveArrayBenchmark</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>-0.001</duration>
      <cases>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.execution.benchmark.PrimitiveArrayBenchmark</className>
          <testName>Write an array in Dataset</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.benchmark.SortBenchmark.xml</file>
      <name>org.apache.spark.sql.execution.benchmark.SortBenchmark</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>-0.001</duration>
      <cases>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.execution.benchmark.SortBenchmark</className>
          <testName>sort</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.benchmark.UnsafeArrayDataBenchmark.xml</file>
      <name>org.apache.spark.sql.execution.benchmark.UnsafeArrayDataBenchmark</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>-0.001</duration>
      <cases>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.execution.benchmark.UnsafeArrayDataBenchmark</className>
          <testName>Benchmark UnsafeArrayData</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.columnar.ColumnStatsSuite.xml</file>
      <name>org.apache.spark.sql.execution.columnar.ColumnStatsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.012</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.ColumnStatsSuite</className>
          <testName>BooleanColumnStats: empty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.execution.columnar.ColumnStatsSuite</className>
          <testName>BooleanColumnStats: non-empty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.ColumnStatsSuite</className>
          <testName>ByteColumnStats: empty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.ColumnStatsSuite</className>
          <testName>ByteColumnStats: non-empty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.ColumnStatsSuite</className>
          <testName>ShortColumnStats: empty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.ColumnStatsSuite</className>
          <testName>ShortColumnStats: non-empty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.ColumnStatsSuite</className>
          <testName>IntColumnStats: empty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.ColumnStatsSuite</className>
          <testName>IntColumnStats: non-empty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.ColumnStatsSuite</className>
          <testName>LongColumnStats: empty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.ColumnStatsSuite</className>
          <testName>LongColumnStats: non-empty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.ColumnStatsSuite</className>
          <testName>FloatColumnStats: empty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.ColumnStatsSuite</className>
          <testName>FloatColumnStats: non-empty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.ColumnStatsSuite</className>
          <testName>DoubleColumnStats: empty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.ColumnStatsSuite</className>
          <testName>DoubleColumnStats: non-empty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.ColumnStatsSuite</className>
          <testName>StringColumnStats: empty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.ColumnStatsSuite</className>
          <testName>StringColumnStats: non-empty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.ColumnStatsSuite</className>
          <testName>DecimalColumnStats: empty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.execution.columnar.ColumnStatsSuite</className>
          <testName>DecimalColumnStats: non-empty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.columnar.ColumnTypeSuite.xml</file>
      <name>org.apache.spark.sql.execution.columnar.ColumnTypeSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.017</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.ColumnTypeSuite</className>
          <testName>defaultSize</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.execution.columnar.ColumnTypeSuite</className>
          <testName>actualSize</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.columnar.ColumnTypeSuite</className>
          <testName>BOOLEAN append/extract</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.ColumnTypeSuite</className>
          <testName>BYTE append/extract</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.ColumnTypeSuite</className>
          <testName>SHORT append/extract</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.ColumnTypeSuite</className>
          <testName>INT append/extract</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.ColumnTypeSuite</className>
          <testName>LONG append/extract</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.ColumnTypeSuite</className>
          <testName>FLOAT append/extract</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.ColumnTypeSuite</className>
          <testName>DOUBLE append/extract</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.ColumnTypeSuite</className>
          <testName>COMPACT_DECIMAL append/extract</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.ColumnTypeSuite</className>
          <testName>STRING append/extract</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.ColumnTypeSuite</className>
          <testName>NULL append/extract</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.ColumnTypeSuite</className>
          <testName>BINARY append/extract</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.ColumnTypeSuite</className>
          <testName>LARGE_DECIMAL append/extract</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.ColumnTypeSuite</className>
          <testName>STRUCT append/extract</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.ColumnTypeSuite</className>
          <testName>ARRAY append/extract</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.ColumnTypeSuite</className>
          <testName>MAP append/extract</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.ColumnTypeSuite</className>
          <testName>column type for decimal types with different precision</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.columnar.InMemoryColumnarQuerySuite.xml</file>
      <name>org.apache.spark.sql.execution.columnar.InMemoryColumnarQuerySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>9.747</duration>
      <cases>
        <case>
          <duration>0.216</duration>
          <className>org.apache.spark.sql.execution.columnar.InMemoryColumnarQuerySuite</className>
          <testName>simple columnar query</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.sql.execution.columnar.InMemoryColumnarQuerySuite</className>
          <testName>default size avoids broadcast</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.102</duration>
          <className>org.apache.spark.sql.execution.columnar.InMemoryColumnarQuerySuite</className>
          <testName>projection</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.132</duration>
          <className>org.apache.spark.sql.execution.columnar.InMemoryColumnarQuerySuite</className>
          <testName>SPARK-1436 regression: in-memory columns must be able to be accessed multiple times</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.13</duration>
          <className>org.apache.spark.sql.execution.columnar.InMemoryColumnarQuerySuite</className>
          <testName>SPARK-1678 regression: compression must not lose repeated values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.099</duration>
          <className>org.apache.spark.sql.execution.columnar.InMemoryColumnarQuerySuite</className>
          <testName>with null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.149</duration>
          <className>org.apache.spark.sql.execution.columnar.InMemoryColumnarQuerySuite</className>
          <testName>SPARK-2729 regression: timestamp data type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.178</duration>
          <className>org.apache.spark.sql.execution.columnar.InMemoryColumnarQuerySuite</className>
          <testName>SPARK-3320 regression: batched column buffer building should work with empty partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.141</duration>
          <className>org.apache.spark.sql.execution.columnar.InMemoryColumnarQuerySuite</className>
          <testName>SPARK-4182 Caching complex types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.092</duration>
          <className>org.apache.spark.sql.execution.columnar.InMemoryColumnarQuerySuite</className>
          <testName>decimal type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>6.876</duration>
          <className>org.apache.spark.sql.execution.columnar.InMemoryColumnarQuerySuite</className>
          <testName>test different data types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.156</duration>
          <className>org.apache.spark.sql.execution.columnar.InMemoryColumnarQuerySuite</className>
          <testName>SPARK-10422: String column in InMemoryColumnarCache needs to override clone method</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.108</duration>
          <className>org.apache.spark.sql.execution.columnar.InMemoryColumnarQuerySuite</className>
          <testName>SPARK-10859: Predicates pushed to InMemoryColumnarTableScan are not evaluated correctly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.265</duration>
          <className>org.apache.spark.sql.execution.columnar.InMemoryColumnarQuerySuite</className>
          <testName>SPARK-14138: Generated SpecificColumnarIterator can exceed JVM size limit for cached DF</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.072</duration>
          <className>org.apache.spark.sql.execution.columnar.InMemoryColumnarQuerySuite</className>
          <testName>SPARK-17549: cached table size should be correctly calculated</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.columnar.NullableColumnAccessorSuite.xml</file>
      <name>org.apache.spark.sql.execution.columnar.NullableColumnAccessorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.08099999</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnAccessorSuite</className>
          <testName>Nullable NULL column accessor: empty column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnAccessorSuite</className>
          <testName>Nullable NULL column accessor: access null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnAccessorSuite</className>
          <testName>Nullable BOOLEAN column accessor: empty column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnAccessorSuite</className>
          <testName>Nullable BOOLEAN column accessor: access null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnAccessorSuite</className>
          <testName>Nullable BYTE column accessor: empty column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnAccessorSuite</className>
          <testName>Nullable BYTE column accessor: access null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnAccessorSuite</className>
          <testName>Nullable SHORT column accessor: empty column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnAccessorSuite</className>
          <testName>Nullable SHORT column accessor: access null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnAccessorSuite</className>
          <testName>Nullable INT column accessor: empty column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnAccessorSuite</className>
          <testName>Nullable INT column accessor: access null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnAccessorSuite</className>
          <testName>Nullable LONG column accessor: empty column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnAccessorSuite</className>
          <testName>Nullable LONG column accessor: access null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnAccessorSuite</className>
          <testName>Nullable FLOAT column accessor: empty column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnAccessorSuite</className>
          <testName>Nullable FLOAT column accessor: access null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnAccessorSuite</className>
          <testName>Nullable DOUBLE column accessor: empty column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnAccessorSuite</className>
          <testName>Nullable DOUBLE column accessor: access null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnAccessorSuite</className>
          <testName>Nullable STRING column accessor: empty column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnAccessorSuite</className>
          <testName>Nullable STRING column accessor: access null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnAccessorSuite</className>
          <testName>Nullable BINARY column accessor: empty column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnAccessorSuite</className>
          <testName>Nullable BINARY column accessor: access null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnAccessorSuite</className>
          <testName>Nullable COMPACT_DECIMAL column accessor: empty column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnAccessorSuite</className>
          <testName>Nullable COMPACT_DECIMAL column accessor: access null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnAccessorSuite</className>
          <testName>Nullable LARGE_DECIMAL column accessor: empty column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnAccessorSuite</className>
          <testName>Nullable LARGE_DECIMAL column accessor: access null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnAccessorSuite</className>
          <testName>Nullable STRUCT column accessor: empty column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnAccessorSuite</className>
          <testName>Nullable STRUCT column accessor: access null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnAccessorSuite</className>
          <testName>Nullable ARRAY column accessor: empty column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnAccessorSuite</className>
          <testName>Nullable ARRAY column accessor: access null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnAccessorSuite</className>
          <testName>Nullable MAP column accessor: empty column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnAccessorSuite</className>
          <testName>Nullable MAP column accessor: access null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite.xml</file>
      <name>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.032</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>BOOLEAN column builder: empty column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>BOOLEAN column builder: buffer size auto growth</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>BOOLEAN column builder: null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>BYTE column builder: empty column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>BYTE column builder: buffer size auto growth</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>BYTE column builder: null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>SHORT column builder: empty column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>SHORT column builder: buffer size auto growth</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>SHORT column builder: null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>INT column builder: empty column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>INT column builder: buffer size auto growth</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>INT column builder: null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>LONG column builder: empty column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>LONG column builder: buffer size auto growth</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>LONG column builder: null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>FLOAT column builder: empty column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>FLOAT column builder: buffer size auto growth</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>FLOAT column builder: null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>DOUBLE column builder: empty column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>DOUBLE column builder: buffer size auto growth</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>DOUBLE column builder: null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>STRING column builder: empty column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>STRING column builder: buffer size auto growth</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>STRING column builder: null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>BINARY column builder: empty column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>BINARY column builder: buffer size auto growth</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>BINARY column builder: null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>COMPACT_DECIMAL column builder: empty column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>COMPACT_DECIMAL column builder: buffer size auto growth</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>COMPACT_DECIMAL column builder: null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>LARGE_DECIMAL column builder: empty column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>LARGE_DECIMAL column builder: buffer size auto growth</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>LARGE_DECIMAL column builder: null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>STRUCT column builder: empty column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>STRUCT column builder: buffer size auto growth</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>STRUCT column builder: null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>ARRAY column builder: empty column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>ARRAY column builder: buffer size auto growth</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>ARRAY column builder: null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>MAP column builder: empty column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>MAP column builder: buffer size auto growth</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.columnar.NullableColumnBuilderSuite</className>
          <testName>MAP column builder: null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite.xml</file>
      <name>org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.3590001</duration>
      <cases>
        <case>
          <duration>0.055</duration>
          <className>org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite</className>
          <testName>SELECT key FROM pruningData WHERE key = 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.035</duration>
          <className>org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite</className>
          <testName>SELECT key FROM pruningData WHERE 1 = key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.039</duration>
          <className>org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite</className>
          <testName>SELECT key FROM pruningData WHERE key &lt;=&gt; 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.038</duration>
          <className>org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite</className>
          <testName>SELECT key FROM pruningData WHERE 1 &lt;=&gt; key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite</className>
          <testName>SELECT key FROM pruningData WHERE key &lt; 12</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite</className>
          <testName>SELECT key FROM pruningData WHERE key &lt;= 11</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.064</duration>
          <className>org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite</className>
          <testName>SELECT key FROM pruningData WHERE key &gt; 88</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.041</duration>
          <className>org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite</className>
          <testName>SELECT key FROM pruningData WHERE key &gt;= 89</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.041</duration>
          <className>org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite</className>
          <testName>SELECT key FROM pruningData WHERE 12 &gt; key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite</className>
          <testName>SELECT key FROM pruningData WHERE 11 &gt;= key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite</className>
          <testName>SELECT key FROM pruningData WHERE 88 &lt; key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite</className>
          <testName>SELECT key FROM pruningData WHERE 89 &lt;= key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.053</duration>
          <className>org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite</className>
          <testName>SELECT key FROM pruningData WHERE value IS NULL</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite</className>
          <testName>SELECT key FROM pruningData WHERE value IS NOT NULL</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.046</duration>
          <className>org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite</className>
          <testName>SELECT key FROM pruningData WHERE key &gt; 8 AND key &lt;= 21</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite</className>
          <testName>SELECT key FROM pruningData WHERE key &lt; 2 OR key &gt; 99</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.057</duration>
          <className>org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite</className>
          <testName>SELECT key FROM pruningData WHERE key &lt; 12 AND key IS NOT NULL</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.044</duration>
          <className>org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite</className>
          <testName>SELECT key FROM pruningData WHERE key &lt; 2 OR (key &gt; 78 AND key &lt; 92)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.041</duration>
          <className>org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite</className>
          <testName>SELECT key FROM pruningData WHERE NOT (key &lt; 88)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.04</duration>
          <className>org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite</className>
          <testName>SELECT key FROM pruningData WHERE key IN (1)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.041</duration>
          <className>org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite</className>
          <testName>SELECT key FROM pruningData WHERE key IN (1, 2)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.046</duration>
          <className>org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite</className>
          <testName>SELECT key FROM pruningData WHERE key IN (1, 11)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.046</duration>
          <className>org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite</className>
          <testName>SELECT key FROM pruningData WHERE key IN (1, 21, 41, 61, 81)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.056</duration>
          <className>org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite</className>
          <testName>SELECT CAST(s AS INT) FROM pruningStringData WHERE s = &apos;100&apos;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.044</duration>
          <className>org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite</className>
          <testName>SELECT CAST(s AS INT) FROM pruningStringData WHERE s &lt; &apos;102&apos;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.046</duration>
          <className>org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite</className>
          <testName>SELECT CAST(s AS INT) FROM pruningStringData WHERE s IN (&apos;99&apos;, &apos;150&apos;, &apos;201&apos;)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.079</duration>
          <className>org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite</className>
          <testName>SELECT key FROM pruningData WHERE key IN (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite</className>
          <testName>SELECT key FROM pruningData WHERE NOT (key IN (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30))</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.049</duration>
          <className>org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite</className>
          <testName>SELECT key FROM pruningData WHERE NOT (key IN (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30)) AND key &gt; 88</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.sql.execution.columnar.PartitionBatchPruningSuite</className>
          <testName>disable IN_MEMORY_PARTITION_PRUNING</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.columnar.compression.BooleanBitSetSuite.xml</file>
      <name>org.apache.spark.sql.execution.columnar.compression.BooleanBitSetSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.032</duration>
      <cases>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.BooleanBitSetSuite</className>
          <testName>BooleanBitSet: empty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.BooleanBitSetSuite</className>
          <testName>BooleanBitSet: less than 1 word</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.BooleanBitSetSuite</className>
          <testName>BooleanBitSet: exactly 1 word</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.BooleanBitSetSuite</className>
          <testName>BooleanBitSet: multiple whole words</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.BooleanBitSetSuite</className>
          <testName>BooleanBitSet: multiple words and 1 more bit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.columnar.compression.DictionaryEncodingSuite.xml</file>
      <name>org.apache.spark.sql.execution.columnar.compression.DictionaryEncodingSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.41500002</duration>
      <cases>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.DictionaryEncodingSuite</className>
          <testName>DictionaryEncoding with INT: empty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.DictionaryEncodingSuite</className>
          <testName>DictionaryEncoding with INT: simple case</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.156</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.DictionaryEncodingSuite</className>
          <testName>DictionaryEncoding with INT: dictionary overflow</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.DictionaryEncodingSuite</className>
          <testName>DictionaryEncoding with LONG: empty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.DictionaryEncodingSuite</className>
          <testName>DictionaryEncoding with LONG: simple case</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.06</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.DictionaryEncodingSuite</className>
          <testName>DictionaryEncoding with LONG: dictionary overflow</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.DictionaryEncodingSuite</className>
          <testName>DictionaryEncoding with STRING: empty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.DictionaryEncodingSuite</className>
          <testName>DictionaryEncoding with STRING: simple case</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.187</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.DictionaryEncodingSuite</className>
          <testName>DictionaryEncoding with STRING: dictionary overflow</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.columnar.compression.IntegralDeltaSuite.xml</file>
      <name>org.apache.spark.sql.execution.columnar.compression.IntegralDeltaSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.154</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.IntegralDeltaSuite</className>
          <testName>IntDelta: empty column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.IntegralDeltaSuite</className>
          <testName>IntDelta: simple case</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.096</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.IntegralDeltaSuite</className>
          <testName>IntDelta: long random series</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.IntegralDeltaSuite</className>
          <testName>LongDelta: empty column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.IntegralDeltaSuite</className>
          <testName>LongDelta: simple case</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.05</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.IntegralDeltaSuite</className>
          <testName>LongDelta: long random series</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.columnar.compression.RunLengthEncodingSuite.xml</file>
      <name>org.apache.spark.sql.execution.columnar.compression.RunLengthEncodingSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.036</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.RunLengthEncodingSuite</className>
          <testName>RunLengthEncoding with BOOLEAN: empty column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.RunLengthEncodingSuite</className>
          <testName>RunLengthEncoding with BOOLEAN: simple case</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.RunLengthEncodingSuite</className>
          <testName>RunLengthEncoding with BOOLEAN: run length == 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.RunLengthEncodingSuite</className>
          <testName>RunLengthEncoding with BOOLEAN: single long run</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.RunLengthEncodingSuite</className>
          <testName>RunLengthEncoding with BYTE: empty column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.RunLengthEncodingSuite</className>
          <testName>RunLengthEncoding with BYTE: simple case</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.RunLengthEncodingSuite</className>
          <testName>RunLengthEncoding with BYTE: run length == 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.RunLengthEncodingSuite</className>
          <testName>RunLengthEncoding with BYTE: single long run</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.RunLengthEncodingSuite</className>
          <testName>RunLengthEncoding with SHORT: empty column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.RunLengthEncodingSuite</className>
          <testName>RunLengthEncoding with SHORT: simple case</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.RunLengthEncodingSuite</className>
          <testName>RunLengthEncoding with SHORT: run length == 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.RunLengthEncodingSuite</className>
          <testName>RunLengthEncoding with SHORT: single long run</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.RunLengthEncodingSuite</className>
          <testName>RunLengthEncoding with INT: empty column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.RunLengthEncodingSuite</className>
          <testName>RunLengthEncoding with INT: simple case</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.RunLengthEncodingSuite</className>
          <testName>RunLengthEncoding with INT: run length == 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.RunLengthEncodingSuite</className>
          <testName>RunLengthEncoding with INT: single long run</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.RunLengthEncodingSuite</className>
          <testName>RunLengthEncoding with LONG: empty column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.RunLengthEncodingSuite</className>
          <testName>RunLengthEncoding with LONG: simple case</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.RunLengthEncodingSuite</className>
          <testName>RunLengthEncoding with LONG: run length == 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.RunLengthEncodingSuite</className>
          <testName>RunLengthEncoding with LONG: single long run</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.RunLengthEncodingSuite</className>
          <testName>RunLengthEncoding with STRING: empty column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.RunLengthEncodingSuite</className>
          <testName>RunLengthEncoding with STRING: simple case</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.RunLengthEncodingSuite</className>
          <testName>RunLengthEncoding with STRING: run length == 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.execution.columnar.compression.RunLengthEncodingSuite</className>
          <testName>RunLengthEncoding with STRING: single long run</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.command.DDLCommandSuite.xml</file>
      <name>org.apache.spark.sql.execution.command.DDLCommandSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.14200003</duration>
      <cases>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>create database</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>create database - property values must be set</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>drop database</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>alter database set dbproperties</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>alter database - property values must be set</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>describe database</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>create function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>drop function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>create table - table file format</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>create table - row format and table file format</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>create table - row format serde and generic file format</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>create table - row format delimited and generic file format</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>create external table - location must be specified</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>create table - property values must be set</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>create table - location implies external</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>create table using - with partitioned by</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>create table using - with bucket</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>alter table/view: rename table/view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>alter table: rename table with database</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>alter table/view: alter table/view properties</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>alter table - property values must be set</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>alter table unset properties - property values must NOT be set</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>alter table: SerDe properties</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>alter table - SerDe property values must be set</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>alter table: add partition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>alter table: recover partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>alter view: add partition (not supported)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>alter table: rename partition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>alter table: exchange partition (not supported)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>alter table/view: drop partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>alter table: archive partition (not supported)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>alter table: unarchive partition (not supported)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>alter table: set file format (not allowed)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>alter table: set location</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>alter table: touch (not supported)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>alter table: compact (not supported)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>alter table: concatenate (not supported)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>alter table: cluster by (not supported)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>alter table: skewed by (not supported)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>alter table: change column name/type/position/comment (not allowed)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>alter table: add/replace columns (not allowed)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>show databases</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>show tblproperties</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>SPARK-14383: DISTRIBUTE and UNSET as non-keywords</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>duplicate keys in table properties</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>duplicate columns in partition specs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>drop table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>drop view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>show columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>show partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>support for other types in DBPROPERTIES</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>support for other types in TBLPROPERTIES</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.command.DDLCommandSuite</className>
          <testName>support for other types in OPTIONS</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.command.DDLSuite.xml</file>
      <name>org.apache.spark.sql.execution.command.DDLSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>6.1109996</duration>
      <cases>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>the qualified path of a database is stored in the catalog</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>Create Database using Default Warehouse Path</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>Create/Drop Database - location</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>Create Database - database already exists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.383</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>Create partitioned data source table without user specified schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.282</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>Create partitioned data source table with user specified schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.235</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>Create non-partitioned data source table without user specified schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.179</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>Create non-partitioned data source table with user specified schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>create table - duplicate column names in the table definition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>create table - partition column names not in table definition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>create table - bucket column names not in table definition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>create table - column repeated in partition columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>create table - column repeated in bucket columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.48</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>Refresh table after changing the data source table partitioning</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.062</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>desc table for parquet data source table using in-memory catalog</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.129</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>Alter/Describe Database</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>Drop/Alter/Describe Database - database does not exists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>drop non-empty database in restrict mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>drop non-empty database in cascade mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>create table in default db</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>create table in a specific db</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>create table using</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>create table using - with partitioned by</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>create table using - with bucket</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.229</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>create temporary view using</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>alter table: rename</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.191</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>alter table: rename cached table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>rename temporary table - destination table with database name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.115</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>rename temporary table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>rename temporary table - destination table already exists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.018</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>alter table: set location</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>alter table: set location (datasource table)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>alter table: set properties</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>alter table: set properties (datasource table)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>alter table: unset properties</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>alter table: unset properties (datasource table)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>alter table: set serde</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>alter table: set serde (datasource table)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.018</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>alter table: set serde partition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>alter table: set serde partition (datasource table)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>alter table: bucketing is not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>alter table: skew is not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>alter table: add partition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>alter table: add partition (datasource table)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>alter table: recover partitions (sequential)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.165</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>alter table: recover partition (parallel)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>alter table: add partition is not supported for views</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>alter table: drop partition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>alter table: drop partition (datasource table)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>alter table: drop partition is not supported for views</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>alter table: rename partition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>alter table: rename partition (datasource table)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.167</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>show tables</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.1</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>show databases</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>drop table - temporary table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>drop table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>drop table - data source table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>drop view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>drop build-in function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.119</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>describe function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>select/insert into the managed table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>select/insert into external table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.083</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>create table using CLUSTERED BY without schema specification</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.095</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>Create Hive Table As Select</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.446</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>Create Data Source Table As Select</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>drop current database</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>drop default database</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.294</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>truncate table - datasource table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.758</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>truncate partitioned table - datasource table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.105</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>create temporary view with mismatched schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.055</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>create temporary view with specified schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.045</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>truncate table - external table, temporary table, view (not allowed)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>truncate table - non-partitioned table (not allowed)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.461</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>SPARK-16034 Partition columns should match when appending to existing data source tables</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.333</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>show functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.018</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>show columns - negative test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.execution.command.DDLSuite</className>
          <testName>SPARK-18009 calling toLocalIterator on commands</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.datasources.FileIndexSuite.xml</file>
      <name>org.apache.spark.sql.execution.datasources.FileIndexSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.269</duration>
      <cases>
        <case>
          <duration>0.196</duration>
          <className>org.apache.spark.sql.execution.datasources.FileIndexSuite</className>
          <testName>InMemoryFileIndex: leaf files are qualified paths</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.05</duration>
          <className>org.apache.spark.sql.execution.datasources.FileIndexSuite</className>
          <testName>InMemoryFileIndex: input paths are converted to qualified paths</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.execution.datasources.FileIndexSuite</className>
          <testName>InMemoryFileIndex: folders that don&apos;t exist don&apos;t throw exceptions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.FileIndexSuite</className>
          <testName>PartitioningAwareFileIndex - file filtering</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.execution.datasources.FileIndexSuite</className>
          <testName>SPARK-17613 - PartitioningAwareFileIndex: base path w/o &apos;/&apos; at end</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.datasources.FileSourceStrategySuite.xml</file>
      <name>org.apache.spark.sql.execution.datasources.FileSourceStrategySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.926</duration>
      <cases>
        <case>
          <duration>0.032</duration>
          <className>org.apache.spark.sql.execution.datasources.FileSourceStrategySuite</className>
          <testName>unpartitioned table, single partition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.execution.datasources.FileSourceStrategySuite</className>
          <testName>unpartitioned table, multiple partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.execution.datasources.FileSourceStrategySuite</className>
          <testName>Unpartitioned table, large file that gets split</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.sql.execution.datasources.FileSourceStrategySuite</className>
          <testName>Unpartitioned table, many files that get split</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.sql.execution.datasources.FileSourceStrategySuite</className>
          <testName>partitioned table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.sql.execution.datasources.FileSourceStrategySuite</className>
          <testName>partitioned table - case insensitive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.sql.execution.datasources.FileSourceStrategySuite</className>
          <testName>partitioned table - after scan filters</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.execution.datasources.FileSourceStrategySuite</className>
          <testName>bucketed table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.datasources.FileSourceStrategySuite</className>
          <testName>Locality support for FileScanRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.execution.datasources.FileSourceStrategySuite</className>
          <testName>Locality support for FileScanRDD - one file per partition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.execution.datasources.FileSourceStrategySuite</className>
          <testName>Locality support for FileScanRDD - large file</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.sql.execution.datasources.FileSourceStrategySuite</className>
          <testName>SPARK-15654 do not split non-splittable files</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.099</duration>
          <className>org.apache.spark.sql.execution.datasources.FileSourceStrategySuite</className>
          <testName>SPARK-14959: Do not call getFileBlockLocations on directories</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.957</duration>
          <className>org.apache.spark.sql.execution.datasources.FileSourceStrategySuite</className>
          <testName>[SPARK-16818] partition pruned file scans implement sameResult correctly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.617</duration>
          <className>org.apache.spark.sql.execution.datasources.FileSourceStrategySuite</className>
          <testName>[SPARK-16818] exchange reuse respects differences in partition pruning</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.066</duration>
          <className>org.apache.spark.sql.execution.datasources.FileSourceStrategySuite</className>
          <testName>ignoreCorruptFiles should work in SQL</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.datasources.HadoopFsRelationSuite.xml</file>
      <name>org.apache.spark.sql.execution.datasources.HadoopFsRelationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.178</duration>
      <cases>
        <case>
          <duration>0.178</duration>
          <className>org.apache.spark.sql.execution.datasources.HadoopFsRelationSuite</className>
          <testName>sizeInBytes should be the total size of all files</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.datasources.RowDataSourceStrategySuite.xml</file>
      <name>org.apache.spark.sql.execution.datasources.RowDataSourceStrategySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.431</duration>
      <cases>
        <case>
          <duration>0.431</duration>
          <className>org.apache.spark.sql.execution.datasources.RowDataSourceStrategySuite</className>
          <testName>SPARK-17673: Exchange reuse respects differences in output schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.datasources.csv.CSVInferSchemaSuite.xml</file>
      <name>org.apache.spark.sql.execution.datasources.csv.CSVInferSchemaSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.041999996</duration>
      <cases>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVInferSchemaSuite</className>
          <testName>String fields types are inferred correctly from null types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVInferSchemaSuite</className>
          <testName>String fields types are inferred correctly from other types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVInferSchemaSuite</className>
          <testName>Timestamp field types are inferred correctly via custom data format</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVInferSchemaSuite</className>
          <testName>Timestamp field types are inferred correctly from other types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVInferSchemaSuite</className>
          <testName>Boolean fields types are inferred correctly from other types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVInferSchemaSuite</className>
          <testName>Type arrays are merged to highest common type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVInferSchemaSuite</className>
          <testName>Null fields are handled properly when a nullValue is specified</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVInferSchemaSuite</className>
          <testName>Merging Nulltypes should yield Nulltype</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVInferSchemaSuite</className>
          <testName>SPARK-18433: Improve DataSource option keys to be more case-insensitive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.datasources.csv.CSVSuite.xml</file>
      <name>org.apache.spark.sql.execution.datasources.csv.CSVSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>6.233999</duration>
      <cases>
        <case>
          <duration>0.163</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>simple csv test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.113</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>simple csv test with calling another function to load</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.141</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>simple csv test with type inference</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.085</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>test inferring booleans</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.077</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>test inferring decimals</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.126</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>test with alternative delimiter and quote</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.11</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>parse unescaped quotes with maxCharsPerColumn</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>bad encoding name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.176</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>test different encoding</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.125</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>test aliases sep and encoding for delimiter and charset</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.136</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>DDL test with tab separated file</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.054</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>DDL test parsing decimal type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.087</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>test for DROPMALFORMED parsing mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.191</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>test for blank column names on read and select columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.096</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>test for FAILFAST parsing mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.112</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>test for tokens more than the fields in the schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.097</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>test with null quote character</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>test with empty file and known schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.061</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>DDL test with empty file</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.053</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>DDL test with schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.268</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>save csv</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.296</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>save csv with quote</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.107</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>save csv with quoteAll enabled</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.1</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>save csv with quote escaping enabled</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.102</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>save csv with quote escaping disabled</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.094</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>commented lines in CSV data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.108</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>inferring schema with commented lines in CSV data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.13</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>inferring timestamp types via custom date format</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>load date types via custom date format</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.077</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>setting comment to null disables comment support</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.086</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>nullable fields with user defined null value of &quot;null&quot;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.268</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>save csv with compression codec option</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.285</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>SPARK-13543 Write the output as uncompressed via option()</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.065</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>Schema inference correctly identifies the datatype when data is sparse</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.115</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>old csv data source name works</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>nulls, NaNs and Infinity values can be parsed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.246</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>error handling for unsupported data types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.093</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>SPARK-15585 turn off quotations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.283</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>Write timestamps correctly in ISO8601 format by default</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.233</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>Write dates correctly in ISO8601 format by default</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.295</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>Roundtrip in reading and writing timestamps</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.207</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>Write dates correctly with dateFormat option</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.243</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>Write timestamps correctly with dateFormat option</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.137</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>load duplicated field names consistently with null or empty strings - case sensitive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.112</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>load duplicated field names consistently with null or empty strings - case insensitive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.121</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVSuite</className>
          <testName>load null when the schema is larger than parsed tokens</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.datasources.csv.CSVTypeCastSuite.xml</file>
      <name>org.apache.spark.sql.execution.datasources.csv.CSVTypeCastSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.010000001</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVTypeCastSuite</className>
          <testName>Can parse decimal type values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVTypeCastSuite</className>
          <testName>Can parse escaped characters</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVTypeCastSuite</className>
          <testName>Does not accept delimiter larger than one character</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVTypeCastSuite</className>
          <testName>Throws exception for unsupported escaped characters</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVTypeCastSuite</className>
          <testName>Nullable types are handled</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVTypeCastSuite</className>
          <testName>String type should also respect `nullValue`</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVTypeCastSuite</className>
          <testName>Throws exception for empty string with non null type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVTypeCastSuite</className>
          <testName>Types are cast correctly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVTypeCastSuite</className>
          <testName>Float and Double Types are cast without respect to platform default Locale</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVTypeCastSuite</className>
          <testName>Float NaN values are parsed correctly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVTypeCastSuite</className>
          <testName>Double NaN values are parsed correctly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVTypeCastSuite</className>
          <testName>Float infinite values can be parsed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.csv.CSVTypeCastSuite</className>
          <testName>Double infinite values can be parsed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.datasources.json.JsonParsingOptionsSuite.xml</file>
      <name>org.apache.spark.sql.execution.datasources.json.JsonParsingOptionsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.274</duration>
      <cases>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonParsingOptionsSuite</className>
          <testName>allowComments off</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.045</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonParsingOptionsSuite</className>
          <testName>allowComments on</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonParsingOptionsSuite</className>
          <testName>allowSingleQuotes off</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonParsingOptionsSuite</className>
          <testName>allowSingleQuotes on</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonParsingOptionsSuite</className>
          <testName>allowUnquotedFieldNames off</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonParsingOptionsSuite</className>
          <testName>allowUnquotedFieldNames on</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonParsingOptionsSuite</className>
          <testName>allowNumericLeadingZeros off</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonParsingOptionsSuite</className>
          <testName>allowNumericLeadingZeros on</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonParsingOptionsSuite</className>
          <testName>allowNonNumericNumbers off</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonParsingOptionsSuite</className>
          <testName>allowNonNumericNumbers on</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonParsingOptionsSuite</className>
          <testName>allowBackslashEscapingAnyCharacter off</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.055</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonParsingOptionsSuite</className>
          <testName>allowBackslashEscapingAnyCharacter on</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.datasources.json.JsonSuite.xml</file>
      <name>org.apache.spark.sql.execution.datasources.json.JsonSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>8.818001</duration>
      <cases>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>Type promotion</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>Get compatible type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.102</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>Complex field and type inferring with null in sampling</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.061</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>Primitive field and type inferring</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.697</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>Complex field and type inferring</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.159</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>GetField operation on complex data type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.494</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>Type conflict in primitive field values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>Type conflict in primitive field values (Ignored)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.098</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>Type conflict in complex field values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.131</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>Type conflict in array elements</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>Handling missing fields</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.179</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>Loading a JSON dataset from a text file</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.17</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>Loading a JSON dataset primitivesAsString returns schema with primitive types as strings</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.711</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>Loading a JSON dataset primitivesAsString returns complex fields as strings</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.065</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>Loading a JSON dataset prefersDecimal returns schema with float types as BigDecimal</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.051</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>Find compatible types even if inferred DecimalType is not capable of other IntegralType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.051</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>Infer big integers correctly even when it does not fit in decimal</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.113</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>Infer floating-point values correctly even when it does not fit in decimal</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.171</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>Loading a JSON dataset from a text file with SQL</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.158</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>Applying schemas</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.265</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>Applying schemas with MapType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.173</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>SPARK-2096 Correctly parse dot notations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.17</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>SPARK-3390 Complex arrays</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.054</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>SPARK-3308 Read top level JSON arrays</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.053</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>Corrupt records: FAILFAST mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.072</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>Corrupt records: DROPMALFORMED mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.039</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>Corrupt records: PERMISSIVE mode, without designated column for malformed records</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.196</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>Corrupt records: PERMISSIVE mode, with designated column for malformed records</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.052</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>SPARK-13953 Rename the corrupt record field via option</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.097</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>SPARK-4068: nulls in arrays</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.939</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>SPARK-4228 DataFrame to JSON</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.108</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>JSONRelation equality test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>inferSchema on empty RDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.408</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>SPARK-7565 MapType in JsonRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>SPARK-8093 Erase empty structs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.469</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>JSON with Partition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.378</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>backward compatibility</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.329</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>SPARK-11544 test pathfilter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.066</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>SPARK-12057 additional corrupt records do not throw exceptions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.104</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>Parse JSON rows having an array type and a struct type in the same field</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.405</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>SPARK-12872 Support to specify the option for compression codec</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.371</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>SPARK-13543 Write the output as uncompressed via option()</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.053</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>Casting long as timestamp</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.275</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>wide nested json table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.12</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>Write dates correctly with dateFormat option</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.13</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>Write timestamps correctly with dateFormat option</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.sql.execution.datasources.json.JsonSuite</className>
          <testName>SPARK-18433: Improve DataSource option keys to be more case-insensitive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.datasources.parquet.ParquetAvroCompatibilitySuite.xml</file>
      <name>org.apache.spark.sql.execution.datasources.parquet.ParquetAvroCompatibilitySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.5280001</duration>
      <cases>
        <case>
          <duration>0.289</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetAvroCompatibilitySuite</className>
          <testName>required primitives</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.17</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetAvroCompatibilitySuite</className>
          <testName>optional primitives</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.193</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetAvroCompatibilitySuite</className>
          <testName>non-nullable arrays</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetAvroCompatibilitySuite</className>
          <testName>0 does not properly support this)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.184</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetAvroCompatibilitySuite</className>
          <testName>SPARK-10136 array of primitive array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.202</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetAvroCompatibilitySuite</className>
          <testName>map of primitive array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.289</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetAvroCompatibilitySuite</className>
          <testName>various complex types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.202</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetAvroCompatibilitySuite</className>
          <testName>SPARK-9407 Push down predicates involving Parquet ENUM columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.datasources.parquet.ParquetEncodingSuite.xml</file>
      <name>org.apache.spark.sql.execution.datasources.parquet.ParquetEncodingSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.62899995</duration>
      <cases>
        <case>
          <duration>0.288</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetEncodingSuite</className>
          <testName>All Types Dictionary</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.214</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetEncodingSuite</className>
          <testName>All Types Null</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.127</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetEncodingSuite</className>
          <testName>Read row group containing both dictionary and plain encoded pages</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.datasources.parquet.ParquetFilterSuite.xml</file>
      <name>org.apache.spark.sql.execution.datasources.parquet.ParquetFilterSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>17.157999</duration>
      <cases>
        <case>
          <duration>0.756</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetFilterSuite</className>
          <testName>filter pushdown - boolean</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.011</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetFilterSuite</className>
          <testName>filter pushdown - integer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.033</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetFilterSuite</className>
          <testName>filter pushdown - long</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.053</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetFilterSuite</className>
          <testName>filter pushdown - float</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.132</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetFilterSuite</className>
          <testName>filter pushdown - double</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.038</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetFilterSuite</className>
          <testName>filter pushdown - string</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.577</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetFilterSuite</className>
          <testName>filter pushdown - binary</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.213</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetFilterSuite</className>
          <testName>SPARK-6554: don&apos;t push down predicates which reference partition columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.239</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetFilterSuite</className>
          <testName>SPARK-10829: Filter combine partition key and attribute doesn&apos;t work in DataSource scan</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.249</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetFilterSuite</className>
          <testName>SPARK-12231: test the filter and empty project in partitioned DataSource scan</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.326</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetFilterSuite</className>
          <testName>SPARK-12231: test the new projection in partitioned DataSource scan</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.299</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetFilterSuite</className>
          <testName>SPARK-11103: Filter applied on merged Parquet schema with new column fails</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.181</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetFilterSuite</className>
          <testName>SPARK-11661 Still pushdown filters returned by unhandledFilters</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.338</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetFilterSuite</className>
          <testName>SPARK-12218: &apos;Not&apos; is included in Parquet filter pushdown</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetFilterSuite</className>
          <testName>SPARK-12218 Converting conjunctions into Parquet filter predicates</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.291</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetFilterSuite</className>
          <testName>SPARK-16371 Do not push down filters when inner name and outer name are the same</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.421</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetFilterSuite</className>
          <testName>Fiters should be pushed down for vectorized Parquet reader at row group level</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.xml</file>
      <name>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>16.776999</duration>
      <cases>
        <case>
          <duration>0.414</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>basic data types (without binary)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.243</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>raw binary</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.126</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>SPARK-11694 Parquet logical types are not being tested properly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.617</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>string</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.047</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>Standard mode - fixed-length decimals</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.633</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>Legacy mode - fixed-length decimals</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.389</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>date type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.35</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>Standard mode - map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.366</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>Legacy mode - map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.336</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>Standard mode - array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.336</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>Legacy mode - array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.399</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>Standard mode - array and double</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.346</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>Legacy mode - array and double</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.377</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>Standard mode - struct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.3</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>Legacy mode - struct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.407</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>Standard mode - nested struct with array of array as field</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.343</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>Legacy mode - nested struct with array of array as field</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.398</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>Standard mode - nested map with struct as value type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.356</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>Legacy mode - nested map with struct as value type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.221</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>nulls</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.247</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>nones</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.061</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>SPARK-10113 Support for unsigned Parquet logical types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.086</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>SPARK-11692 Support for Parquet logical types, JSON and BSON (embedded types)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.257</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>compression codec</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.263</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>read raw Parquet file</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.018</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>write metadata</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.393</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>save - overwrite</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.318</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>save - ignore</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.079</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>save - throw</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.35</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>save - append</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.16</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>SPARK-6315 regression test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.062</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>class shouldn&apos;t be overridden</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.188</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>SPARK-6330 regression test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.157</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>SPARK-7837 Do not close output writer twice when commitTask() fails</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.213</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>SPARK-11044 Parquet writer version fixed as version1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.367</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>null and non-null strings</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.293</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>read dictionary encoded decimals written as INT32</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.311</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>read dictionary encoded decimals written as INT64</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.301</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>read dictionary encoded decimals written as FIXED_LEN_BYTE_ARRAY</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.245</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>SPARK-12589 copy() on rows returned from reader works for strings</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.131</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>VectorizedParquetRecordReader - direct path read</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.272</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>VectorizedParquetRecordReader - partition column types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite</className>
          <testName>SPARK-18433: Improve DataSource option keys to be more case-insensitive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.datasources.parquet.ParquetInteroperabilitySuite.xml</file>
      <name>org.apache.spark.sql.execution.datasources.parquet.ParquetInteroperabilitySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.236</duration>
      <cases>
        <case>
          <duration>0.236</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetInteroperabilitySuite</className>
          <testName>parquet files with different physical schemas but share the same logical schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite.xml</file>
      <name>org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>7.463001</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite</className>
          <testName>column type inference</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite</className>
          <testName>parse invalid partitioned directories</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite</className>
          <testName>parse partition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite</className>
          <testName>parse partition with base paths</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite</className>
          <testName>parse partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite</className>
          <testName>parse partitions with type inference disabled</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.687</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite</className>
          <testName>read partitioned table - normal case</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.39</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite</className>
          <testName>read partitioned table using different path options</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.642</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite</className>
          <testName>read partitioned table - partition key included in Parquet file</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.53</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite</className>
          <testName>read partitioned table - with nulls</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.479</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite</className>
          <testName>read partitioned table - with nulls and partition keys are included in Parquet file</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.309</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite</className>
          <testName>read partitioned table - merging compatible schemas</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.117</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite</className>
          <testName>SPARK-7749 Non-partitioned table should have empty partition spec</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.245</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite</className>
          <testName>SPARK-7847: Dynamic partition directory path escaping and unescaping</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.385</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite</className>
          <testName>Various partition value types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.305</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite</className>
          <testName>Various inferred partition value types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.307</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite</className>
          <testName>SPARK-8037: Ignores files whose name starts with dot</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.444</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite</className>
          <testName>SPARK-11678: Partition discovery stops at the root path of the dataset</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.254</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite</className>
          <testName>use basePath to specify the root dir of a partitioned table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.144</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite</className>
          <testName>use basePath and file globbing to selectively load partitioned table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.58</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite</className>
          <testName>_SUCCESS should not break partitioning discovery</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite</className>
          <testName>listConflictingPartitionColumns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.437</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite</className>
          <testName>Parallel partition discovery</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.188</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite</className>
          <testName>SPARK-15895 summary files in non-leaf partition directories</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.datasources.parquet.ParquetProtobufCompatibilitySuite.xml</file>
      <name>org.apache.spark.sql.execution.datasources.parquet.ParquetProtobufCompatibilitySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.161</duration>
      <cases>
        <case>
          <duration>0.203</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetProtobufCompatibilitySuite</className>
          <testName>unannotated array of primitive type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.438</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetProtobufCompatibilitySuite</className>
          <testName>unannotated array of struct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.167</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetProtobufCompatibilitySuite</className>
          <testName>struct with unannotated array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.204</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetProtobufCompatibilitySuite</className>
          <testName>unannotated array of struct with unannotated array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.149</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetProtobufCompatibilitySuite</className>
          <testName>unannotated array of string</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite.xml</file>
      <name>org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>9.827001</duration>
      <cases>
        <case>
          <duration>0.505</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite</className>
          <testName>simple select queries</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.305</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite</className>
          <testName>appending</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.449</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite</className>
          <testName>overwriting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.453</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite</className>
          <testName>SPARK-15678: not use cache on overwrite</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.396</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite</className>
          <testName>SPARK-15678: not use cache on append</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.284</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite</className>
          <testName>self-join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.363</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite</className>
          <testName>nested data - struct with array field</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.345</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite</className>
          <testName>nested data - array of struct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.292</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite</className>
          <testName>SPARK-1913 regression: columns only referenced by pushed down filters should remain</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.08</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite</className>
          <testName>SPARK-5309 strings stored using dictionary compression in parquet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.264</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite</className>
          <testName>SPARK-6917 DecimalType should work with non-native types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.395</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite</className>
          <testName>Enabling/disabling merging partfiles when merging parquet schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.394</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite</className>
          <testName>Enabling/disabling schema merging</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.231</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite</className>
          <testName>parquet() should respect user specified options</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.183</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite</className>
          <testName>SPARK-9119 Decimal should be correctly written into parquet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.373</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite</className>
          <testName>SPARK-10005 Schema merging for nested struct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.184</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite</className>
          <testName>SPARK-10301 requested schema clipping - same schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.274</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite</className>
          <testName>SPARK-11997 parquet with null partition values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite</className>
          <testName>SPARK-10301 requested schema clipping - schemas with disjoint sets of fields</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.325</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite</className>
          <testName>SPARK-10301 requested schema clipping - requested schema contains physical schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.306</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite</className>
          <testName>SPARK-10301 requested schema clipping - physical schema contains requested schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.178</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite</className>
          <testName>SPARK-10301 requested schema clipping - schemas overlap but don&apos;t contain each other</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.202</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite</className>
          <testName>SPARK-10301 requested schema clipping - deeply nested struct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.257</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite</className>
          <testName>SPARK-10301 requested schema clipping - out of order</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.296</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite</className>
          <testName>SPARK-10301 requested schema clipping - schema merging</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.214</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite</className>
          <testName>Standard mode - SPARK-10301 requested schema clipping - UDT</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.174</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite</className>
          <testName>Legacy mode - SPARK-10301 requested schema clipping - UDT</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite</className>
          <testName>expand UDT in StructType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite</className>
          <testName>expand UDT in ArrayType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite</className>
          <testName>expand UDT in MapType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.376</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite</className>
          <testName>returning batch for wide table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.08</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite</className>
          <testName>SPARK-15719: disable writing summary files by default</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.213</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite</className>
          <testName>SPARK-15804: write out the metadata to parquet file</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.231</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite</className>
          <testName>SPARK-16344: array of struct with a single field named &apos;element&apos;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.205</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetQuerySuite</className>
          <testName>SPARK-16632: read Parquet int32 as ByteType and ShortType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaInferenceSuite.xml</file>
      <name>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaInferenceSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.0070000007</duration>
      <cases>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaInferenceSuite</className>
          <testName>sql =&gt; parquet: basic types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaInferenceSuite</className>
          <testName>sql &lt;= parquet: basic types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaInferenceSuite</className>
          <testName>sql =&gt; parquet: logical integral types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaInferenceSuite</className>
          <testName>sql &lt;= parquet: logical integral types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaInferenceSuite</className>
          <testName>sql =&gt; parquet: string</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaInferenceSuite</className>
          <testName>sql &lt;= parquet: string</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaInferenceSuite</className>
          <testName>sql =&gt; parquet: binary enum as string</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaInferenceSuite</className>
          <testName>sql &lt;= parquet: binary enum as string</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaInferenceSuite</className>
          <testName>sql =&gt; parquet: non-nullable array - non-standard</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaInferenceSuite</className>
          <testName>sql &lt;= parquet: non-nullable array - non-standard</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaInferenceSuite</className>
          <testName>sql =&gt; parquet: non-nullable array - standard</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaInferenceSuite</className>
          <testName>sql &lt;= parquet: non-nullable array - standard</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaInferenceSuite</className>
          <testName>sql =&gt; parquet: nullable array - non-standard</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaInferenceSuite</className>
          <testName>sql &lt;= parquet: nullable array - non-standard</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaInferenceSuite</className>
          <testName>sql =&gt; parquet: nullable array - standard</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaInferenceSuite</className>
          <testName>sql &lt;= parquet: nullable array - standard</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaInferenceSuite</className>
          <testName>sql =&gt; parquet: map - standard</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaInferenceSuite</className>
          <testName>sql &lt;= parquet: map - standard</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaInferenceSuite</className>
          <testName>sql =&gt; parquet: map - non-standard</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaInferenceSuite</className>
          <testName>sql &lt;= parquet: map - non-standard</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaInferenceSuite</className>
          <testName>sql =&gt; parquet: struct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaInferenceSuite</className>
          <testName>sql &lt;= parquet: struct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaInferenceSuite</className>
          <testName>sql =&gt; parquet: deeply nested type - non-standard</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaInferenceSuite</className>
          <testName>sql &lt;= parquet: deeply nested type - non-standard</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaInferenceSuite</className>
          <testName>sql =&gt; parquet: deeply nested type - standard</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaInferenceSuite</className>
          <testName>sql &lt;= parquet: deeply nested type - standard</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaInferenceSuite</className>
          <testName>sql =&gt; parquet: optional types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaInferenceSuite</className>
          <testName>sql &lt;= parquet: optional types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite.xml</file>
      <name>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.23000003</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>DataType string parser compatibility</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>merge with metastore schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>merge missing nullable fields from Metastore schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.207</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>schema merging failure error message</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>sql &lt;= parquet: Backwards-compatibility: LIST with nullable element type - 1 - standard</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>sql &lt;= parquet: Backwards-compatibility: LIST with nullable element type - 2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>sql &lt;= parquet: Backwards-compatibility: LIST with non-nullable element type - 1 - standard</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>sql &lt;= parquet: Backwards-compatibility: LIST with non-nullable element type - 2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>sql &lt;= parquet: Backwards-compatibility: LIST with non-nullable element type - 3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>sql &lt;= parquet: Backwards-compatibility: LIST with non-nullable element type - 4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>sql &lt;= parquet: Backwards-compatibility: LIST with non-nullable element type - 5 - parquet-avro style</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>sql &lt;= parquet: Backwards-compatibility: LIST with non-nullable element type - 6 - parquet-thrift style</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>sql &lt;= parquet: Backwards-compatibility: LIST with non-nullable element type 7 - parquet-protobuf primitive lists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>sql &lt;= parquet: Backwards-compatibility: LIST with non-nullable element type 8 - parquet-protobuf non-primitive lists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>sql =&gt; parquet: Backwards-compatibility: LIST with nullable element type - 1 - standard</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>x</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>sql =&gt; parquet: Backwards-compatibility: LIST with non-nullable element type - 1 - standard</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>x</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>sql &lt;= parquet: Backwards-compatibility: MAP with non-nullable value type - 1 - standard</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>sql &lt;= parquet: Backwards-compatibility: MAP with non-nullable value type - 2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>x</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>sql &lt;= parquet: Backwards-compatibility: MAP with nullable value type - 1 - standard</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>sql &lt;= parquet: Backwards-compatibility: MAP with nullable value type - 2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>sql &lt;= parquet: Backwards-compatibility: MAP with nullable value type - 3 - parquet-avro style</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>sql =&gt; parquet: Backwards-compatibility: MAP with non-nullable value type - 1 - standard</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>x</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>sql =&gt; parquet: Backwards-compatibility: MAP with nullable value type - 1 - standard</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>x</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>sql =&gt; parquet: DECIMAL(1, 0) - standard</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>sql &lt;= parquet: DECIMAL(1, 0) - standard</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>sql =&gt; parquet: DECIMAL(8, 3) - standard</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>sql &lt;= parquet: DECIMAL(8, 3) - standard</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>sql =&gt; parquet: DECIMAL(9, 3) - standard</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>sql &lt;= parquet: DECIMAL(9, 3) - standard</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>sql =&gt; parquet: DECIMAL(18, 3) - standard</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>sql &lt;= parquet: DECIMAL(18, 3) - standard</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>sql =&gt; parquet: DECIMAL(19, 3) - standard</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>sql &lt;= parquet: DECIMAL(19, 3) - standard</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>x</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>x</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>x</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>x</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>x</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>x</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>x</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>x</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>Clipping - simple nested struct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>Clipping - parquet-protobuf style array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>Clipping - parquet-thrift style array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>Clipping - parquet-avro style array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>Clipping - parquet-hive style array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>Clipping - 2-level list of required struct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>Clipping - standard array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>Clipping - empty requested schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>Clipping - disjoint field sets</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>Clipping - parquet-avro style map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>Clipping - standard map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaSuite</className>
          <testName>Clipping - standard map with complex key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.datasources.parquet.ParquetThriftCompatibilitySuite.xml</file>
      <name>org.apache.spark.sql.execution.datasources.parquet.ParquetThriftCompatibilitySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.674</duration>
      <cases>
        <case>
          <duration>0.476</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetThriftCompatibilitySuite</className>
          <testName>Read Parquet file generated by parquet-thrift</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.198</duration>
          <className>org.apache.spark.sql.execution.datasources.parquet.ParquetThriftCompatibilitySuite</className>
          <testName>SPARK-10136 list of primitive list</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.datasources.text.TextSuite.xml</file>
      <name>org.apache.spark.sql.execution.datasources.text.TextSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.47</duration>
      <cases>
        <case>
          <duration>0.075</duration>
          <className>org.apache.spark.sql.execution.datasources.text.TextSuite</className>
          <testName>reading text file</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.sql.execution.datasources.text.TextSuite</className>
          <testName>text() API</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.129</duration>
          <className>org.apache.spark.sql.execution.datasources.text.TextSuite</className>
          <testName>text() can handle column name beyond `value`</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.execution.datasources.text.TextSuite</className>
          <testName>error handling for invalid schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.07</duration>
          <className>org.apache.spark.sql.execution.datasources.text.TextSuite</className>
          <testName>textFile()</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.041</duration>
          <className>org.apache.spark.sql.execution.datasources.text.TextSuite</className>
          <testName>text()</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.406</duration>
          <className>org.apache.spark.sql.execution.datasources.text.TextSuite</className>
          <testName>SPARK-13503 Support to specify the option for compression codec for TEXT</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.112</duration>
          <className>org.apache.spark.sql.execution.datasources.text.TextSuite</className>
          <testName>SPARK-13543 Write the output as uncompressed via option()</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.218</duration>
          <className>org.apache.spark.sql.execution.datasources.text.TextSuite</className>
          <testName>SPARK-14343: select partitioning column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.371</duration>
          <className>org.apache.spark.sql.execution.datasources.text.TextSuite</className>
          <testName>SPARK-15654: should not split gz files</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.debug.DebuggingSuite.xml</file>
      <name>org.apache.spark.sql.execution.debug.DebuggingSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.158</duration>
      <cases>
        <case>
          <duration>0.092</duration>
          <className>org.apache.spark.sql.execution.debug.DebuggingSuite</className>
          <testName>debug()</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.sql.execution.debug.DebuggingSuite</className>
          <testName>debug()</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.032</duration>
          <className>org.apache.spark.sql.execution.debug.DebuggingSuite</className>
          <testName>debugCodegen</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.joins.BroadcastJoinSuite.xml</file>
      <name>org.apache.spark.sql.execution.joins.BroadcastJoinSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>4.5199995</duration>
      <cases>
        <case>
          <duration>4.122</duration>
          <className>org.apache.spark.sql.execution.joins.BroadcastJoinSuite</className>
          <testName>unsafe broadcast hash join updates peak execution memory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.118</duration>
          <className>org.apache.spark.sql.execution.joins.BroadcastJoinSuite</className>
          <testName>unsafe broadcast hash outer join updates peak execution memory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.121</duration>
          <className>org.apache.spark.sql.execution.joins.BroadcastJoinSuite</className>
          <testName>unsafe broadcast left semi join updates peak execution memory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.sql.execution.joins.BroadcastJoinSuite</className>
          <testName>broadcast hint isn&apos;t bothered by authBroadcastJoinThreshold set to low values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.sql.execution.joins.BroadcastJoinSuite</className>
          <testName>broadcast hint isn&apos;t bothered by a disabled authBroadcastJoinThreshold</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.sql.execution.joins.BroadcastJoinSuite</className>
          <testName>broadcast hint isn&apos;t propagated after a join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.086</duration>
          <className>org.apache.spark.sql.execution.joins.BroadcastJoinSuite</className>
          <testName>broadcast hint is propagated correctly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.joins.BroadcastJoinSuite</className>
          <testName>join key rewritten</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.joins.ExistenceJoinSuite.xml</file>
      <name>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>3.6920004</duration>
      <cases>
        <case>
          <duration>0.175</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test single condition (equal) for left semi join using ShuffledHashJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.103</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test single condition (equal) for left semi join using BroadcastHashJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.19</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test single condition (equal) for left semi join using SortMergeJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.098</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test single condition (equal) for left semi join using BroadcastNestedLoopJoin build left</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.094</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test single condition (equal) for left semi join using BroadcastNestedLoopJoin build right</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.089</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test composed condition (equal &amp; non-equal) for left semi join using ShuffledHashJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.089</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test composed condition (equal &amp; non-equal) for left semi join using BroadcastHashJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.15</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test composed condition (equal &amp; non-equal) for left semi join using SortMergeJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.102</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test composed condition (equal &amp; non-equal) for left semi join using BroadcastNestedLoopJoin build left</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.067</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test composed condition (equal &amp; non-equal) for left semi join using BroadcastNestedLoopJoin build right</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test composed condition (both non-equal) for left semi join using ShuffledHashJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test composed condition (both non-equal) for left semi join using BroadcastHashJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test composed condition (both non-equal) for left semi join using SortMergeJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.087</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test composed condition (both non-equal) for left semi join using BroadcastNestedLoopJoin build left</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.069</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test composed condition (both non-equal) for left semi join using BroadcastNestedLoopJoin build right</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.101</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test single condition (equal) for left Anti join using ShuffledHashJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.073</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test single condition (equal) for left Anti join using BroadcastHashJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.158</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test single condition (equal) for left Anti join using SortMergeJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.074</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test single condition (equal) for left Anti join using BroadcastNestedLoopJoin build left</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.079</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test single condition (equal) for left Anti join using BroadcastNestedLoopJoin build right</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.167</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test single unique condition (equal) for left Anti join using ShuffledHashJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.153</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test single unique condition (equal) for left Anti join using BroadcastHashJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.226</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test single unique condition (equal) for left Anti join using SortMergeJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.151</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test single unique condition (equal) for left Anti join using BroadcastNestedLoopJoin build left</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.14</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test single unique condition (equal) for left Anti join using BroadcastNestedLoopJoin build right</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.076</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test composed condition (equal &amp; non-equal) test for anti join using ShuffledHashJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.082</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test composed condition (equal &amp; non-equal) test for anti join using BroadcastHashJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.147</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test composed condition (equal &amp; non-equal) test for anti join using SortMergeJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.089</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test composed condition (equal &amp; non-equal) test for anti join using BroadcastNestedLoopJoin build left</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.06</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test composed condition (equal &amp; non-equal) test for anti join using BroadcastNestedLoopJoin build right</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test composed condition (both non-equal) for anti join using ShuffledHashJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test composed condition (both non-equal) for anti join using BroadcastHashJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test composed condition (both non-equal) for anti join using SortMergeJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.077</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test composed condition (both non-equal) for anti join using BroadcastNestedLoopJoin build left</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.055</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test composed condition (both non-equal) for anti join using BroadcastNestedLoopJoin build right</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.092</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test composed unique condition (both non-equal) for anti join using ShuffledHashJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.081</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test composed unique condition (both non-equal) for anti join using BroadcastHashJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.153</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test composed unique condition (both non-equal) for anti join using SortMergeJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.071</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test composed unique condition (both non-equal) for anti join using BroadcastNestedLoopJoin build left</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.073</duration>
          <className>org.apache.spark.sql.execution.joins.ExistenceJoinSuite</className>
          <testName>test composed unique condition (both non-equal) for anti join using BroadcastNestedLoopJoin build right</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.joins.HashedRelationSuite.xml</file>
      <name>org.apache.spark.sql.execution.joins.HashedRelationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.7480001</duration>
      <cases>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.execution.joins.HashedRelationSuite</className>
          <testName>UnsafeHashedRelation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.joins.HashedRelationSuite</className>
          <testName>test serialization empty hash map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.sql.execution.joins.HashedRelationSuite</className>
          <testName>LongToUnsafeRowMap</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.execution.joins.HashedRelationSuite</className>
          <testName>LongToUnsafeRowMap with very wide range</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.634</duration>
          <className>org.apache.spark.sql.execution.joins.HashedRelationSuite</className>
          <testName>LongToUnsafeRowMap with random keys</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.075</duration>
          <className>org.apache.spark.sql.execution.joins.HashedRelationSuite</className>
          <testName>Spark-14521</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.execution.joins.HashedRelationSuite</className>
          <testName>build HashedRelation that is larger than 1G</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.execution.joins.HashedRelationSuite</className>
          <testName>build HashedRelation with more than 100 millions rows</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.joins.InnerJoinSuite.xml</file>
      <name>org.apache.spark.sql.execution.joins.InnerJoinSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.3319995</duration>
      <cases>
        <case>
          <duration>0.095</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>inner join, one match per row using BroadcastHashJoin (build=left)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.035</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>inner join, one match per row using BroadcastHashJoin (build=right)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.054</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>inner join, one match per row using ShuffledHashJoin (build=left)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>inner join, one match per row using ShuffledHashJoin (build=right)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.114</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>inner join, one match per row using SortMergeJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.044</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>inner join, one match per row using CartesianProduct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>inner join, one match per row using BroadcastNestedLoopJoin build left</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>inner join, one match per row using BroadcastNestedLoopJoin build right</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.082</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>inner join, multiple matches using BroadcastHashJoin (build=left)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.035</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>inner join, multiple matches using BroadcastHashJoin (build=right)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.05</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>inner join, multiple matches using ShuffledHashJoin (build=left)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>inner join, multiple matches using ShuffledHashJoin (build=right)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.122</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>inner join, multiple matches using SortMergeJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>inner join, multiple matches using CartesianProduct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>inner join, multiple matches using BroadcastNestedLoopJoin build left</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>inner join, multiple matches using BroadcastNestedLoopJoin build right</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.044</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>inner join, no matches using BroadcastHashJoin (build=left)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>inner join, no matches using BroadcastHashJoin (build=right)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.033</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>inner join, no matches using ShuffledHashJoin (build=left)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.054</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>inner join, no matches using ShuffledHashJoin (build=right)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.077</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>inner join, no matches using SortMergeJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.035</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>inner join, no matches using CartesianProduct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>inner join, no matches using BroadcastNestedLoopJoin build left</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>inner join, no matches using BroadcastNestedLoopJoin build right</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.063</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>inner join, null safe using BroadcastHashJoin (build=left)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>inner join, null safe using BroadcastHashJoin (build=right)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>inner join, null safe using ShuffledHashJoin (build=left)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.052</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>inner join, null safe using ShuffledHashJoin (build=right)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.091</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>inner join, null safe using SortMergeJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.032</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>inner join, null safe using CartesianProduct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>inner join, null safe using BroadcastNestedLoopJoin build left</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>inner join, null safe using BroadcastNestedLoopJoin build right</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.086</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>SPARK-15822 - test structs as keys using BroadcastHashJoin (build=left)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>SPARK-15822 - test structs as keys using BroadcastHashJoin (build=right)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.391</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>SPARK-15822 - test structs as keys using ShuffledHashJoin (build=left)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.051</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>SPARK-15822 - test structs as keys using ShuffledHashJoin (build=right)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.09</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>SPARK-15822 - test structs as keys using SortMergeJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.049</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>SPARK-15822 - test structs as keys using CartesianProduct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>SPARK-15822 - test structs as keys using BroadcastNestedLoopJoin build left</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.sql.execution.joins.InnerJoinSuite</className>
          <testName>SPARK-15822 - test structs as keys using BroadcastNestedLoopJoin build right</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.joins.OuterJoinSuite.xml</file>
      <name>org.apache.spark.sql.execution.joins.OuterJoinSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.137</duration>
      <cases>
        <case>
          <duration>0.118</duration>
          <className>org.apache.spark.sql.execution.joins.OuterJoinSuite</className>
          <testName>basic left outer join using ShuffledHashJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.041</duration>
          <className>org.apache.spark.sql.execution.joins.OuterJoinSuite</className>
          <testName>basic left outer join using BroadcastHashJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.075</duration>
          <className>org.apache.spark.sql.execution.joins.OuterJoinSuite</className>
          <testName>basic left outer join using SortMergeJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.06</duration>
          <className>org.apache.spark.sql.execution.joins.OuterJoinSuite</className>
          <testName>basic left outer join using BroadcastNestedLoopJoin build left</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.sql.execution.joins.OuterJoinSuite</className>
          <testName>basic left outer join using BroadcastNestedLoopJoin build right</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.038</duration>
          <className>org.apache.spark.sql.execution.joins.OuterJoinSuite</className>
          <testName>basic right outer join using ShuffledHashJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.sql.execution.joins.OuterJoinSuite</className>
          <testName>basic right outer join using BroadcastHashJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.091</duration>
          <className>org.apache.spark.sql.execution.joins.OuterJoinSuite</className>
          <testName>basic right outer join using SortMergeJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.sql.execution.joins.OuterJoinSuite</className>
          <testName>basic right outer join using BroadcastNestedLoopJoin build left</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.035</duration>
          <className>org.apache.spark.sql.execution.joins.OuterJoinSuite</className>
          <testName>basic right outer join using BroadcastNestedLoopJoin build right</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.062</duration>
          <className>org.apache.spark.sql.execution.joins.OuterJoinSuite</className>
          <testName>basic full outer join using SortMergeJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.sql.execution.joins.OuterJoinSuite</className>
          <testName>basic full outer join using BroadcastNestedLoopJoin build left</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.sql.execution.joins.OuterJoinSuite</className>
          <testName>basic full outer join using BroadcastNestedLoopJoin build right</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.sql.execution.joins.OuterJoinSuite</className>
          <testName>left outer join with both inputs empty using ShuffledHashJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.sql.execution.joins.OuterJoinSuite</className>
          <testName>left outer join with both inputs empty using BroadcastHashJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.054</duration>
          <className>org.apache.spark.sql.execution.joins.OuterJoinSuite</className>
          <testName>left outer join with both inputs empty using SortMergeJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.sql.execution.joins.OuterJoinSuite</className>
          <testName>left outer join with both inputs empty using BroadcastNestedLoopJoin build left</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.sql.execution.joins.OuterJoinSuite</className>
          <testName>left outer join with both inputs empty using BroadcastNestedLoopJoin build right</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.sql.execution.joins.OuterJoinSuite</className>
          <testName>right outer join with both inputs empty using ShuffledHashJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.sql.execution.joins.OuterJoinSuite</className>
          <testName>right outer join with both inputs empty using BroadcastHashJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.055</duration>
          <className>org.apache.spark.sql.execution.joins.OuterJoinSuite</className>
          <testName>right outer join with both inputs empty using SortMergeJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.sql.execution.joins.OuterJoinSuite</className>
          <testName>right outer join with both inputs empty using BroadcastNestedLoopJoin build left</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.sql.execution.joins.OuterJoinSuite</className>
          <testName>right outer join with both inputs empty using BroadcastNestedLoopJoin build right</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.051</duration>
          <className>org.apache.spark.sql.execution.joins.OuterJoinSuite</className>
          <testName>full outer join with both inputs empty using SortMergeJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.sql.execution.joins.OuterJoinSuite</className>
          <testName>full outer join with both inputs empty using BroadcastNestedLoopJoin build left</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.sql.execution.joins.OuterJoinSuite</className>
          <testName>full outer join with both inputs empty using BroadcastNestedLoopJoin build right</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.metric.SQLMetricsSuite.xml</file>
      <name>org.apache.spark.sql.execution.metric.SQLMetricsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.6190002</duration>
      <cases>
        <case>
          <duration>0.04</duration>
          <className>org.apache.spark.sql.execution.metric.SQLMetricsSuite</className>
          <testName>LocalTableScanExec computes metrics in collect and take</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.076</duration>
          <className>org.apache.spark.sql.execution.metric.SQLMetricsSuite</className>
          <testName>Filter metrics</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.04</duration>
          <className>org.apache.spark.sql.execution.metric.SQLMetricsSuite</className>
          <testName>WholeStageCodegen metrics</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.333</duration>
          <className>org.apache.spark.sql.execution.metric.SQLMetricsSuite</className>
          <testName>Aggregate metrics</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.103</duration>
          <className>org.apache.spark.sql.execution.metric.SQLMetricsSuite</className>
          <testName>Sort metrics</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.174</duration>
          <className>org.apache.spark.sql.execution.metric.SQLMetricsSuite</className>
          <testName>SortMergeJoin metrics</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.272</duration>
          <className>org.apache.spark.sql.execution.metric.SQLMetricsSuite</className>
          <testName>SortMergeJoin(outer) metrics</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.108</duration>
          <className>org.apache.spark.sql.execution.metric.SQLMetricsSuite</className>
          <testName>BroadcastHashJoin metrics</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.136</duration>
          <className>org.apache.spark.sql.execution.metric.SQLMetricsSuite</className>
          <testName>BroadcastHashJoin(outer) metrics</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.073</duration>
          <className>org.apache.spark.sql.execution.metric.SQLMetricsSuite</className>
          <testName>BroadcastNestedLoopJoin metrics</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.069</duration>
          <className>org.apache.spark.sql.execution.metric.SQLMetricsSuite</className>
          <testName>BroadcastLeftSemiJoinHash metrics</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.07</duration>
          <className>org.apache.spark.sql.execution.metric.SQLMetricsSuite</className>
          <testName>CartesianProduct metrics</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.12</duration>
          <className>org.apache.spark.sql.execution.metric.SQLMetricsSuite</className>
          <testName>save metrics</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.execution.metric.SQLMetricsSuite</className>
          <testName>metrics can be loaded by history server</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.python.RowQueueSuite.xml</file>
      <name>org.apache.spark.sql.execution.python.RowQueueSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.024999999</duration>
      <cases>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.execution.python.RowQueueSuite</className>
          <testName>in-memory queue</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.execution.python.RowQueueSuite</className>
          <testName>disk queue</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.sql.execution.python.RowQueueSuite</className>
          <testName>hybrid queue</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.streaming.CompactibleFileStreamLogSuite.xml</file>
      <name>org.apache.spark.sql.execution.streaming.CompactibleFileStreamLogSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.354</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.streaming.CompactibleFileStreamLogSuite</className>
          <testName>getBatchIdFromFileName</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.streaming.CompactibleFileStreamLogSuite</className>
          <testName>isCompactionBatch</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.streaming.CompactibleFileStreamLogSuite</className>
          <testName>nextCompactionBatchId</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.execution.streaming.CompactibleFileStreamLogSuite</className>
          <testName>getValidBatchesBeforeCompactionBatch</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.streaming.CompactibleFileStreamLogSuite</className>
          <testName>getAllValidBatches</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.execution.streaming.CompactibleFileStreamLogSuite</className>
          <testName>deriveCompactInterval</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.sql.execution.streaming.CompactibleFileStreamLogSuite</className>
          <testName>batchIdToPath</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.execution.streaming.CompactibleFileStreamLogSuite</className>
          <testName>serialize</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.streaming.CompactibleFileStreamLogSuite</className>
          <testName>deserialize</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.207</duration>
          <className>org.apache.spark.sql.execution.streaming.CompactibleFileStreamLogSuite</className>
          <testName>compact</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.111</duration>
          <className>org.apache.spark.sql.execution.streaming.CompactibleFileStreamLogSuite</className>
          <testName>delete expired file</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.streaming.FileStreamSinkLogSuite.xml</file>
      <name>org.apache.spark.sql.execution.streaming.FileStreamSinkLogSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.342</duration>
      <cases>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.sql.execution.streaming.FileStreamSinkLogSuite</className>
          <testName>compactLogs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.streaming.FileStreamSinkLogSuite</className>
          <testName>serialize</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.streaming.FileStreamSinkLogSuite</className>
          <testName>deserialize</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.222</duration>
          <className>org.apache.spark.sql.execution.streaming.FileStreamSinkLogSuite</className>
          <testName>compact</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.098</duration>
          <className>org.apache.spark.sql.execution.streaming.FileStreamSinkLogSuite</className>
          <testName>delete expired file</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.streaming.FileStreamSourceSuite.xml</file>
      <name>org.apache.spark.sql.execution.streaming.FileStreamSourceSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.10699999</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.streaming.FileStreamSourceSuite</className>
          <testName>SeenFilesMap</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.streaming.FileStreamSourceSuite</className>
          <testName>SeenFilesMap should only consider a file old if it is earlier than last purge time</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.105</duration>
          <className>org.apache.spark.sql.execution.streaming.FileStreamSourceSuite</className>
          <testName>do not recheck that files exist during getBatch</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.streaming.ForeachSinkSuite.xml</file>
      <name>org.apache.spark.sql.execution.streaming.ForeachSinkSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>10.828</duration>
      <cases>
        <case>
          <duration>0.185</duration>
          <className>org.apache.spark.sql.execution.streaming.ForeachSinkSuite</className>
          <testName>foreach() with `append` output mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.257</duration>
          <className>org.apache.spark.sql.execution.streaming.ForeachSinkSuite</className>
          <testName>foreach() with `complete` output mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>10.016</duration>
          <className>org.apache.spark.sql.execution.streaming.ForeachSinkSuite</className>
          <testName>foreach with error</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.37</duration>
          <className>org.apache.spark.sql.execution.streaming.ForeachSinkSuite</className>
          <testName>foreach with watermark</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.streaming.HDFSMetadataLogSuite.xml</file>
      <name>org.apache.spark.sql.execution.streaming.HDFSMetadataLogSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.821</duration>
      <cases>
        <case>
          <duration>0.055</duration>
          <className>org.apache.spark.sql.execution.streaming.HDFSMetadataLogSuite</className>
          <testName>FileManager: FileContextManager</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.052</duration>
          <className>org.apache.spark.sql.execution.streaming.HDFSMetadataLogSuite</className>
          <testName>FileManager: FileSystemManager</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.055</duration>
          <className>org.apache.spark.sql.execution.streaming.HDFSMetadataLogSuite</className>
          <testName>HDFSMetadataLog: basic</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.execution.streaming.HDFSMetadataLogSuite</className>
          <testName>HDFSMetadataLog: fallback from FileContext to FileSystem</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.05</duration>
          <className>org.apache.spark.sql.execution.streaming.HDFSMetadataLogSuite</className>
          <testName>HDFSMetadataLog: purge</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.035</duration>
          <className>org.apache.spark.sql.execution.streaming.HDFSMetadataLogSuite</className>
          <testName>HDFSMetadataLog: restart</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.566</duration>
          <className>org.apache.spark.sql.execution.streaming.HDFSMetadataLogSuite</className>
          <testName>HDFSMetadataLog: metadata directory collision</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.streaming.OffsetSeqLogSuite.xml</file>
      <name>org.apache.spark.sql.execution.streaming.OffsetSeqLogSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.063</duration>
      <cases>
        <case>
          <duration>0.063</duration>
          <className>org.apache.spark.sql.execution.streaming.OffsetSeqLogSuite</className>
          <testName>serialization - deserialization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.streaming.ProcessingTimeExecutorSuite.xml</file>
      <name>org.apache.spark.sql.execution.streaming.ProcessingTimeExecutorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.02</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.streaming.ProcessingTimeExecutorSuite</className>
          <testName>nextBatchTime</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.streaming.ProcessingTimeExecutorSuite</className>
          <testName>calling nextBatchTime with the result of a previous call should return the next interval</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.sql.execution.streaming.ProcessingTimeExecutorSuite</className>
          <testName>batch termination</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.execution.streaming.ProcessingTimeExecutorSuite</className>
          <testName>notifyBatchFallingBehind</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.streaming.StreamMetricsSuite.xml</file>
      <name>org.apache.spark.sql.execution.streaming.StreamMetricsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.004</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.streaming.StreamMetricsSuite</className>
          <testName>rates, latencies, trigger details - basic life cycle</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.streaming.StreamMetricsSuite</className>
          <testName>rates and latencies - after trigger with no data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.execution.streaming.StreamMetricsSuite</className>
          <testName>rates - after trigger with multiple sources, and one source having no info</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.streaming.StreamMetricsSuite</className>
          <testName>registered Codahale metrics</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.streaming.TextSocketStreamSuite.xml</file>
      <name>org.apache.spark.sql.execution.streaming.TextSocketStreamSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.23400001</duration>
      <cases>
        <case>
          <duration>0.115</duration>
          <className>org.apache.spark.sql.execution.streaming.TextSocketStreamSuite</className>
          <testName>basic usage</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.084</duration>
          <className>org.apache.spark.sql.execution.streaming.TextSocketStreamSuite</className>
          <testName>timestamped usage</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.streaming.TextSocketStreamSuite</className>
          <testName>params not given</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.streaming.TextSocketStreamSuite</className>
          <testName>non-boolean includeTimestamp</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.execution.streaming.TextSocketStreamSuite</className>
          <testName>no server up</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.sql.execution.streaming.TextSocketStreamSuite</className>
          <testName>input row metrics</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorSuite.xml</file>
      <name>org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.015000001</duration>
      <cases>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorSuite</className>
          <testName>report, verify, getLocation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorSuite</className>
          <testName>make inactive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorSuite</className>
          <testName>multiple references have same underlying coordinator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.streaming.state.StateStoreRDDSuite.xml</file>
      <name>org.apache.spark.sql.execution.streaming.state.StateStoreRDDSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>7.7130003</duration>
      <cases>
        <case>
          <duration>0.372</duration>
          <className>org.apache.spark.sql.execution.streaming.state.StateStoreRDDSuite</className>
          <testName>versioning and immutability</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.739</duration>
          <className>org.apache.spark.sql.execution.streaming.state.StateStoreRDDSuite</className>
          <testName>recovering from files</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.282</duration>
          <className>org.apache.spark.sql.execution.streaming.state.StateStoreRDDSuite</className>
          <testName>usage with iterators - only gets and only puts</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.169</duration>
          <className>org.apache.spark.sql.execution.streaming.state.StateStoreRDDSuite</className>
          <testName>preferred locations using StateStoreCoordinator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>5.151</duration>
          <className>org.apache.spark.sql.execution.streaming.state.StateStoreRDDSuite</className>
          <testName>distributed test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.streaming.state.StateStoreSuite.xml</file>
      <name>org.apache.spark.sql.execution.streaming.state.StateStoreSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.2639999</duration>
      <cases>
        <case>
          <duration>0.138</duration>
          <className>org.apache.spark.sql.execution.streaming.state.StateStoreSuite</className>
          <testName>get, put, remove, commit, and all data iterator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.04</duration>
          <className>org.apache.spark.sql.execution.streaming.state.StateStoreSuite</className>
          <testName>updates iterator with all combos of updates and removes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.044</duration>
          <className>org.apache.spark.sql.execution.streaming.state.StateStoreSuite</className>
          <testName>cancel</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.062</duration>
          <className>org.apache.spark.sql.execution.streaming.state.StateStoreSuite</className>
          <testName>getStore with unexpected versions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.269</duration>
          <className>org.apache.spark.sql.execution.streaming.state.StateStoreSuite</className>
          <testName>snapshotting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.195</duration>
          <className>org.apache.spark.sql.execution.streaming.state.StateStoreSuite</className>
          <testName>cleaning</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.13</duration>
          <className>org.apache.spark.sql.execution.streaming.state.StateStoreSuite</className>
          <testName>corrupted file handling</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.sql.execution.streaming.state.StateStoreSuite</className>
          <testName>get</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.302</duration>
          <className>org.apache.spark.sql.execution.streaming.state.StateStoreSuite</className>
          <testName>maintenance</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.sql.execution.streaming.state.StateStoreSuite</className>
          <testName>SPARK-18342: commit fails when rename fails</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.sql.execution.streaming.state.StateStoreSuite</className>
          <testName>SPARK-18416: do not create temp delta file until the store is updated</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.ui.SQLListenerMemoryLeakSuite.xml</file>
      <name>org.apache.spark.sql.execution.ui.SQLListenerMemoryLeakSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>3.866</duration>
      <cases>
        <case>
          <duration>3.866</duration>
          <className>org.apache.spark.sql.execution.ui.SQLListenerMemoryLeakSuite</className>
          <testName>no memory leak</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.ui.SQLListenerSuite.xml</file>
      <name>org.apache.spark.sql.execution.ui.SQLListenerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.63100004</duration>
      <cases>
        <case>
          <duration>0.069</duration>
          <className>org.apache.spark.sql.execution.ui.SQLListenerSuite</className>
          <testName>basic</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.sql.execution.ui.SQLListenerSuite</className>
          <testName>onExecutionEnd happens before onJobEnd(JobSucceeded)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.sql.execution.ui.SQLListenerSuite</className>
          <testName>onExecutionEnd happens before multiple onJobEnd(JobSucceeded)s</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.sql.execution.ui.SQLListenerSuite</className>
          <testName>onExecutionEnd happens before onJobEnd(JobFailed)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.107</duration>
          <className>org.apache.spark.sql.execution.ui.SQLListenerSuite</className>
          <testName>SPARK-11126: no memory leak when running non SQL jobs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.223</duration>
          <className>org.apache.spark.sql.execution.ui.SQLListenerSuite</className>
          <testName>SPARK-13055: history listener only tracks SQL metrics</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.sql.execution.ui.SQLListenerSuite</className>
          <testName>driver side SQL metrics</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.15</duration>
          <className>org.apache.spark.sql.execution.ui.SQLListenerSuite</className>
          <testName>roundtripping SparkListenerDriverAccumUpdates through JsonProtocol (SPARK-18462)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.execution.vectorized.ColumnarBatchSuite.xml</file>
      <name>org.apache.spark.sql.execution.vectorized.ColumnarBatchSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>26.508003</duration>
      <cases>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.execution.vectorized.ColumnarBatchSuite</className>
          <testName>Null Apis</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.vectorized.ColumnarBatchSuite</className>
          <testName>Byte Apis</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.execution.vectorized.ColumnarBatchSuite</className>
          <testName>Int Apis</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.execution.vectorized.ColumnarBatchSuite</className>
          <testName>Long Apis</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.execution.vectorized.ColumnarBatchSuite</className>
          <testName>Double APIs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.execution.vectorized.ColumnarBatchSuite</className>
          <testName>String APIs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.execution.vectorized.ColumnarBatchSuite</className>
          <testName>Int Array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.vectorized.ColumnarBatchSuite</className>
          <testName>Struct Column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.execution.vectorized.ColumnarBatchSuite</className>
          <testName>ColumnarBatch basic</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.execution.vectorized.ColumnarBatchSuite</className>
          <testName>Convert rows</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>5.864</duration>
          <className>org.apache.spark.sql.execution.vectorized.ColumnarBatchSuite</className>
          <testName>Random flat schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>20.589</duration>
          <className>org.apache.spark.sql.execution.vectorized.ColumnarBatchSuite</className>
          <testName>Random nested schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.vectorized.ColumnarBatchSuite</className>
          <testName>null filtered columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.execution.vectorized.ColumnarBatchSuite</className>
          <testName>mutable ColumnarBatch rows</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.execution.vectorized.ColumnarBatchSuite</className>
          <testName>exceeding maximum capacity should throw an error</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.expressions.ReduceAggregatorSuite.xml</file>
      <name>org.apache.spark.sql.expressions.ReduceAggregatorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.01</duration>
      <cases>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.expressions.ReduceAggregatorSuite</className>
          <testName>zero value</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.expressions.ReduceAggregatorSuite</className>
          <testName>reduce, merge and finish</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.expressions.ReduceAggregatorSuite</className>
          <testName>requires at least one input row</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.internal.CatalogSuite.xml</file>
      <name>org.apache.spark.sql.internal.CatalogSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.16</duration>
      <cases>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.sql.internal.CatalogSuite</className>
          <testName>current database</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.072</duration>
          <className>org.apache.spark.sql.internal.CatalogSuite</className>
          <testName>list databases</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.112</duration>
          <className>org.apache.spark.sql.internal.CatalogSuite</className>
          <testName>list tables</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.179</duration>
          <className>org.apache.spark.sql.internal.CatalogSuite</className>
          <testName>list tables with database</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.104</duration>
          <className>org.apache.spark.sql.internal.CatalogSuite</className>
          <testName>list functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.186</duration>
          <className>org.apache.spark.sql.internal.CatalogSuite</className>
          <testName>list functions with database</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.053</duration>
          <className>org.apache.spark.sql.internal.CatalogSuite</className>
          <testName>list columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.sql.internal.CatalogSuite</className>
          <testName>list columns in temporary table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.049</duration>
          <className>org.apache.spark.sql.internal.CatalogSuite</className>
          <testName>list columns in database</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.internal.CatalogSuite</className>
          <testName>toString</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.internal.CatalogSuite</className>
          <testName>toString</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.internal.CatalogSuite</className>
          <testName>toString</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.internal.CatalogSuite</className>
          <testName>toString</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.104</duration>
          <className>org.apache.spark.sql.internal.CatalogSuite</className>
          <testName>show</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.internal.CatalogSuite</className>
          <testName>createExternalTable should fail if path is not given for file-based data source</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.internal.CatalogSuite</className>
          <testName>createExternalTable should fail if provider is hive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.202</duration>
          <className>org.apache.spark.sql.internal.CatalogSuite</className>
          <testName>dropTempView should not un-cache and drop metastore table if a same-name table exists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.internal.CatalogSuite</className>
          <testName>get database</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.sql.internal.CatalogSuite</className>
          <testName>get table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.sql.internal.CatalogSuite</className>
          <testName>get function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.internal.CatalogSuite</className>
          <testName>database exists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.sql.internal.CatalogSuite</className>
          <testName>table exists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.internal.CatalogSuite</className>
          <testName>function exists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.internal.SQLConfEntrySuite.xml</file>
      <name>org.apache.spark.sql.internal.SQLConfEntrySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.0060000005</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.internal.SQLConfEntrySuite</className>
          <testName>intConf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.internal.SQLConfEntrySuite</className>
          <testName>longConf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.internal.SQLConfEntrySuite</className>
          <testName>booleanConf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.internal.SQLConfEntrySuite</className>
          <testName>doubleConf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.internal.SQLConfEntrySuite</className>
          <testName>stringConf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.internal.SQLConfEntrySuite</className>
          <testName>enumConf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.internal.SQLConfEntrySuite</className>
          <testName>stringSeqConf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.internal.SQLConfEntrySuite</className>
          <testName>optionalConf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.internal.SQLConfEntrySuite</className>
          <testName>duplicate entry</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.internal.SQLConfSuite.xml</file>
      <name>org.apache.spark.sql.internal.SQLConfSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.64000005</duration>
      <cases>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.internal.SQLConfSuite</className>
          <testName>propagate from spark conf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.sql.internal.SQLConfSuite</className>
          <testName>programmatic ways of basic setting and getting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.sql.internal.SQLConfSuite</className>
          <testName>parse SQL set commands</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.203</duration>
          <className>org.apache.spark.sql.internal.SQLConfSuite</className>
          <testName>set command for display</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.internal.SQLConfSuite</className>
          <testName>deprecated property</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.091</duration>
          <className>org.apache.spark.sql.internal.SQLConfSuite</className>
          <testName>reset - public conf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.07</duration>
          <className>org.apache.spark.sql.internal.SQLConfSuite</className>
          <testName>reset - internal conf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.sql.internal.SQLConfSuite</className>
          <testName>reset - user-defined conf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.internal.SQLConfSuite</className>
          <testName>invalid conf value</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.internal.SQLConfSuite</className>
          <testName>Test SHUFFLE_TARGET_POSTSHUFFLE_INPUT_SIZE&apos;s method</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.sql.internal.SQLConfSuite</className>
          <testName>SparkSession can access configs set in SparkConf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.internal.SQLConfSuite</className>
          <testName>default value of WAREHOUSE_PATH</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.123</duration>
          <className>org.apache.spark.sql.internal.SQLConfSuite</className>
          <testName>MAX_CASES_BRANCHES</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.sql.internal.SQLConfSuite</className>
          <testName>static SQL conf comes from SparkConf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.internal.SQLConfSuite</className>
          <testName>cannot set/unset static SQL conf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.internal.VariableSubstitutionSuite.xml</file>
      <name>org.apache.spark.sql.internal.VariableSubstitutionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.008</duration>
      <cases>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.internal.VariableSubstitutionSuite</className>
          <testName>system property</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.internal.VariableSubstitutionSuite</className>
          <testName>environmental variables</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.internal.VariableSubstitutionSuite</className>
          <testName>Spark configuration variable</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.internal.VariableSubstitutionSuite</className>
          <testName>multiple substitutes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.internal.VariableSubstitutionSuite</className>
          <testName>test nested substitutes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.jdbc.JDBCSuite.xml</file>
      <name>org.apache.spark.sql.jdbc.JDBCSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.7780001</duration>
      <cases>
        <case>
          <duration>0.041</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>SELECT *</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.508</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>SELECT * WHERE (simple predicates)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>SELECT COUNT(1) WHERE (predicates)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>SELECT * WHERE (quoted strings)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>SELECT first field</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>SELECT first field when fetchsize is two</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>SELECT second field</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>SELECT second field when fetchsize is two</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>SELECT * partitioned</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.067</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>SELECT WHERE (simple predicates) partitioned</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>SELECT second field partitioned</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.018</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>Register JDBC query with renamed fields</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>Basic API</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>Basic API with illegal fetchsize</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.096</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>Basic API with FetchSize</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>Partitioning via JDBCPartitioningInfo API</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>Partitioning via list-of-where-clauses API</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.069</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>Partitioning on column that might have null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.049</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>Partitioning on column where numPartitions is zero</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>Partitioning on column where numPartitions are more than the number of total rows</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>Partitioning on column where lowerBound is equal to upperBound</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>Partitioning on column where lowerBound is larger than upperBound</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>SELECT * on partitioned table with a nullable partition column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.032</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>H2 integral types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>H2 null entries</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.035</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>H2 string types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.035</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>H2 time types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.049</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>test DATE types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>test DATE types in cache</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>test types for null value</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.087</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>H2 floating-point types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.033</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>SQL query as table name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>Pass extra properties via OPTIONS</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>Remap types via JdbcDialects</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>Default jdbc dialect registration</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>quote column names by jdbc dialect</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>compile filters</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>Dialect unregister</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>Aggregated dialects</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>DB2Dialect type mapping</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>PostgresDialect type mapping</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>DerbyDialect jdbc type mapping</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>OracleDialect jdbc type mapping</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>table exists query by jdbc dialect</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>where for Date and Timestamp</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>test credentials in the properties are not in plan output</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>test credentials in the connection url are not in the plan output</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>SPARK 12941: The data type mapping for StringType to Oracle</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>SPARK-16625: General data types to be mapped to Oracle</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.072</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>SPARK-15916: JDBC filter operator push down should respect operator precedence</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.sql.jdbc.JDBCSuite</className>
          <testName>SPARK-16387: Reserved SQL words are not escaped by JDBC writer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.jdbc.JDBCWriteSuite.xml</file>
      <name>org.apache.spark.sql.jdbc.JDBCWriteSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.6999997</duration>
      <cases>
        <case>
          <duration>0.165</duration>
          <className>org.apache.spark.sql.jdbc.JDBCWriteSuite</className>
          <testName>Basic CREATE</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.jdbc.JDBCWriteSuite</className>
          <testName>Basic CREATE with illegal batchsize</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.174</duration>
          <className>org.apache.spark.sql.jdbc.JDBCWriteSuite</className>
          <testName>Basic CREATE with batchsize</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.163</duration>
          <className>org.apache.spark.sql.jdbc.JDBCWriteSuite</className>
          <testName>CREATE with ignore</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.139</duration>
          <className>org.apache.spark.sql.jdbc.JDBCWriteSuite</className>
          <testName>CREATE with overwrite</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.119</duration>
          <className>org.apache.spark.sql.jdbc.JDBCWriteSuite</className>
          <testName>CREATE then INSERT to append</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.162</duration>
          <className>org.apache.spark.sql.jdbc.JDBCWriteSuite</className>
          <testName>Truncate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.jdbc.JDBCWriteSuite</className>
          <testName>createTableOptions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.044</duration>
          <className>org.apache.spark.sql.jdbc.JDBCWriteSuite</className>
          <testName>Incompatible INSERT to append</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.091</duration>
          <className>org.apache.spark.sql.jdbc.JDBCWriteSuite</className>
          <testName>INSERT to JDBC Datasource</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.116</duration>
          <className>org.apache.spark.sql.jdbc.JDBCWriteSuite</className>
          <testName>INSERT to JDBC Datasource with overwrite</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.097</duration>
          <className>org.apache.spark.sql.jdbc.JDBCWriteSuite</className>
          <testName>save works for format(&quot;jdbc&quot;) if url and dbtable are set</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.101</duration>
          <className>org.apache.spark.sql.jdbc.JDBCWriteSuite</className>
          <testName>Overwrite</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.jdbc.JDBCWriteSuite</className>
          <testName>save errors if url is not specified</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.jdbc.JDBCWriteSuite</className>
          <testName>save errors if dbtable is not specified</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.267</duration>
          <className>org.apache.spark.sql.jdbc.JDBCWriteSuite</className>
          <testName>save errors if wrong user/password combination</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.jdbc.JDBCWriteSuite</className>
          <testName>save errors if partitionColumn and numPartitions and bounds not set</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.032</duration>
          <className>org.apache.spark.sql.jdbc.JDBCWriteSuite</className>
          <testName>SPARK-18433: Improve DataSource option keys to be more case-insensitive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.sources.CreateTableAsSelectSuite.xml</file>
      <name>org.apache.spark.sql.sources.CreateTableAsSelectSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.291</duration>
      <cases>
        <case>
          <duration>0.223</duration>
          <className>org.apache.spark.sql.sources.CreateTableAsSelectSuite</className>
          <testName>CREATE TABLE USING AS SELECT</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.056</duration>
          <className>org.apache.spark.sql.sources.CreateTableAsSelectSuite</className>
          <testName>CREATE TABLE USING AS SELECT based on the file without write permission</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.476</duration>
          <className>org.apache.spark.sql.sources.CreateTableAsSelectSuite</className>
          <testName>create a table, drop it and create another one with the same name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.sources.CreateTableAsSelectSuite</className>
          <testName> AS query</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.sources.CreateTableAsSelectSuite</className>
          <testName> AS query</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.108</duration>
          <className>org.apache.spark.sql.sources.CreateTableAsSelectSuite</className>
          <testName>create table using as select - with partitioned by</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.123</duration>
          <className>org.apache.spark.sql.sources.CreateTableAsSelectSuite</className>
          <testName>create table using as select - with non-zero buckets</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.sources.CreateTableAsSelectSuite</className>
          <testName>create table using as select - with zero buckets</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.286</duration>
          <className>org.apache.spark.sql.sources.CreateTableAsSelectSuite</className>
          <testName>SPARK-17409: CTAS of decimal calculation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.sources.CreateTableAsSelectSuite</className>
          <testName>specifying the column list for CTAS</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.sources.DDLSourceLoadSuite.xml</file>
      <name>org.apache.spark.sql.sources.DDLSourceLoadSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.031000001</duration>
      <cases>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.sources.DDLSourceLoadSuite</className>
          <testName>data sources with the same name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.sources.DDLSourceLoadSuite</className>
          <testName>load data source from format alias</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.sources.DDLSourceLoadSuite</className>
          <testName>specify full classname with duplicate formats</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.sources.DDLSourceLoadSuite</className>
          <testName>should fail to load ORC without Hive Support</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.sources.DDLTestSuite.xml</file>
      <name>org.apache.spark.sql.sources.DDLTestSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.059</duration>
      <cases>
        <case>
          <duration>0.054</duration>
          <className>org.apache.spark.sql.sources.DDLTestSuite</className>
          <testName>describe ddlPeople</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.sources.DDLTestSuite</className>
          <testName>SPARK-7686 DescribeCommand should have correct physical plan output attributes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.sources.DataSourceAnalysisSuite.xml</file>
      <name>org.apache.spark.sql.sources.DataSourceAnalysisSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.019</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.sources.DataSourceAnalysisSuite</className>
          <testName>convertStaticPartitions only handle INSERT having at least static partitions (caseSensitive: true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.sources.DataSourceAnalysisSuite</className>
          <testName>Missing columns (caseSensitive: true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.sources.DataSourceAnalysisSuite</className>
          <testName>Missing partitioning columns (caseSensitive: true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.sources.DataSourceAnalysisSuite</className>
          <testName>Wrong partitioning columns (caseSensitive: true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.sources.DataSourceAnalysisSuite</className>
          <testName>Static partitions need to appear before dynamic partitions (caseSensitive: true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.sources.DataSourceAnalysisSuite</className>
          <testName>All static partitions (caseSensitive: true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.sources.DataSourceAnalysisSuite</className>
          <testName>Static partition and dynamic partition (caseSensitive: true)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.sources.DataSourceAnalysisSuite</className>
          <testName>convertStaticPartitions only handle INSERT having at least static partitions (caseSensitive: false)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.sources.DataSourceAnalysisSuite</className>
          <testName>Missing columns (caseSensitive: false)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.sources.DataSourceAnalysisSuite</className>
          <testName>Missing partitioning columns (caseSensitive: false)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.sources.DataSourceAnalysisSuite</className>
          <testName>Wrong partitioning columns (caseSensitive: false)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.sources.DataSourceAnalysisSuite</className>
          <testName>Static partitions need to appear before dynamic partitions (caseSensitive: false)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.sources.DataSourceAnalysisSuite</className>
          <testName>All static partitions (caseSensitive: false)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.sources.DataSourceAnalysisSuite</className>
          <testName>Static partition and dynamic partition (caseSensitive: false)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.sources.FilteredScanSuite.xml</file>
      <name>org.apache.spark.sql.sources.FilteredScanSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.1290002</duration>
      <cases>
        <case>
          <duration>0.075</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>SELECT * FROM oneToTenFiltered</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.051</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>SELECT a, b FROM oneToTenFiltered</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.04</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>SELECT b, a FROM oneToTenFiltered</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.067</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>SELECT a FROM oneToTenFiltered</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.046</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>SELECT b FROM oneToTenFiltered</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.05</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>SELECT a * 2 FROM oneToTenFiltered</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.057</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>SELECT A AS b FROM oneToTenFiltered</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.223</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>b</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.235</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>b</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.051</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>SELECT a, b FROM oneToTenFiltered WHERE a = 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.056</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>SELECT a, b FROM oneToTenFiltered WHERE a IN (1,3,5)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.046</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>SELECT a, b FROM oneToTenFiltered WHERE A = 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.068</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>SELECT a, b FROM oneToTenFiltered WHERE b = 2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>SELECT a, b FROM oneToTenFiltered WHERE a IS NULL</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.044</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>SELECT a, b FROM oneToTenFiltered WHERE a IS NOT NULL</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.044</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>SELECT a, b FROM oneToTenFiltered WHERE a &lt; 5 AND a &gt; 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.045</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>SELECT a, b FROM oneToTenFiltered WHERE a &lt; 3 OR a &gt; 8</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.051</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>SELECT a, b FROM oneToTenFiltered WHERE NOT (a &lt; 6)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.049</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>SELECT a, b, c FROM oneToTenFiltered WHERE c like &apos;c%&apos;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>SELECT a, b, c FROM oneToTenFiltered WHERE c like &apos;%D&apos;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>SELECT a, b, c FROM oneToTenFiltered WHERE c like &apos;%eE%&apos;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>PushDown Returns 1: SELECT * FROM oneToTenFiltered WHERE A = 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>PushDown Returns 1: SELECT a FROM oneToTenFiltered WHERE A = 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>PushDown Returns 1: SELECT b FROM oneToTenFiltered WHERE A = 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>PushDown Returns 1: SELECT a, b FROM oneToTenFiltered WHERE A = 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.054</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>PushDown Returns 1: SELECT * FROM oneToTenFiltered WHERE a = 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>PushDown Returns 1: SELECT * FROM oneToTenFiltered WHERE 1 = a</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>PushDown Returns 9: SELECT * FROM oneToTenFiltered WHERE a &gt; 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>PushDown Returns 9: SELECT * FROM oneToTenFiltered WHERE a &gt;= 2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>PushDown Returns 9: SELECT * FROM oneToTenFiltered WHERE 1 &lt; a</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>PushDown Returns 9: SELECT * FROM oneToTenFiltered WHERE 2 &lt;= a</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>PushDown Returns 0: SELECT * FROM oneToTenFiltered WHERE 1 &gt; a</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>PushDown Returns 2: SELECT * FROM oneToTenFiltered WHERE 2 &gt;= a</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>PushDown Returns 0: SELECT * FROM oneToTenFiltered WHERE a &lt; 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>PushDown Returns 2: SELECT * FROM oneToTenFiltered WHERE a &lt;= 2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>PushDown Returns 8: SELECT * FROM oneToTenFiltered WHERE a &gt; 1 AND a &lt; 10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>PushDown Returns 3: SELECT * FROM oneToTenFiltered WHERE a IN (1,3,5)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>PushDown Returns 0: SELECT * FROM oneToTenFiltered WHERE a = 20</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>PushDown Returns 10: SELECT * FROM oneToTenFiltered WHERE b = 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>PushDown Returns 3: SELECT * FROM oneToTenFiltered WHERE a &lt; 5 AND a &gt; 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>PushDown Returns 4: SELECT * FROM oneToTenFiltered WHERE a &lt; 3 OR a &gt; 8</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>PushDown Returns 5: SELECT * FROM oneToTenFiltered WHERE NOT (a &lt; 6)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>PushDown Returns 1: SELECT a, b, c FROM oneToTenFiltered WHERE c like &apos;c%&apos;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>PushDown Returns 0: SELECT a, b, c FROM oneToTenFiltered WHERE c like &apos;C%&apos;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>PushDown Returns 1: SELECT a, b, c FROM oneToTenFiltered WHERE c like &apos;%D&apos;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>PushDown Returns 0: SELECT a, b, c FROM oneToTenFiltered WHERE c like &apos;%d&apos;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>PushDown Returns 1: SELECT a, b, c FROM oneToTenFiltered WHERE c like &apos;%eE%&apos;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>PushDown Returns 0: SELECT a, b, c FROM oneToTenFiltered WHERE c like &apos;%Ee%&apos;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.04</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>PushDown Returns 1: SELECT c FROM oneToTenFiltered WHERE c = &apos;aaaaaAAAAA&apos;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>PushDown Returns 1: SELECT c FROM oneToTenFiltered WHERE c IN (&apos;aaaaaAAAAA&apos;, &apos;foo&apos;)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>PushDown Returns 10: SELECT c FROM oneToTenFiltered WHERE A + b &gt; 9</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.sql.sources.FilteredScanSuite</className>
          <testName>PushDown Returns 3: SELECT a FROM oneToTenFiltered WHERE a + b &gt; 9 AND b &lt; 16 AND c IN (&apos;bbbbbBBBBB&apos;, &apos;cccccCCCCC&apos;, &apos;dddddDDDDD&apos;, &apos;foo&apos;)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.sources.FiltersSuite.xml</file>
      <name>org.apache.spark.sql.sources.FiltersSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.013000001</duration>
      <cases>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.sources.FiltersSuite</className>
          <testName>EqualTo references</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.sources.FiltersSuite</className>
          <testName>EqualNullSafe references</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.sources.FiltersSuite</className>
          <testName>GreaterThan references</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.sources.FiltersSuite</className>
          <testName>GreaterThanOrEqual references</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.sources.FiltersSuite</className>
          <testName>LessThan references</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.sources.FiltersSuite</className>
          <testName>LessThanOrEqual references</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.sources.FiltersSuite</className>
          <testName>In references</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.sources.FiltersSuite</className>
          <testName>IsNull references</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.sources.FiltersSuite</className>
          <testName>IsNotNull references</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.sources.FiltersSuite</className>
          <testName>And references</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.sources.FiltersSuite</className>
          <testName>Or references</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.sources.FiltersSuite</className>
          <testName>StringStartsWith references</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.sources.FiltersSuite</className>
          <testName>StringEndsWith references</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.sources.FiltersSuite</className>
          <testName>StringContains references</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.sources.InsertSuite.xml</file>
      <name>org.apache.spark.sql.sources.InsertSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.921</duration>
      <cases>
        <case>
          <duration>0.169</duration>
          <className>org.apache.spark.sql.sources.InsertSuite</className>
          <testName>Simple INSERT OVERWRITE a JSONRelation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.sql.sources.InsertSuite</className>
          <testName>insert into a temp view that does not point to an insertable data source</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.267</duration>
          <className>org.apache.spark.sql.sources.InsertSuite</className>
          <testName>PreInsert casting and renaming</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.sources.InsertSuite</className>
          <testName>SELECT clause generating a different number of columns is not allowed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.714</duration>
          <className>org.apache.spark.sql.sources.InsertSuite</className>
          <testName>INSERT OVERWRITE a JSONRelation multiple times</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.273</duration>
          <className>org.apache.spark.sql.sources.InsertSuite</className>
          <testName>INSERT INTO JSONRelation for now</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.166</duration>
          <className>org.apache.spark.sql.sources.InsertSuite</className>
          <testName>INSERT INTO TABLE with Comment in columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.316</duration>
          <className>org.apache.spark.sql.sources.InsertSuite</className>
          <testName>INSERT INTO TABLE - complex type but different names</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.sources.InsertSuite</className>
          <testName>it is not allowed to write to a table while querying it</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.497</duration>
          <className>org.apache.spark.sql.sources.InsertSuite</className>
          <testName>Caching</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.06</duration>
          <className>org.apache.spark.sql.sources.InsertSuite</className>
          <testName>it&apos;s not allowed to insert into a relation that is not an InsertableRelation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.434</duration>
          <className>org.apache.spark.sql.sources.InsertSuite</className>
          <testName>SPARK-15824 - Execute an INSERT wrapped in a WITH statement immediately</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.sources.PartitionedWriteSuite.xml</file>
      <name>org.apache.spark.sql.sources.PartitionedWriteSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.801</duration>
      <cases>
        <case>
          <duration>1.059</duration>
          <className>org.apache.spark.sql.sources.PartitionedWriteSuite</className>
          <testName>write many partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.572</duration>
          <className>org.apache.spark.sql.sources.PartitionedWriteSuite</className>
          <testName>write many partitions with repeats</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.17</duration>
          <className>org.apache.spark.sql.sources.PartitionedWriteSuite</className>
          <testName>partitioned columns should appear at the end of schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.sources.PathOptionSuite.xml</file>
      <name>org.apache.spark.sql.sources.PathOptionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.07000001</duration>
      <cases>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.sql.sources.PathOptionSuite</className>
          <testName>path option always exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.sql.sources.PathOptionSuite</className>
          <testName>path option also exist for write path</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.sql.sources.PathOptionSuite</className>
          <testName>path option always represent the value of table location</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.sources.PrunedScanSuite.xml</file>
      <name>org.apache.spark.sql.sources.PrunedScanSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.065</duration>
      <cases>
        <case>
          <duration>0.08</duration>
          <className>org.apache.spark.sql.sources.PrunedScanSuite</className>
          <testName>SELECT * FROM oneToTenPruned</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.044</duration>
          <className>org.apache.spark.sql.sources.PrunedScanSuite</className>
          <testName>SELECT a, b FROM oneToTenPruned</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.065</duration>
          <className>org.apache.spark.sql.sources.PrunedScanSuite</className>
          <testName>SELECT b, a FROM oneToTenPruned</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.045</duration>
          <className>org.apache.spark.sql.sources.PrunedScanSuite</className>
          <testName>SELECT a FROM oneToTenPruned</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.059</duration>
          <className>org.apache.spark.sql.sources.PrunedScanSuite</className>
          <testName>SELECT a, a FROM oneToTenPruned</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.039</duration>
          <className>org.apache.spark.sql.sources.PrunedScanSuite</className>
          <testName>SELECT b FROM oneToTenPruned</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.sql.sources.PrunedScanSuite</className>
          <testName>SELECT a * 2 FROM oneToTenPruned</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.062</duration>
          <className>org.apache.spark.sql.sources.PrunedScanSuite</className>
          <testName>SELECT A AS b FROM oneToTenPruned</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.256</duration>
          <className>org.apache.spark.sql.sources.PrunedScanSuite</className>
          <testName>b</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.23</duration>
          <className>org.apache.spark.sql.sources.PrunedScanSuite</className>
          <testName>b</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.sql.sources.PrunedScanSuite</className>
          <testName>Columns output a,b: SELECT * FROM oneToTenPruned</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.sql.sources.PrunedScanSuite</className>
          <testName>Columns output a,b: SELECT a, b FROM oneToTenPruned</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.sql.sources.PrunedScanSuite</className>
          <testName>Columns output b,a: SELECT b, a FROM oneToTenPruned</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.038</duration>
          <className>org.apache.spark.sql.sources.PrunedScanSuite</className>
          <testName>Columns output b: SELECT b, b FROM oneToTenPruned</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.sql.sources.PrunedScanSuite</className>
          <testName>Columns output a: SELECT a FROM oneToTenPruned</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.sql.sources.PrunedScanSuite</className>
          <testName>Columns output b: SELECT b FROM oneToTenPruned</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.sources.ResolvedDataSourceSuite.xml</file>
      <name>org.apache.spark.sql.sources.ResolvedDataSourceSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.014</duration>
      <cases>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.sources.ResolvedDataSourceSuite</className>
          <testName>jdbc</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.sources.ResolvedDataSourceSuite</className>
          <testName>json</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.sources.ResolvedDataSourceSuite</className>
          <testName>parquet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.sources.ResolvedDataSourceSuite</className>
          <testName>csv</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.sources.ResolvedDataSourceSuite</className>
          <testName>error message for unknown data sources</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.sources.SaveLoadSuite.xml</file>
      <name>org.apache.spark.sql.sources.SaveLoadSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>3.249</duration>
      <cases>
        <case>
          <duration>0.503</duration>
          <className>org.apache.spark.sql.sources.SaveLoadSuite</className>
          <testName>save with path and load</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.463</duration>
          <className>org.apache.spark.sql.sources.SaveLoadSuite</className>
          <testName>save with string mode and path, and load</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.451</duration>
          <className>org.apache.spark.sql.sources.SaveLoadSuite</className>
          <testName>save with path and datasource, and load</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.435</duration>
          <className>org.apache.spark.sql.sources.SaveLoadSuite</className>
          <testName>save with data source and options, and load</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.397</duration>
          <className>org.apache.spark.sql.sources.SaveLoadSuite</className>
          <testName>save and save again</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.sources.TableScanSuite.xml</file>
      <name>org.apache.spark.sql.sources.TableScanSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.1230001</duration>
      <cases>
        <case>
          <duration>0.09</duration>
          <className>org.apache.spark.sql.sources.TableScanSuite</className>
          <testName>SELECT * FROM oneToTen</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.sql.sources.TableScanSuite</className>
          <testName>SELECT i FROM oneToTen</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.sql.sources.TableScanSuite</className>
          <testName>SELECT i FROM oneToTen WHERE i &lt; 5</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.049</duration>
          <className>org.apache.spark.sql.sources.TableScanSuite</className>
          <testName>SELECT i * 2 FROM oneToTen</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.665</duration>
          <className>org.apache.spark.sql.sources.TableScanSuite</className>
          <testName>i + 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.217</duration>
          <className>org.apache.spark.sql.sources.TableScanSuite</className>
          <testName>Schema and all fields</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.092</duration>
          <className>org.apache.spark.sql.sources.TableScanSuite</className>
          <testName>SELECT count(*) FROM tableWithSchema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.056</duration>
          <className>org.apache.spark.sql.sources.TableScanSuite</className>
          <testName>SELECT `string$%Field` FROM tableWithSchema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.094</duration>
          <className>org.apache.spark.sql.sources.TableScanSuite</className>
          <testName>SELECT int_Field FROM tableWithSchema WHERE int_Field &lt; 5</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.067</duration>
          <className>org.apache.spark.sql.sources.TableScanSuite</className>
          <testName>SELECT `longField_:,&lt;&gt;=+/~^` * 2 FROM tableWithSchema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.083</duration>
          <className>org.apache.spark.sql.sources.TableScanSuite</className>
          <testName>key, arrayFieldSimple[1] FROM tableWithSchema a where int_Field=1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.085</duration>
          <className>org.apache.spark.sql.sources.TableScanSuite</className>
          <testName>`value_(2)` FROM tableWithSchema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.336</duration>
          <className>org.apache.spark.sql.sources.TableScanSuite</className>
          <testName>Caching</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.044</duration>
          <className>org.apache.spark.sql.sources.TableScanSuite</className>
          <testName>defaultSource</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.sql.sources.TableScanSuite</className>
          <testName>exceptions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.138</duration>
          <className>org.apache.spark.sql.sources.TableScanSuite</className>
          <testName>read the data source tables that do not extend SchemaRelationProvider</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.sql.sources.TableScanSuite</className>
          <testName>SPARK-5196 schema field with comment</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.streaming.FileStreamSinkSuite.xml</file>
      <name>org.apache.spark.sql.streaming.FileStreamSinkSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.5720003</duration>
      <cases>
        <case>
          <duration>0.283</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSinkSuite</className>
          <testName>unpartitioned writing and batch reading</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.387</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSinkSuite</className>
          <testName>partitioned writing and batch reading</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.802</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSinkSuite</className>
          <testName>writing with aggregation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.046</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSinkSuite</className>
          <testName>parquet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSinkSuite</className>
          <testName>text</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSinkSuite</className>
          <testName>json</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.streaming.FileStreamSourceStressTestSuite.xml</file>
      <name>org.apache.spark.sql.streaming.FileStreamSourceStressTestSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.488</duration>
      <cases>
        <case>
          <duration>1.488</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSourceStressTestSuite</className>
          <testName>file source stress test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.streaming.FileStreamSourceSuite.xml</file>
      <name>org.apache.spark.sql.streaming.FileStreamSourceSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>11.408999</duration>
      <cases>
        <case>
          <duration>0.018</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSourceSuite</className>
          <testName>FileStreamSource schema: no path</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSourceSuite</className>
          <testName>FileStreamSource schema: path doesn&apos;t exist (without schema) should throw exception</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSourceSuite</className>
          <testName>FileStreamSource schema: path doesn&apos;t exist (with schema) should throw exception</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSourceSuite</className>
          <testName>FileStreamSource schema: text, no existing files, no schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSourceSuite</className>
          <testName>FileStreamSource schema: text, existing files, no schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSourceSuite</className>
          <testName>FileStreamSource schema: text, existing files, schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.124</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSourceSuite</className>
          <testName>FileStreamSource schema: parquet, existing files, no schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.071</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSourceSuite</className>
          <testName>FileStreamSource schema: parquet, existing files, schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSourceSuite</className>
          <testName>FileStreamSource schema: json, no existing files, no schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.033</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSourceSuite</className>
          <testName>FileStreamSource schema: json, existing files, no schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSourceSuite</className>
          <testName>FileStreamSource schema: json, existing files, schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.278</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSourceSuite</className>
          <testName>read from text files</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.249</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSourceSuite</className>
          <testName>read from textfile</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.146</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSourceSuite</className>
          <testName>SPARK-17165 should not track the list of seen files indefinitely</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.238</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSourceSuite</className>
          <testName>read from json files</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.132</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSourceSuite</className>
          <testName>read from json files with inferring schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.11</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSourceSuite</className>
          <testName>reading from json files inside partitioned directory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.244</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSourceSuite</className>
          <testName>reading from json files with changing schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.454</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSourceSuite</className>
          <testName>read from parquet files</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.636</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSourceSuite</className>
          <testName>read from parquet files with changing schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.413</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSourceSuite</className>
          <testName>read new files in nested directories with globbing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.361</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSourceSuite</className>
          <testName>read new files in partitioned table with globbing, should not read partition data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.319</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSourceSuite</className>
          <testName>read new files in partitioned table without globbing, should read partition data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.398</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSourceSuite</className>
          <testName>when schema inference is turned on, should read partition data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.215</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSourceSuite</className>
          <testName>fault tolerance</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.632</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSourceSuite</className>
          <testName>max files per trigger</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSourceSuite</className>
          <testName>max files per trigger - incorrect values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.11</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSourceSuite</className>
          <testName>explain</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.354</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSourceSuite</className>
          <testName>SPARK-17372 - write file names to WAL as Array[String]</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.399</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSourceSuite</className>
          <testName>compact interval metadata log</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.354</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSourceSuite</className>
          <testName>get arbitrary batch from FileStreamSource</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.08</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSourceSuite</className>
          <testName>input row metrics</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.streaming.FileStreamSourceSuite</className>
          <testName>SPARK-18433: Improve DataSource option keys to be more case-insensitive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.streaming.FileStreamStressSuite.xml</file>
      <name>org.apache.spark.sql.streaming.FileStreamStressSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>49.284</duration>
      <cases>
        <case>
          <duration>21.376</duration>
          <className>org.apache.spark.sql.streaming.FileStreamStressSuite</className>
          <testName>fault tolerance stress test - unpartitioned output</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>27.908</duration>
          <className>org.apache.spark.sql.streaming.FileStreamStressSuite</className>
          <testName>fault tolerance stress test - partitioned output</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.streaming.LongOffsetSuite.xml</file>
      <name>org.apache.spark.sql.streaming.LongOffsetSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.001</duration>
      <cases>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.streaming.LongOffsetSuite</className>
          <testName>comparison 1 &lt;=&gt; 2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.streaming.LongOffsetSuite</className>
          <testName>comparison 1 &lt;=&gt; 3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.streaming.MemorySinkSuite.xml</file>
      <name>org.apache.spark.sql.streaming.MemorySinkSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.883</duration>
      <cases>
        <case>
          <duration>0.362</duration>
          <className>org.apache.spark.sql.streaming.MemorySinkSuite</className>
          <testName>directly add data in Append output mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.355</duration>
          <className>org.apache.spark.sql.streaming.MemorySinkSuite</className>
          <testName>directly add data in Update output mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.344</duration>
          <className>org.apache.spark.sql.streaming.MemorySinkSuite</className>
          <testName>directly add data in Complete output mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.107</duration>
          <className>org.apache.spark.sql.streaming.MemorySinkSuite</className>
          <testName>registering as a table in Append output mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.63</duration>
          <className>org.apache.spark.sql.streaming.MemorySinkSuite</className>
          <testName>registering as a table in Complete output mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.039</duration>
          <className>org.apache.spark.sql.streaming.MemorySinkSuite</className>
          <testName>MemoryPlan statistics</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.streaming.MemorySinkSuite</className>
          <testName>stress test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.streaming.MemorySinkSuite</className>
          <testName>error when no name is specified</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.sql.streaming.MemorySinkSuite</className>
          <testName>error if attempting to resume specific checkpoint</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.streaming.MemorySourceStressSuite.xml</file>
      <name>org.apache.spark.sql.streaming.MemorySourceStressSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.275</duration>
      <cases>
        <case>
          <duration>1.275</duration>
          <className>org.apache.spark.sql.streaming.MemorySourceStressSuite</className>
          <testName>memory stress test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.streaming.StreamSuite.xml</file>
      <name>org.apache.spark.sql.streaming.StreamSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>3.0149999</duration>
      <cases>
        <case>
          <duration>0.138</duration>
          <className>org.apache.spark.sql.streaming.StreamSuite</className>
          <testName>map with recovery</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.156</duration>
          <className>org.apache.spark.sql.streaming.StreamSuite</className>
          <testName>join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.156</duration>
          <className>org.apache.spark.sql.streaming.StreamSuite</className>
          <testName>union two streams</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.084</duration>
          <className>org.apache.spark.sql.streaming.StreamSuite</className>
          <testName>sql queries</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.105</duration>
          <className>org.apache.spark.sql.streaming.StreamSuite</className>
          <testName>DataFrame reuse</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.sql.streaming.StreamSuite</className>
          <testName>unsupported queries</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.139</duration>
          <className>org.apache.spark.sql.streaming.StreamSuite</className>
          <testName>minimize delay between batch construction and execution</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.094</duration>
          <className>org.apache.spark.sql.streaming.StreamSuite</className>
          <testName>insert an extraStrategy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.sql.streaming.StreamSuite</className>
          <testName>fatal errors from a source should be sent to the user</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.streaming.StreamSuite</className>
          <testName>output mode API in Scala</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.085</duration>
          <className>org.apache.spark.sql.streaming.StreamSuite</className>
          <testName>explain</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.streaming.StreamingAggregationSuite.xml</file>
      <name>org.apache.spark.sql.streaming.StreamingAggregationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>6.3459997</duration>
      <cases>
        <case>
          <duration>1.17</duration>
          <className>org.apache.spark.sql.streaming.StreamingAggregationSuite</className>
          <testName>simple count, update mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.24</duration>
          <className>org.apache.spark.sql.streaming.StreamingAggregationSuite</className>
          <testName>simple count, complete mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.sql.streaming.StreamingAggregationSuite</className>
          <testName>simple count, append mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.501</duration>
          <className>org.apache.spark.sql.streaming.StreamingAggregationSuite</className>
          <testName>sort after aggregate in complete mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.038</duration>
          <className>org.apache.spark.sql.streaming.StreamingAggregationSuite</className>
          <testName>state metrics</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.583</duration>
          <className>org.apache.spark.sql.streaming.StreamingAggregationSuite</className>
          <testName>multiple keys</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.371</duration>
          <className>org.apache.spark.sql.streaming.StreamingAggregationSuite</className>
          <testName>midbatch failure</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.423</duration>
          <className>org.apache.spark.sql.streaming.StreamingAggregationSuite</className>
          <testName>typed aggregators</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.streaming.StreamingQueryListenerSuite.xml</file>
      <name>org.apache.spark.sql.streaming.StreamingQueryListenerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.0819999</duration>
      <cases>
        <case>
          <duration>0.203</duration>
          <className>org.apache.spark.sql.streaming.StreamingQueryListenerSuite</className>
          <testName>single listener, check trigger statuses</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.04</duration>
          <className>org.apache.spark.sql.streaming.StreamingQueryListenerSuite</className>
          <testName>adding and removing listener</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.68</duration>
          <className>org.apache.spark.sql.streaming.StreamingQueryListenerSuite</className>
          <testName>event ordering</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.08</duration>
          <className>org.apache.spark.sql.streaming.StreamingQueryListenerSuite</className>
          <testName>exception should be reported in QueryTerminated</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.06</duration>
          <className>org.apache.spark.sql.streaming.StreamingQueryListenerSuite</className>
          <testName>QueryStarted serialization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.streaming.StreamingQueryListenerSuite</className>
          <testName>QueryProgress serialization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.streaming.StreamingQueryListenerSuite</className>
          <testName>QueryTerminated serialization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.streaming.StreamingQueryListenerSuite</className>
          <testName>0</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.streaming.StreamingQueryListenerSuite</className>
          <testName>1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.streaming.StreamingQueryManagerSuite.xml</file>
      <name>org.apache.spark.sql.streaming.StreamingQueryManagerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>18.434</duration>
      <cases>
        <case>
          <duration>0.293</duration>
          <className>org.apache.spark.sql.streaming.StreamingQueryManagerSuite</className>
          <testName>listing</testName>
          <skipped>false</skipped>
          <errorStackTrace>sbt.ForkMain$ForkError: org.scalatest.exceptions.TestFailedException: Streaming Query - query1 [state = TERMINATED] did not equal null
	at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:500)
	at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1555)
	at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:466)
	at org.apache.spark.sql.streaming.StreamingQueryManagerSuite$$anonfun$3$$anonfun$apply$mcV$sp$1.apply(StreamingQueryManagerSuite.scala:78)
	at org.apache.spark.sql.streaming.StreamingQueryManagerSuite$$anonfun$3$$anonfun$apply$mcV$sp$1.apply(StreamingQueryManagerSuite.scala:57)
	at org.apache.spark.sql.streaming.StreamingQueryManagerSuite$$anonfun$org$apache$spark$sql$streaming$StreamingQueryManagerSuite$$withQueriesOn$1.apply$mcV$sp(StreamingQueryManagerSuite.scala:244)
	at org.apache.spark.sql.streaming.StreamingQueryManagerSuite$$anonfun$org$apache$spark$sql$streaming$StreamingQueryManagerSuite$$withQueriesOn$1.apply(StreamingQueryManagerSuite.scala:219)
	at org.apache.spark.sql.streaming.StreamingQueryManagerSuite$$anonfun$org$apache$spark$sql$streaming$StreamingQueryManagerSuite$$withQueriesOn$1.apply(StreamingQueryManagerSuite.scala:219)
	at org.scalatest.concurrent.Timeouts$class.timeoutAfter(Timeouts.scala:326)
	at org.scalatest.concurrent.Timeouts$class.failAfter(Timeouts.scala:245)
	at org.apache.spark.sql.streaming.StreamingQueryManagerSuite.failAfter(StreamingQueryManagerSuite.scala:35)
	at org.apache.spark.sql.streaming.StreamingQueryManagerSuite.org$apache$spark$sql$streaming$StreamingQueryManagerSuite$$withQueriesOn(StreamingQueryManagerSuite.scala:219)
	at org.apache.spark.sql.streaming.StreamingQueryManagerSuite$$anonfun$3.apply$mcV$sp(StreamingQueryManagerSuite.scala:57)
	at org.apache.spark.sql.streaming.StreamingQueryManagerSuite$$anonfun$3.apply(StreamingQueryManagerSuite.scala:52)
	at org.apache.spark.sql.streaming.StreamingQueryManagerSuite$$anonfun$3.apply(StreamingQueryManagerSuite.scala:52)
	at org.apache.spark.sql.catalyst.util.package$.quietly(package.scala:42)
	at org.apache.spark.sql.test.SQLTestUtils$$anonfun$testQuietly$1.apply$mcV$sp(SQLTestUtils.scala:246)
	at org.apache.spark.sql.test.SQLTestUtils$$anonfun$testQuietly$1.apply(SQLTestUtils.scala:246)
	at org.apache.spark.sql.test.SQLTestUtils$$anonfun$testQuietly$1.apply(SQLTestUtils.scala:246)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:68)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.streaming.StreamingQueryManagerSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(StreamingQueryManagerSuite.scala:35)
	at org.scalatest.BeforeAndAfterEach$class.runTest(BeforeAndAfterEach.scala:255)
	at org.apache.spark.sql.streaming.StreamingQueryManagerSuite.org$scalatest$BeforeAndAfter$$super$runTest(StreamingQueryManagerSuite.scala:35)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.streaming.StreamingQueryManagerSuite.runTest(StreamingQueryManagerSuite.scala:35)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:31)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.streaming.StreamingQueryManagerSuite.org$scalatest$BeforeAndAfter$$super$run(StreamingQueryManagerSuite.scala:35)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.streaming.StreamingQueryManagerSuite.run(StreamingQueryManagerSuite.scala:35)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:357)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:502)
	at sbt.ForkMain$Run$2.call(ForkMain.java:296)
	at sbt.ForkMain$Run$2.call(ForkMain.java:286)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
</errorStackTrace>
          <errorDetails>org.scalatest.exceptions.TestFailedException: Streaming Query - query1 [state = TERMINATED] did not equal null</errorDetails>
          <failedSince>96</failedSince>
        </case>
        <case>
          <duration>12.547</duration>
          <className>org.apache.spark.sql.streaming.StreamingQueryManagerSuite</className>
          <testName>awaitAnyTermination without timeout and resetTerminated</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>5.594</duration>
          <className>org.apache.spark.sql.streaming.StreamingQueryManagerSuite</className>
          <testName>awaitAnyTermination with timeout and resetTerminated</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.streaming.StreamingQueryStatusSuite.xml</file>
      <name>org.apache.spark.sql.streaming.StreamingQueryStatusSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.007999999</duration>
      <cases>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.streaming.StreamingQueryStatusSuite</className>
          <testName>toString</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.streaming.StreamingQueryStatusSuite</className>
          <testName>json</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.streaming.StreamingQueryStatusSuite</className>
          <testName>prettyJson</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.streaming.StreamingQuerySuite.xml</file>
      <name>org.apache.spark.sql.streaming.StreamingQuerySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.7690003</duration>
      <cases>
        <case>
          <duration>0.055</duration>
          <className>org.apache.spark.sql.streaming.StreamingQuerySuite</className>
          <testName>names unique across active queries, ids unique across all started queries</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.183</duration>
          <className>org.apache.spark.sql.streaming.StreamingQuerySuite</className>
          <testName>lifecycle states and awaitTermination</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.193</duration>
          <className>org.apache.spark.sql.streaming.StreamingQuerySuite</className>
          <testName>query statuses</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.sql.streaming.StreamingQuerySuite</className>
          <testName>codahale metrics</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.095</duration>
          <className>org.apache.spark.sql.streaming.StreamingQuerySuite</className>
          <testName>input row calculation with mixed batch and streaming sources</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.057</duration>
          <className>org.apache.spark.sql.streaming.StreamingQuerySuite</className>
          <testName>input row calculation with trigger DF having multiple leaves</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.161</duration>
          <className>org.apache.spark.sql.streaming.StreamingQuerySuite</className>
          <testName>StreamExecution metadata garbage collection</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.streaming.WatermarkSuite.xml</file>
      <name>org.apache.spark.sql.streaming.WatermarkSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>3.817</duration>
      <cases>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.streaming.WatermarkSuite</className>
          <testName>error on bad column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.streaming.WatermarkSuite</className>
          <testName>error on wrong type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.329</duration>
          <className>org.apache.spark.sql.streaming.WatermarkSuite</className>
          <testName>watermark metric</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.668</duration>
          <className>org.apache.spark.sql.streaming.WatermarkSuite</className>
          <testName>append-mode watermark aggregation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.streaming.WatermarkSuite</className>
          <testName>recovery</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.032</duration>
          <className>org.apache.spark.sql.streaming.WatermarkSuite</className>
          <testName>dropping old data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.05</duration>
          <className>org.apache.spark.sql.streaming.WatermarkSuite</className>
          <testName>complete mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.714</duration>
          <className>org.apache.spark.sql.streaming.WatermarkSuite</className>
          <testName>group by on raw timestamp</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite.xml</file>
      <name>org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>3.3839998</duration>
      <cases>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite</className>
          <testName>write cannot be called on streaming datasets</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite</className>
          <testName>resolve default source</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite</className>
          <testName>resolve full class</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite</className>
          <testName>options</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite</className>
          <testName>partitioning</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite</className>
          <testName>stream paths</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite</className>
          <testName>test different data types for options</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.072</duration>
          <className>org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite</className>
          <testName>unique query names</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite</className>
          <testName>trigger</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite</className>
          <testName>source metadataPath</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite</className>
          <testName>check outputMode(string) throws exception on unsupported modes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite</className>
          <testName>check foreach() catches null writers</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite</className>
          <testName>check foreach() does not support partitioning</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.01</duration>
          <className>org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite</className>
          <testName>ConsoleSink can be correctly loaded</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite</className>
          <testName>prevent all column partitioning</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite</className>
          <testName>ConsoleSink should not require checkpointLocation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.777</duration>
          <className>org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite</className>
          <testName>MemorySink can recover from a checkpoint in Complete Mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite</className>
          <testName>append mode memory sink&apos;s do not support checkpoint recovery</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.34</duration>
          <className>org.apache.spark.sql.streaming.test.DataStreamReaderWriterSuite</className>
          <testName>SPARK-18510: use user specified types for partition columns in file sources</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.test.DataFrameReaderWriterSuite.xml</file>
      <name>org.apache.spark.sql.test.DataFrameReaderWriterSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>6.3110003</duration>
      <cases>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.sql.test.DataFrameReaderWriterSuite</className>
          <testName>writeStream cannot be called on non-streaming datasets</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.test.DataFrameReaderWriterSuite</className>
          <testName>resolve default source</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.test.DataFrameReaderWriterSuite</className>
          <testName>resolve default source without extending SchemaRelationProvider</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.test.DataFrameReaderWriterSuite</className>
          <testName>resolve full class</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.test.DataFrameReaderWriterSuite</className>
          <testName>options</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.test.DataFrameReaderWriterSuite</className>
          <testName>save mode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.test.DataFrameReaderWriterSuite</className>
          <testName>test path option in load</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.test.DataFrameReaderWriterSuite</className>
          <testName>test different data types for options</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.test.DataFrameReaderWriterSuite</className>
          <testName>check jdbc() does not support partitioning or bucketing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.033</duration>
          <className>org.apache.spark.sql.test.DataFrameReaderWriterSuite</className>
          <testName>prevent all column partitioning</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.test.DataFrameReaderWriterSuite</className>
          <testName>load API</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.075</duration>
          <className>org.apache.spark.sql.test.DataFrameReaderWriterSuite</className>
          <testName>read a data source that does not extend SchemaRelationProvider</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.046</duration>
          <className>org.apache.spark.sql.test.DataFrameReaderWriterSuite</className>
          <testName>read a data source that does not extend RelationProvider</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.705</duration>
          <className>org.apache.spark.sql.test.DataFrameReaderWriterSuite</className>
          <testName>text - API and behavior regarding schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.41</duration>
          <className>org.apache.spark.sql.test.DataFrameReaderWriterSuite</className>
          <testName>textFile - API and behavior regarding schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.111</duration>
          <className>org.apache.spark.sql.test.DataFrameReaderWriterSuite</className>
          <testName>csv - API and behavior regarding schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.904</duration>
          <className>org.apache.spark.sql.test.DataFrameReaderWriterSuite</className>
          <testName>json - API and behavior regarding schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.18</duration>
          <className>org.apache.spark.sql.test.DataFrameReaderWriterSuite</className>
          <testName>parquet - API and behavior regarding schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.test.DataFrameReaderWriterSuite</className>
          <testName>orc - API</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.412</duration>
          <className>org.apache.spark.sql.test.DataFrameReaderWriterSuite</className>
          <testName>column nullability and comment - write and then read</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.236</duration>
          <className>org.apache.spark.sql.test.DataFrameReaderWriterSuite</className>
          <testName>SPARK-17230: write out results of decimal calculation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.119</duration>
          <className>org.apache.spark.sql.test.DataFrameReaderWriterSuite</className>
          <testName>saveAsTable with mode Append should not fail if the table not exists but a same-name temp view exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.255</duration>
          <className>org.apache.spark.sql.test.DataFrameReaderWriterSuite</className>
          <testName>saveAsTable with mode Append should not fail if the table already exists and a same-name temp view exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.085</duration>
          <className>org.apache.spark.sql.test.DataFrameReaderWriterSuite</className>
          <testName>saveAsTable with mode ErrorIfExists should not fail if the table not exists but a same-name temp view exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.08</duration>
          <className>org.apache.spark.sql.test.DataFrameReaderWriterSuite</className>
          <testName>saveAsTable with mode Overwrite should not drop the temp view if the table not exists but a same-name temp view exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.242</duration>
          <className>org.apache.spark.sql.test.DataFrameReaderWriterSuite</className>
          <testName>saveAsTable with mode Overwrite should not fail if the table already exists and a same-name temp view exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.08</duration>
          <className>org.apache.spark.sql.test.DataFrameReaderWriterSuite</className>
          <testName>saveAsTable with mode Ignore should create the table if the table not exists but a same-name temp view exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.279</duration>
          <className>org.apache.spark.sql.test.DataFrameReaderWriterSuite</className>
          <testName>SPARK-18510: use user specified types for partition columns in file sources</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/org.apache.spark.sql.util.DataFrameCallbackSuite.xml</file>
      <name>org.apache.spark.sql.util.DataFrameCallbackSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.417</duration>
      <cases>
        <case>
          <duration>0.125</duration>
          <className>org.apache.spark.sql.util.DataFrameCallbackSuite</className>
          <testName>execute callback functions when a DataFrame action finished successfully</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.058</duration>
          <className>org.apache.spark.sql.util.DataFrameCallbackSuite</className>
          <testName>execute callback functions when a DataFrame action failed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.235</duration>
          <className>org.apache.spark.sql.util.DataFrameCallbackSuite</className>
          <testName>get numRows metrics by callback</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.util.DataFrameCallbackSuite</className>
          <testName>get size metrics by callback</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/test.org.apache.spark.sql.JavaApplySchemaSuite.xml</file>
      <name>test.org.apache.spark.sql.JavaApplySchemaSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.408</duration>
      <cases>
        <case>
          <duration>0.128</duration>
          <className>test.org.apache.spark.sql.JavaApplySchemaSuite</className>
          <testName>applySchema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.101</duration>
          <className>test.org.apache.spark.sql.JavaApplySchemaSuite</className>
          <testName>dataFrameRDDOperations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.179</duration>
          <className>test.org.apache.spark.sql.JavaApplySchemaSuite</className>
          <testName>applySchemaToJSON</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite.xml</file>
      <name>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.53900003</duration>
      <cases>
        <case>
          <duration>0.028</duration>
          <className>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite</className>
          <testName>testFormatAPI</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.08</duration>
          <className>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite</className>
          <testName>testTextAPI</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.08</duration>
          <className>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite</className>
          <testName>testJsonAPI</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.031</duration>
          <className>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite</className>
          <testName>testLoadAPI</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.033</duration>
          <className>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite</className>
          <testName>testOptionsAPI</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.03</duration>
          <className>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite</className>
          <testName>testSaveModeAPI</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.078</duration>
          <className>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite</className>
          <testName>testCsvAPI</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.116</duration>
          <className>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite</className>
          <testName>testParquetAPI</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.063</duration>
          <className>test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite</className>
          <testName>testTextFileAPI</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/test.org.apache.spark.sql.JavaDataFrameSuite.xml</file>
      <name>test.org.apache.spark.sql.JavaDataFrameSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>7.742</duration>
      <cases>
        <case>
          <duration>0.406</duration>
          <className>test.org.apache.spark.sql.JavaDataFrameSuite</className>
          <testName>testCollectAndTake</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.375</duration>
          <className>test.org.apache.spark.sql.JavaDataFrameSuite</className>
          <testName>testVarargMethods</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.353</duration>
          <className>test.org.apache.spark.sql.JavaDataFrameSuite</className>
          <testName>testCreateStructTypeFromList</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.537</duration>
          <className>test.org.apache.spark.sql.JavaDataFrameSuite</className>
          <testName>testSampleBy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.521</duration>
          <className>test.org.apache.spark.sql.JavaDataFrameSuite</className>
          <testName>testCrosstab</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.379</duration>
          <className>test.org.apache.spark.sql.JavaDataFrameSuite</className>
          <testName>testCreateDataFromFromList</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.388</duration>
          <className>test.org.apache.spark.sql.JavaDataFrameSuite</className>
          <testName>testFrequentItems</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.409</duration>
          <className>test.org.apache.spark.sql.JavaDataFrameSuite</className>
          <testName>testExecution</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.47</duration>
          <className>test.org.apache.spark.sql.JavaDataFrameSuite</className>
          <testName>testTextLoad</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.803</duration>
          <className>test.org.apache.spark.sql.JavaDataFrameSuite</className>
          <testName>pivot</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.492</duration>
          <className>test.org.apache.spark.sql.JavaDataFrameSuite</className>
          <testName>testGenericLoad</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.521</duration>
          <className>test.org.apache.spark.sql.JavaDataFrameSuite</className>
          <testName>testCountMinSketch</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.463</duration>
          <className>test.org.apache.spark.sql.JavaDataFrameSuite</className>
          <testName>testCreateDataFrameFromJavaBeans</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.421</duration>
          <className>test.org.apache.spark.sql.JavaDataFrameSuite</className>
          <testName>testCorrelation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.439</duration>
          <className>test.org.apache.spark.sql.JavaDataFrameSuite</className>
          <testName>testBloomFilter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.393</duration>
          <className>test.org.apache.spark.sql.JavaDataFrameSuite</className>
          <testName>testCovariance</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.372</duration>
          <className>test.org.apache.spark.sql.JavaDataFrameSuite</className>
          <testName>testCreateDataFrameFromLocalJavaBeans</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/test.org.apache.spark.sql.JavaDatasetAggregatorSuite.xml</file>
      <name>test.org.apache.spark.sql.JavaDatasetAggregatorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.6210003</duration>
      <cases>
        <case>
          <duration>0.551</duration>
          <className>test.org.apache.spark.sql.JavaDatasetAggregatorSuite</className>
          <testName>testTypedAggregationCount</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.558</duration>
          <className>test.org.apache.spark.sql.JavaDatasetAggregatorSuite</className>
          <testName>testTypedAggregationSumDouble</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.43</duration>
          <className>test.org.apache.spark.sql.JavaDatasetAggregatorSuite</className>
          <testName>testTypedAggregationSumLong</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.564</duration>
          <className>test.org.apache.spark.sql.JavaDatasetAggregatorSuite</className>
          <testName>testTypedAggregationAnonClass</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.518</duration>
          <className>test.org.apache.spark.sql.JavaDatasetAggregatorSuite</className>
          <testName>testTypedAggregationAverage</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/test.org.apache.spark.sql.JavaDatasetSuite.xml</file>
      <name>test.org.apache.spark.sql.JavaDatasetSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>9.489</duration>
      <cases>
        <case>
          <duration>0.423</duration>
          <className>test.org.apache.spark.sql.JavaDatasetSuite</className>
          <testName>testRuntimeNullabilityCheck</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.345</duration>
          <className>test.org.apache.spark.sql.JavaDatasetSuite</className>
          <testName>testRandomSplit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.391</duration>
          <className>test.org.apache.spark.sql.JavaDatasetSuite</className>
          <testName>testTypedFilterPreservingSchema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.43</duration>
          <className>test.org.apache.spark.sql.JavaDatasetSuite</className>
          <testName>testJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.331</duration>
          <className>test.org.apache.spark.sql.JavaDatasetSuite</className>
          <testName>testTake</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.386</duration>
          <className>test.org.apache.spark.sql.JavaDatasetSuite</className>
          <testName>testToLocalIterator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.372</duration>
          <className>test.org.apache.spark.sql.JavaDatasetSuite</className>
          <testName>testForeach</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.397</duration>
          <className>test.org.apache.spark.sql.JavaDatasetSuite</className>
          <testName>testJavaEncoder</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.366</duration>
          <className>test.org.apache.spark.sql.JavaDatasetSuite</className>
          <testName>testPrimitiveEncoder</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.475</duration>
          <className>test.org.apache.spark.sql.JavaDatasetSuite</className>
          <testName>testCommonOperation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.756</duration>
          <className>test.org.apache.spark.sql.JavaDatasetSuite</className>
          <testName>testGroupBy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.649</duration>
          <className>test.org.apache.spark.sql.JavaDatasetSuite</className>
          <testName>testSetOperation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.391</duration>
          <className>test.org.apache.spark.sql.JavaDatasetSuite</className>
          <testName>testKryoEncoder</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.414</duration>
          <className>test.org.apache.spark.sql.JavaDatasetSuite</className>
          <testName>testJavaBeanEncoder2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.398</duration>
          <className>test.org.apache.spark.sql.JavaDatasetSuite</className>
          <testName>testCollect</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.387</duration>
          <className>test.org.apache.spark.sql.JavaDatasetSuite</className>
          <testName>testKryoEncoderErrorMessageForPrivateClass</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.63</duration>
          <className>test.org.apache.spark.sql.JavaDatasetSuite</className>
          <testName>testJavaBeanEncoder</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.478</duration>
          <className>test.org.apache.spark.sql.JavaDatasetSuite</className>
          <testName>testTupleEncoder</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.468</duration>
          <className>test.org.apache.spark.sql.JavaDatasetSuite</className>
          <testName>testNestedTupleEncoder</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.344</duration>
          <className>test.org.apache.spark.sql.JavaDatasetSuite</className>
          <testName>testReduce</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.317</duration>
          <className>test.org.apache.spark.sql.JavaDatasetSuite</className>
          <testName>testSelect</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.341</duration>
          <className>test.org.apache.spark.sql.JavaDatasetSuite</className>
          <testName>testJavaEncoderErrorMessageForPrivateClass</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/test.org.apache.spark.sql.JavaRowSuite.xml</file>
      <name>test.org.apache.spark.sql.JavaRowSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.002</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>test.org.apache.spark.sql.JavaRowSuite</className>
          <testName>constructSimpleRow</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>test.org.apache.spark.sql.JavaRowSuite</className>
          <testName>constructComplexRow</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/test.org.apache.spark.sql.JavaSaveLoadSuite.xml</file>
      <name>test.org.apache.spark.sql.JavaSaveLoadSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.581</duration>
      <cases>
        <case>
          <duration>0.287</duration>
          <className>test.org.apache.spark.sql.JavaSaveLoadSuite</className>
          <testName>saveAndLoadWithSchema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.294</duration>
          <className>test.org.apache.spark.sql.JavaSaveLoadSuite</className>
          <testName>saveAndLoad</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/core/target/test-reports/test.org.apache.spark.sql.JavaUDFSuite.xml</file>
      <name>test.org.apache.spark.sql.JavaUDFSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.373</duration>
      <cases>
        <case>
          <duration>0.121</duration>
          <className>test.org.apache.spark.sql.JavaUDFSuite</className>
          <testName>udf1Test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.125</duration>
          <className>test.org.apache.spark.sql.JavaUDFSuite</className>
          <testName>udf2Test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.127</duration>
          <className>test.org.apache.spark.sql.JavaUDFSuite</className>
          <testName>udf3Test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive-thriftserver/target/test-reports/org.apache.spark.sql.hive.thriftserver.CliSuite.xml</file>
      <name>org.apache.spark.sql.hive.thriftserver.CliSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>253.61302</duration>
      <cases>
        <case>
          <duration>31.069</duration>
          <className>org.apache.spark.sql.hive.thriftserver.CliSuite</className>
          <testName>Simple commands</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.275</duration>
          <className>org.apache.spark.sql.hive.thriftserver.CliSuite</className>
          <testName>Single command with -e</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>42.559</duration>
          <className>org.apache.spark.sql.hive.thriftserver.CliSuite</className>
          <testName>Single command with --database</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>26.815</duration>
          <className>org.apache.spark.sql.hive.thriftserver.CliSuite</className>
          <testName>Commands using SerDe provided in --jars</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>21.997</duration>
          <className>org.apache.spark.sql.hive.thriftserver.CliSuite</className>
          <testName>SPARK-11188 Analysis error reporting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>19.933</duration>
          <className>org.apache.spark.sql.hive.thriftserver.CliSuite</className>
          <testName>SPARK-11624 Spark SQL CLI should set sessionState only once</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>20.81</duration>
          <className>org.apache.spark.sql.hive.thriftserver.CliSuite</className>
          <testName>list jars</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>21.235</duration>
          <className>org.apache.spark.sql.hive.thriftserver.CliSuite</className>
          <testName>list jar &lt;jarfile&gt;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>20.893</duration>
          <className>org.apache.spark.sql.hive.thriftserver.CliSuite</className>
          <testName>list files</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>20.723</duration>
          <className>org.apache.spark.sql.hive.thriftserver.CliSuite</className>
          <testName>list file &lt;filepath&gt;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>26.304</duration>
          <className>org.apache.spark.sql.hive.thriftserver.CliSuite</className>
          <testName>apply hiveconf from cli command</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive-thriftserver/target/test-reports/org.apache.spark.sql.hive.thriftserver.HiveThriftBinaryServerSuite.xml</file>
      <name>org.apache.spark.sql.hive.thriftserver.HiveThriftBinaryServerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>22.799</duration>
      <cases>
        <case>
          <duration>0.285</duration>
          <className>org.apache.spark.sql.hive.thriftserver.HiveThriftBinaryServerSuite</className>
          <testName>GetInfo Thrift API</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>9.354</duration>
          <className>org.apache.spark.sql.hive.thriftserver.HiveThriftBinaryServerSuite</className>
          <testName>SPARK-16563 ThriftCLIService FetchResults repeat fetching result</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.859</duration>
          <className>org.apache.spark.sql.hive.thriftserver.HiveThriftBinaryServerSuite</className>
          <testName>JDBC query execution</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.173</duration>
          <className>org.apache.spark.sql.hive.thriftserver.HiveThriftBinaryServerSuite</className>
          <testName>Checks Hive version</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.77</duration>
          <className>org.apache.spark.sql.hive.thriftserver.HiveThriftBinaryServerSuite</className>
          <testName>SPARK-3004 regression: result set containing NULL</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.036</duration>
          <className>org.apache.spark.sql.hive.thriftserver.HiveThriftBinaryServerSuite</className>
          <testName>SPARK-4292 regression: result set iterator issue</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.793</duration>
          <className>org.apache.spark.sql.hive.thriftserver.HiveThriftBinaryServerSuite</className>
          <testName>SPARK-4309 regression: Date type support</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.023</duration>
          <className>org.apache.spark.sql.hive.thriftserver.HiveThriftBinaryServerSuite</className>
          <testName>SPARK-4407 regression: Complex type support</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.692</duration>
          <className>org.apache.spark.sql.hive.thriftserver.HiveThriftBinaryServerSuite</className>
          <testName>SPARK-12143 regression: Binary type support</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.956</duration>
          <className>org.apache.spark.sql.hive.thriftserver.HiveThriftBinaryServerSuite</className>
          <testName>test multiple session</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.thriftserver.HiveThriftBinaryServerSuite</className>
          <testName>test jdbc cancel</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.608</duration>
          <className>org.apache.spark.sql.hive.thriftserver.HiveThriftBinaryServerSuite</className>
          <testName>test add jar</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.093</duration>
          <className>org.apache.spark.sql.hive.thriftserver.HiveThriftBinaryServerSuite</className>
          <testName>Checks Hive version via SET -v</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.063</duration>
          <className>org.apache.spark.sql.hive.thriftserver.HiveThriftBinaryServerSuite</className>
          <testName>Checks Hive version via SET</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.071</duration>
          <className>org.apache.spark.sql.hive.thriftserver.HiveThriftBinaryServerSuite</className>
          <testName>SPARK-11595 ADD JAR with input path having URL scheme</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.sql.hive.thriftserver.HiveThriftBinaryServerSuite</className>
          <testName>SPARK-11043 check operation log root directory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive-thriftserver/target/test-reports/org.apache.spark.sql.hive.thriftserver.HiveThriftHttpServerSuite.xml</file>
      <name>org.apache.spark.sql.hive.thriftserver.HiveThriftHttpServerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>9.029</duration>
      <cases>
        <case>
          <duration>8.797</duration>
          <className>org.apache.spark.sql.hive.thriftserver.HiveThriftHttpServerSuite</className>
          <testName>JDBC query execution</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.232</duration>
          <className>org.apache.spark.sql.hive.thriftserver.HiveThriftHttpServerSuite</className>
          <testName>Checks Hive version</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive-thriftserver/target/test-reports/org.apache.spark.sql.hive.thriftserver.JdbcConnectionUriSuite.xml</file>
      <name>org.apache.spark.sql.hive.thriftserver.JdbcConnectionUriSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.392</duration>
      <cases>
        <case>
          <duration>1.392</duration>
          <className>org.apache.spark.sql.hive.thriftserver.JdbcConnectionUriSuite</className>
          <testName>SPARK-17819 Support default database in connection URIs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive-thriftserver/target/test-reports/org.apache.spark.sql.hive.thriftserver.SingleSessionSuite.xml</file>
      <name>org.apache.spark.sql.hive.thriftserver.SingleSessionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>5.01</duration>
      <cases>
        <case>
          <duration>5.01</duration>
          <className>org.apache.spark.sql.hive.thriftserver.SingleSessionSuite</className>
          <testName>test single session</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive-thriftserver/target/test-reports/org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperationSuite.xml</file>
      <name>org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.024</duration>
      <cases>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperationSuite</className>
          <testName>SPARK-17112 `select null` via JDBC triggers IllegalArgumentException in ThriftServer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive-thriftserver/target/test-reports/org.apache.spark.sql.hive.thriftserver.UISeleniumSuite.xml</file>
      <name>org.apache.spark.sql.hive.thriftserver.UISeleniumSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>-0.001</duration>
      <cases>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.thriftserver.UISeleniumSuite</className>
          <testName>thrift server ui test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.catalyst.ExpressionSQLBuilderSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.ExpressionSQLBuilderSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.01</duration>
      <cases>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.ExpressionSQLBuilderSuite</className>
          <testName>literal</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.ExpressionSQLBuilderSuite</className>
          <testName>attributes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.catalyst.ExpressionSQLBuilderSuite</className>
          <testName>binary comparisons</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.ExpressionSQLBuilderSuite</className>
          <testName>logical operators</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.catalyst.ExpressionSQLBuilderSuite</className>
          <testName>arithmetic expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.catalyst.ExpressionSQLBuilderSuite</className>
          <testName>window specification</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.catalyst.ExpressionSQLBuilderSuite</className>
          <testName>interval arithmetic</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.catalyst.ExpressionToSQLSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.ExpressionToSQLSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>28.716002</duration>
      <cases>
        <case>
          <duration>1.603</duration>
          <className>org.apache.spark.sql.catalyst.ExpressionToSQLSuite</className>
          <testName>misc non-aggregate functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.27</duration>
          <className>org.apache.spark.sql.catalyst.ExpressionToSQLSuite</className>
          <testName>math functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>16.547</duration>
          <className>org.apache.spark.sql.catalyst.ExpressionToSQLSuite</className>
          <testName>aggregate functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.335</duration>
          <className>org.apache.spark.sql.catalyst.ExpressionToSQLSuite</className>
          <testName>string functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.52</duration>
          <className>org.apache.spark.sql.catalyst.ExpressionToSQLSuite</className>
          <testName>datetime functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.334</duration>
          <className>org.apache.spark.sql.catalyst.ExpressionToSQLSuite</className>
          <testName>collection functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.726</duration>
          <className>org.apache.spark.sql.catalyst.ExpressionToSQLSuite</className>
          <testName>misc functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.381</duration>
          <className>org.apache.spark.sql.catalyst.ExpressionToSQLSuite</className>
          <testName>subquery</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite.xml</file>
      <name>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>81.48001</duration>
      <cases>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>Test should fail if the SQL query cannot be parsed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>Test should fail if the golden file cannot be found</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>Test should fail if the SQL query cannot be regenerated</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>Test should fail if the SQL query did not equal to the golden SQL</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.248</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>range</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.144</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>in</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.177</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>not in</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.187</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>not like</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.683</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>aggregate function in having clause</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.218</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>aggregate function in order by clause</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.242</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>aggregate function in order by clause with multiple order keys</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.165</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>order by asc nulls last</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.148</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>order by desc nulls first</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.217</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>type widening in union</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.785</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>union distinct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.212</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>three-child union</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.773</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>intersect</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.475</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>except</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.322</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>self join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.77</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>self join with group by</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.16</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>case</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.165</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>case with else</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.199</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>case with key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.179</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>case with key and else</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.553</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>select distinct without aggregate functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.353</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>rollup/cube #1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.374</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>rollup/cube #2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.189</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>rollup/cube #3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.346</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>rollup/cube #4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.251</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>rollup/cube #5</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>6.815</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>rollup/cube #6</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.728</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>rollup/cube #7</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.208</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>rollup/cube #8</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.13</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>rollup/cube #9</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.653</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>grouping sets #1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>6.049</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>grouping sets #2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.481</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>cluster by</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.204</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>distribute by</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.429</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>distribute by with sort by</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.865</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>SPARK-13720: sort by after having</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.624</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>distinct aggregation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.805</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>TABLESAMPLE</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.47</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>multi-distinct columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.872</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>persisted data source relations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.583</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>script transformation - schemaless</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.343</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>script transformation - alias list</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.336</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>script transformation - alias list with type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.341</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>script transformation - row format delimited clause with only one format property</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.268</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>script transformation - row format delimited clause with multiple format properties</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.265</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>script transformation - row format serde clauses with SERDEPROPERTIES</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.303</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>script transformation - row format serde clauses without SERDEPROPERTIES</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>plans with non-SQL expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.699</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>named expression in column names shouldn&apos;t be quoted</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.105</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>window basic</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.718</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>multiple window functions in one expression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.488</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>regular expressions and window functions in one expression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.714</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>aggregate functions and window functions in one expression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.463</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>window with different window specification</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.995</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>window with the same window specification with aggregate + having</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.979</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>window with the same window specification with aggregate functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.162</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>window with the same window specification with aggregate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.748</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>window with the same window specification without aggregate and filter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.112</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>window clause</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.44</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>special window functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.78</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>window with join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.854</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>join 2 tables and aggregate function in having clause</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.182</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>generator in project list without FROM clause</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.39</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>generator in project list with non-referenced table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.372</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>generator in project list with referenced table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.415</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>generator in project list with non-UDTF expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.326</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>generator in lateral view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.345</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>generator in lateral view with ambiguous names</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.355</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>use JSON_TUPLE as generator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.481</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>nested generator in lateral view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.573</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>generate with other operators</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.158</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>filter after subquery</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.32</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>SPARK-14933 - select parquet table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.257</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>predicate subquery</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.556</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>broadcast join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.688</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>subquery using single table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.867</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>correlated subqueries using EXISTS on where clause</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.757</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>correlated subqueries using EXISTS on having clause</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.883</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>correlated subqueries using NOT EXISTS on where clause</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.159</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>correlated subqueries using NOT EXISTS on having clause</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.46</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>subquery using IN on where clause</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.407</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>subquery using IN on having clause</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.331</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>SPARK-14933 - select orc table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.148</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>inline tables</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.238</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>SPARK-17750 - interval arithmetic</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.197</duration>
          <className>org.apache.spark.sql.catalyst.LogicalPlanToSQLSuite</className>
          <testName>SPARK-17982 - limit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.CachedTableSuite.xml</file>
      <name>org.apache.spark.sql.hive.CachedTableSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>6.403</duration>
      <cases>
        <case>
          <duration>0.58</duration>
          <className>org.apache.spark.sql.hive.CachedTableSuite</className>
          <testName>cache table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.138</duration>
          <className>org.apache.spark.sql.hive.CachedTableSuite</className>
          <testName>cache invalidation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.137</duration>
          <className>org.apache.spark.sql.hive.CachedTableSuite</className>
          <testName>Drop cached table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.hive.CachedTableSuite</className>
          <testName>DROP nonexistant table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.hive.CachedTableSuite</className>
          <testName>uncache of nonexistant tables</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.105</duration>
          <className>org.apache.spark.sql.hive.CachedTableSuite</className>
          <testName>no error on uncache of non-cached table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.212</duration>
          <className>org.apache.spark.sql.hive.CachedTableSuite</className>
          <testName>&apos;CACHE TABLE&apos; and &apos;UNCACHE TABLE&apos; HiveQL statement</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.132</duration>
          <className>org.apache.spark.sql.hive.CachedTableSuite</className>
          <testName>CACHE TABLE tableName AS SELECT * FROM anotherTable</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.125</duration>
          <className>org.apache.spark.sql.hive.CachedTableSuite</className>
          <testName>CACHE TABLE tableName AS SELECT </testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.141</duration>
          <className>org.apache.spark.sql.hive.CachedTableSuite</className>
          <testName>CACHE LAZY TABLE tableName</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.131</duration>
          <className>org.apache.spark.sql.hive.CachedTableSuite</className>
          <testName>CACHE TABLE with Hive UDF</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.503</duration>
          <className>org.apache.spark.sql.hive.CachedTableSuite</className>
          <testName>REFRESH TABLE also needs to recache the data (data source tables)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.36</duration>
          <className>org.apache.spark.sql.hive.CachedTableSuite</className>
          <testName>SPARK-15678: REFRESH PATH</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.445</duration>
          <className>org.apache.spark.sql.hive.CachedTableSuite</className>
          <testName>Cache/Uncache Qualified Tables</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.sql.hive.CachedTableSuite</className>
          <testName>Cache Table As Select - having database name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.193</duration>
          <className>org.apache.spark.sql.hive.CachedTableSuite</className>
          <testName>SPARK-11246 cache parquet table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.16</duration>
          <className>org.apache.spark.sql.hive.CachedTableSuite</className>
          <testName>cache a table using CatalogFileIndex</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.ClasspathDependenciesSuite.xml</file>
      <name>org.apache.spark.sql.hive.ClasspathDependenciesSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.01</duration>
      <cases>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.ClasspathDependenciesSuite</className>
          <testName>shaded Protobuf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.hive.ClasspathDependenciesSuite</className>
          <testName>shaded Kryo</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.hive.ClasspathDependenciesSuite</className>
          <testName>hive-common</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.hive.ClasspathDependenciesSuite</className>
          <testName>hive-exec</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.hive.ClasspathDependenciesSuite</className>
          <testName>Forbidden Dependencies</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.hive.ClasspathDependenciesSuite</className>
          <testName>parquet-hadoop-bundle</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite.xml</file>
      <name>org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>13.97</duration>
      <cases>
        <case>
          <duration>5.038</duration>
          <className>org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite</className>
          <testName>Persist non-partitioned parquet relation into metastore as managed table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.155</duration>
          <className>org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite</className>
          <testName>Persist non-partitioned parquet relation into metastore as external table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.781</duration>
          <className>org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite</className>
          <testName>Persist non-partitioned parquet relation into metastore as managed table using CTAS</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.828</duration>
          <className>org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite</className>
          <testName>Persist non-partitioned orc relation into metastore as managed table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.626</duration>
          <className>org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite</className>
          <testName>Persist non-partitioned orc relation into metastore as external table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.542</duration>
          <className>org.apache.spark.sql.hive.DataSourceWithHiveMetastoreCatalogSuite</className>
          <testName>Persist non-partitioned orc relation into metastore as managed table using CTAS</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.ErrorPositionSuite.xml</file>
      <name>org.apache.spark.sql.hive.ErrorPositionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.13499998</duration>
      <cases>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.hive.ErrorPositionSuite</className>
          <testName>ambiguous attribute reference 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.ErrorPositionSuite</className>
          <testName>ambiguous attribute reference 2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.ErrorPositionSuite</className>
          <testName>ambiguous attribute reference 3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.088</duration>
          <className>org.apache.spark.sql.hive.ErrorPositionSuite</className>
          <testName>unresolved attribute 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.ErrorPositionSuite</className>
          <testName>unresolved attribute 2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.ErrorPositionSuite</className>
          <testName>unresolved attribute 3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.ErrorPositionSuite</className>
          <testName>unresolved attribute 4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.ErrorPositionSuite</className>
          <testName>unresolved attribute 5</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.hive.ErrorPositionSuite</className>
          <testName>unresolved attribute 6</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.hive.ErrorPositionSuite</className>
          <testName>unresolved attribute 7</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.hive.ErrorPositionSuite</className>
          <testName>multi-char unresolved attribute</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.hive.ErrorPositionSuite</className>
          <testName>unresolved attribute group by</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.hive.ErrorPositionSuite</className>
          <testName>unresolved attribute order by</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.hive.ErrorPositionSuite</className>
          <testName>unresolved attribute where</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.ErrorPositionSuite</className>
          <testName>unresolved attribute backticks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.hive.ErrorPositionSuite</className>
          <testName>parse error</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.hive.ErrorPositionSuite</className>
          <testName>bad relation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.ErrorPositionSuite</className>
          <testName>other expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.HiveContextCompatibilitySuite.xml</file>
      <name>org.apache.spark.sql.hive.HiveContextCompatibilitySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.82299995</duration>
      <cases>
        <case>
          <duration>0.219</duration>
          <className>org.apache.spark.sql.hive.HiveContextCompatibilitySuite</className>
          <testName>basic operations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.604</duration>
          <className>org.apache.spark.sql.hive.HiveContextCompatibilitySuite</className>
          <testName>basic DDLs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.HiveDDLCommandSuite.xml</file>
      <name>org.apache.spark.sql.hive.HiveDDLCommandSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.365</duration>
      <cases>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.hive.HiveDDLCommandSuite</className>
          <testName>Test CTAS #1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.hive.HiveDDLCommandSuite</className>
          <testName>Test CTAS #2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.hive.HiveDDLCommandSuite</className>
          <testName>Test CTAS #3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.hive.HiveDDLCommandSuite</className>
          <testName>Test CTAS #4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.hive.HiveDDLCommandSuite</className>
          <testName>Test CTAS #5</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.hive.HiveDDLCommandSuite</className>
          <testName>CTAS statement with a PARTITIONED BY clause is not allowed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.HiveDDLCommandSuite</className>
          <testName>CTAS statement with schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.sql.hive.HiveDDLCommandSuite</className>
          <testName>unsupported operations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.hive.HiveDDLCommandSuite</className>
          <testName>Invalid interval term should throw AnalysisException</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.sql.hive.HiveDDLCommandSuite</className>
          <testName>use native json_tuple instead of hive&apos;s UDTF in LATERAL VIEW</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.sql.hive.HiveDDLCommandSuite</className>
          <testName>transform query spec</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.hive.HiveDDLCommandSuite</className>
          <testName>use backticks in output of Script Transform</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.hive.HiveDDLCommandSuite</className>
          <testName>use backticks in output of Generator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.HiveDDLCommandSuite</className>
          <testName>use escaped backticks in output of Generator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.HiveDDLCommandSuite</className>
          <testName>create table - basic</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.hive.HiveDDLCommandSuite</className>
          <testName>create table - with database name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.HiveDDLCommandSuite</className>
          <testName>create table - temporary</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.hive.HiveDDLCommandSuite</className>
          <testName>create table - external</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.hive.HiveDDLCommandSuite</className>
          <testName>create table - if not exists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.hive.HiveDDLCommandSuite</className>
          <testName>create table - comment</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.hive.HiveDDLCommandSuite</className>
          <testName>create table - partitioned columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.HiveDDLCommandSuite</className>
          <testName>create table - clustered by</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.hive.HiveDDLCommandSuite</className>
          <testName>create table - skewed by</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.hive.HiveDDLCommandSuite</className>
          <testName>create table - row format</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.HiveDDLCommandSuite</className>
          <testName>create table - file format</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.HiveDDLCommandSuite</className>
          <testName>create table - storage handler</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.hive.HiveDDLCommandSuite</className>
          <testName>create table - properties</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.HiveDDLCommandSuite</className>
          <testName>create table - everything!</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.hive.HiveDDLCommandSuite</className>
          <testName>create view -- basic</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.hive.HiveDDLCommandSuite</className>
          <testName>create view - full</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.hive.HiveDDLCommandSuite</className>
          <testName>create view -- partitioned view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.HiveDDLCommandSuite</className>
          <testName>MSCK REPAIR table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.hive.HiveDDLCommandSuite</className>
          <testName>create table like</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.hive.HiveDDLCommandSuite</className>
          <testName>load data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.HiveDDLCommandSuite</className>
          <testName>Test the default fileformat for Hive-serde tables</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.196</duration>
          <className>org.apache.spark.sql.hive.HiveDDLCommandSuite</className>
          <testName>table name with schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.hive.HiveDDLCommandSuite</className>
          <testName>xml should be loaded</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.HiveDataFrameJoinSuite.xml</file>
      <name>org.apache.spark.sql.hive.HiveDataFrameJoinSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.282</duration>
      <cases>
        <case>
          <duration>0.282</duration>
          <className>org.apache.spark.sql.hive.HiveDataFrameJoinSuite</className>
          <testName>join - self join auto resolve ambiguity with case insensitivity</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.HiveExternalCatalogSuite.xml</file>
      <name>org.apache.spark.sql.hive.HiveExternalCatalogSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>15.06</duration>
      <cases>
        <case>
          <duration>0.088</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>basic create and list databases</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.034</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>get database when a database exists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.32</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>get database should throw exception when the database does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.321</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>list databases without pattern</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.264</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>list databases with pattern</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.271</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>drop database</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.325</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>drop database when the database is not empty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.242</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>drop database when the database does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.218</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>alter database</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.211</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>alter database should throw exception when the database does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.227</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>the table type of an external table should be EXTERNAL_TABLE</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.2</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>create table when the table already exists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.25</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>drop table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.195</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>drop table when database/table does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.267</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>rename table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.196</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>rename table when database/table does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.212</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>rename table when destination table already exists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.247</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>alter table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.218</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>alter table when database/table does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.2</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>get table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.173</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>get table when database/table does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.151</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>list tables without pattern</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.175</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>list tables with pattern</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.229</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>column names should be case-preserving and column nullability should be retained</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.161</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>basic create and list partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.155</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>create partitions when database/table does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.188</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>create partitions that already exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.281</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>create partitions without location</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.289</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>list partitions with partial partition spec</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.923</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>drop partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.164</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>drop partitions when database/table does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.158</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>drop partitions that do not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.226</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>get partition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.126</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>get partition when database/table does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.291</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>rename partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.533</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>rename partitions should update the location for managed table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.161</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>rename partitions when database/table does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.203</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>rename partitions when the new partition already exists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.461</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>alter partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.209</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>alter partitions when database/table does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>basic create and list functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.165</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>create function when database does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.128</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>create function that already exists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.128</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>drop function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.132</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>drop function when database does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.134</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>drop function that does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.155</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>get function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.141</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>get function when database does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.158</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>rename function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.132</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>rename function when database does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.162</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>rename function when new function already exists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.142</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>list functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.184</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>create/drop database should create/delete the directory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.302</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>create/drop/rename table should create/delete/rename the directory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.618</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>create/drop/rename partitions should create/delete/rename the directory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.292</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>drop partition from external table should not delete the directory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.31</duration>
          <className>org.apache.spark.sql.hive.HiveExternalCatalogSuite</className>
          <testName>list partitions by filter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.HiveInspectorSuite.xml</file>
      <name>org.apache.spark.sql.hive.HiveInspectorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.022000002</duration>
      <cases>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.hive.HiveInspectorSuite</className>
          <testName>Test wrap SettableStructObjectInspector</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.hive.HiveInspectorSuite</className>
          <testName>oi =&gt; datatype =&gt; oi</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.hive.HiveInspectorSuite</className>
          <testName>wrap / unwrap null, constant null and writables</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.HiveInspectorSuite</className>
          <testName>wrap / unwrap primitive writable object inspector</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.hive.HiveInspectorSuite</className>
          <testName>wrap / unwrap primitive java object inspector</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.hive.HiveInspectorSuite</className>
          <testName>wrap / unwrap Struct Type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.hive.HiveInspectorSuite</className>
          <testName>wrap / unwrap Array Type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.hive.HiveInspectorSuite</className>
          <testName>wrap / unwrap Map Type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.HiveMetadataCacheSuite.xml</file>
      <name>org.apache.spark.sql.hive.HiveMetadataCacheSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.218</duration>
      <cases>
        <case>
          <duration>0.445</duration>
          <className>org.apache.spark.sql.hive.HiveMetadataCacheSuite</className>
          <testName>SPARK-16337 temporary view refresh</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.957</duration>
          <className>org.apache.spark.sql.hive.HiveMetadataCacheSuite</className>
          <testName>partitioned table is cached when partition pruning is true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.816</duration>
          <className>org.apache.spark.sql.hive.HiveMetadataCacheSuite</className>
          <testName>partitioned table is cached when partition pruning is false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.HiveMetastoreCatalogSuite.xml</file>
      <name>org.apache.spark.sql.hive.HiveMetastoreCatalogSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.18699999</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.hive.HiveMetastoreCatalogSuite</className>
          <testName>struct field should accept underscore in sub-column name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.hive.HiveMetastoreCatalogSuite</className>
          <testName>udt to metastore type conversion</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.13</duration>
          <className>org.apache.spark.sql.hive.HiveMetastoreCatalogSuite</className>
          <testName>duplicated metastore relations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.hive.HiveMetastoreCatalogSuite</className>
          <testName>should not truncate struct type catalog string</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.051</duration>
          <className>org.apache.spark.sql.hive.HiveMetastoreCatalogSuite</className>
          <testName>view relation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.HiveParquetSuite.xml</file>
      <name>org.apache.spark.sql.hive.HiveParquetSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>24.006998</duration>
      <cases>
        <case>
          <duration>16.108</duration>
          <className>org.apache.spark.sql.hive.HiveParquetSuite</className>
          <testName>Case insensitive attribute names</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.394</duration>
          <className>org.apache.spark.sql.hive.HiveParquetSuite</className>
          <testName>SELECT on Parquet table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.442</duration>
          <className>org.apache.spark.sql.hive.HiveParquetSuite</className>
          <testName>Simple column projection + filter on Parquet table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.338</duration>
          <className>org.apache.spark.sql.hive.HiveParquetSuite</className>
          <testName>Converting Hive to Parquet Table via saveAsParquetFile</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.725</duration>
          <className>org.apache.spark.sql.hive.HiveParquetSuite</className>
          <testName>INSERT OVERWRITE TABLE Parquet table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.HiveUtilsSuite.xml</file>
      <name>org.apache.spark.sql.hive.HiveUtilsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.004</duration>
      <cases>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.hive.HiveUtilsSuite</className>
          <testName>newTemporaryConfiguration overwrites listener configurations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.HiveVariableSubstitutionSuite.xml</file>
      <name>org.apache.spark.sql.hive.HiveVariableSubstitutionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.164</duration>
      <cases>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.hive.HiveVariableSubstitutionSuite</className>
          <testName>SET hivevar with prefix</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.hive.HiveVariableSubstitutionSuite</className>
          <testName>SET hivevar with dotted name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.052</duration>
          <className>org.apache.spark.sql.hive.HiveVariableSubstitutionSuite</className>
          <testName>hivevar substitution</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.063</duration>
          <className>org.apache.spark.sql.hive.HiveVariableSubstitutionSuite</className>
          <testName>variable substitution without a prefix</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.sql.hive.HiveVariableSubstitutionSuite</className>
          <testName>variable substitution precedence</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.InsertIntoHiveTableSuite.xml</file>
      <name>org.apache.spark.sql.hive.InsertIntoHiveTableSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>11.985</duration>
      <cases>
        <case>
          <duration>0.765</duration>
          <className>org.apache.spark.sql.hive.InsertIntoHiveTableSuite</className>
          <testName>insertInto() HiveTable</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.sql.hive.InsertIntoHiveTableSuite</className>
          <testName>Double create fails when allowExisting = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.041</duration>
          <className>org.apache.spark.sql.hive.InsertIntoHiveTableSuite</className>
          <testName>Double create does not fail when allowExisting = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.364</duration>
          <className>org.apache.spark.sql.hive.InsertIntoHiveTableSuite</className>
          <testName>Map as value type of MapType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.67</duration>
          <className>org.apache.spark.sql.hive.InsertIntoHiveTableSuite</className>
          <testName>SPARK-4203:random partition directory order</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.832</duration>
          <className>org.apache.spark.sql.hive.InsertIntoHiveTableSuite</className>
          <testName>INSERT OVERWRITE - partition IF NOT EXISTS</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.355</duration>
          <className>org.apache.spark.sql.hive.InsertIntoHiveTableSuite</className>
          <testName>containsNull == false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.335</duration>
          <className>org.apache.spark.sql.hive.InsertIntoHiveTableSuite</className>
          <testName>valueContainsNull == false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.33</duration>
          <className>org.apache.spark.sql.hive.InsertIntoHiveTableSuite</className>
          <testName>nullable == false)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.135</duration>
          <className>org.apache.spark.sql.hive.InsertIntoHiveTableSuite</className>
          <testName>Reject partitioning that does not match table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.099</duration>
          <className>org.apache.spark.sql.hive.InsertIntoHiveTableSuite</className>
          <testName>Test partition mode = strict</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.771</duration>
          <className>org.apache.spark.sql.hive.InsertIntoHiveTableSuite</className>
          <testName>Detect table partitioning</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.17</duration>
          <className>org.apache.spark.sql.hive.InsertIntoHiveTableSuite</className>
          <testName>Hive SerDe table - partitionBy() can&apos;t be used together with insertInto()</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.108</duration>
          <className>org.apache.spark.sql.hive.InsertIntoHiveTableSuite</className>
          <testName>Data source table - partitionBy() can&apos;t be used together with insertInto()</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.107</duration>
          <className>org.apache.spark.sql.hive.InsertIntoHiveTableSuite</className>
          <testName>Hive SerDe table - SPARK-16036: better error message when insert into a table with mismatch schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.079</duration>
          <className>org.apache.spark.sql.hive.InsertIntoHiveTableSuite</className>
          <testName>Data source table - SPARK-16036: better error message when insert into a table with mismatch schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.783</duration>
          <className>org.apache.spark.sql.hive.InsertIntoHiveTableSuite</className>
          <testName>Hive SerDe table - SPARK-16037: INSERT statement should match columns by position</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.619</duration>
          <className>org.apache.spark.sql.hive.InsertIntoHiveTableSuite</className>
          <testName>Data source table - SPARK-16037: INSERT statement should match columns by position</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.678</duration>
          <className>org.apache.spark.sql.hive.InsertIntoHiveTableSuite</className>
          <testName>Hive SerDe table - INSERT INTO a partitioned table (semantic and error handling)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.498</duration>
          <className>org.apache.spark.sql.hive.InsertIntoHiveTableSuite</className>
          <testName>Data source table - INSERT INTO a partitioned table (semantic and error handling)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.456</duration>
          <className>org.apache.spark.sql.hive.InsertIntoHiveTableSuite</className>
          <testName>Hive SerDe table - insertInto() should match columns by position and ignore column names</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.431</duration>
          <className>org.apache.spark.sql.hive.InsertIntoHiveTableSuite</className>
          <testName>Data source table - insertInto() should match columns by position and ignore column names</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.442</duration>
          <className>org.apache.spark.sql.hive.InsertIntoHiveTableSuite</className>
          <testName>Hive SerDe table - insertInto() should match unnamed columns by position</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.422</duration>
          <className>org.apache.spark.sql.hive.InsertIntoHiveTableSuite</className>
          <testName>Data source table - insertInto() should match unnamed columns by position</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.105</duration>
          <className>org.apache.spark.sql.hive.InsertIntoHiveTableSuite</className>
          <testName>Hive SerDe table - insertInto() should reject missing columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.115</duration>
          <className>org.apache.spark.sql.hive.InsertIntoHiveTableSuite</className>
          <testName>Data source table - insertInto() should reject missing columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.127</duration>
          <className>org.apache.spark.sql.hive.InsertIntoHiveTableSuite</className>
          <testName>Hive SerDe table - insertInto() should reject extra columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.112</duration>
          <className>org.apache.spark.sql.hive.InsertIntoHiveTableSuite</className>
          <testName>Data source table - insertInto() should reject extra columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.JavaDataFrameSuite.xml</file>
      <name>org.apache.spark.sql.hive.JavaDataFrameSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.6570001</duration>
      <cases>
        <case>
          <duration>1.023</duration>
          <className>org.apache.spark.sql.hive.JavaDataFrameSuite</className>
          <testName>testUDAF</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.634</duration>
          <className>org.apache.spark.sql.hive.JavaDataFrameSuite</className>
          <testName>saveTableAndQueryIt</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite.xml</file>
      <name>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.569</duration>
      <cases>
        <case>
          <duration>0.625</duration>
          <className>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite</className>
          <testName>saveExternalTableAndQueryIt</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.281</duration>
          <className>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite</className>
          <testName>saveTableAndQueryIt</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.663</duration>
          <className>org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite</className>
          <testName>saveExternalTableWithSchemaAndQueryIt</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.ListTablesSuite.xml</file>
      <name>org.apache.spark.sql.hive.ListTablesSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.582</duration>
      <cases>
        <case>
          <duration>0.327</duration>
          <className>org.apache.spark.sql.hive.ListTablesSuite</className>
          <testName>get all tables of current database</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.255</duration>
          <className>org.apache.spark.sql.hive.ListTablesSuite</className>
          <testName>getting all tables with a database name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.MetastoreDataSourcesSuite.xml</file>
      <name>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>21.647</duration>
      <cases>
        <case>
          <duration>0.358</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>persistent JSON table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.403</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>persistent JSON table with a user specified schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.352</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>persistent JSON table with a user specified schema with a subset of fields</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.355</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>resolve shortened provider names</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.395</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>drop table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.648</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>check change without refresh</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.529</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>drop, change, recreate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.405</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>invalidate cache and reload</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.487</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>CTAS</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.521</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>CTAS with IF NOT EXISTS</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.557</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>CTAS a managed table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.459</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>saveAsTable(CTAS) using append and insertInto when the target table is Hive serde</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.54</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>SPARK-5839 HiveMetastoreCatalog does not recognize table aliases of data source tables</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.889</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>save table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.685</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>create external table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>path required error</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.303</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>scan a parquet table created through a CTAS statement</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.601</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>Pre insert nullability check (ArrayType)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.722</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>Pre insert nullability check (MapType)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.291</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>SPARK-6024 wide schema support</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.128</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.857</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>Saving partitionBy columns information</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.501</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>Saving information for sortBy and bucketBy columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.66</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>insert into a table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.502</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>append table using different formats</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.098</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>append a table using the same formats but different names</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.266</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>SPARK-8156:create table to specific database by &apos;use dbname&apos;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.06</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>skip hive metadata on table creation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.532</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>CTAS: persisted partitioned data source table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.591</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>CTAS: persisted bucketed data source table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.469</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>CTAS: persisted partitioned bucketed data source table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.427</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>saveAsTable[append]: the column order doesn&apos;t matter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.166</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>saveAsTable[append]: mismatch column names</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.18</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>saveAsTable[append]: too many columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>save API - format hive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>saveAsTable API - format hive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>create a data source table using hive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>create a temp view using hive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.438</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>saveAsTable - source and target are the same table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.763</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>insertInto - source and target are the same table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.174</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>saveAsTable[append]: less columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.328</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>SPARK-15025: create datasource table with path with select</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.304</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>SPARK-15269 external data source table creation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>read table with corrupted schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>should keep data source entries in table properties when debug mode is on</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.225</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>SPARK-17470: support old table that stores table location in storage properties</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.35</duration>
          <className>org.apache.spark.sql.hive.MetastoreDataSourcesSuite</className>
          <testName>SPARK-18464: support old table which doesn&apos;t store schema in table properties</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.MetastoreRelationSuite.xml</file>
      <name>org.apache.spark.sql.hive.MetastoreRelationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.601</duration>
      <cases>
        <case>
          <duration>0.057</duration>
          <className>org.apache.spark.sql.hive.MetastoreRelationSuite</className>
          <testName>makeCopy and toJSON should work</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.544</duration>
          <className>org.apache.spark.sql.hive.MetastoreRelationSuite</className>
          <testName>SPARK-17409: Do Not Optimize Query in CTAS (Hive Serde Table) More Than Once</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.MultiDatabaseSuite.xml</file>
      <name>org.apache.spark.sql.hive.MultiDatabaseSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>7.021</duration>
      <cases>
        <case>
          <duration>0.679</duration>
          <className>org.apache.spark.sql.hive.MultiDatabaseSuite</className>
          <testName>saveAsTable() to non-default database - with USE - Overwrite</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.374</duration>
          <className>org.apache.spark.sql.hive.MultiDatabaseSuite</className>
          <testName>saveAsTable() to non-default database - without USE - Overwrite</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.682</duration>
          <className>org.apache.spark.sql.hive.MultiDatabaseSuite</className>
          <testName>createExternalTable() to non-default database - with USE</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.684</duration>
          <className>org.apache.spark.sql.hive.MultiDatabaseSuite</className>
          <testName>createExternalTable() to non-default database - without USE</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.682</duration>
          <className>org.apache.spark.sql.hive.MultiDatabaseSuite</className>
          <testName>saveAsTable() to non-default database - with USE - Append</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.474</duration>
          <className>org.apache.spark.sql.hive.MultiDatabaseSuite</className>
          <testName>saveAsTable() to non-default database - without USE - Append</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.458</duration>
          <className>org.apache.spark.sql.hive.MultiDatabaseSuite</className>
          <testName>insertInto() non-default database - with USE</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.476</duration>
          <className>org.apache.spark.sql.hive.MultiDatabaseSuite</className>
          <testName>insertInto() non-default database - without USE</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.209</duration>
          <className>org.apache.spark.sql.hive.MultiDatabaseSuite</className>
          <testName>Looks up tables in non-default database</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.456</duration>
          <className>org.apache.spark.sql.hive.MultiDatabaseSuite</className>
          <testName>Drops a table in a non-default database</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.899</duration>
          <className>org.apache.spark.sql.hive.MultiDatabaseSuite</className>
          <testName>Refreshes a table in a non-default database - with USE</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.936</duration>
          <className>org.apache.spark.sql.hive.MultiDatabaseSuite</className>
          <testName>Refreshes a table in a non-default database - without USE</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.sql.hive.MultiDatabaseSuite</className>
          <testName>invalid database name and table names</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.ParquetHiveCompatibilitySuite.xml</file>
      <name>org.apache.spark.sql.hive.ParquetHiveCompatibilitySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.2889998</duration>
      <cases>
        <case>
          <duration>0.407</duration>
          <className>org.apache.spark.sql.hive.ParquetHiveCompatibilitySuite</className>
          <testName>simple primitives</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.353</duration>
          <className>org.apache.spark.sql.hive.ParquetHiveCompatibilitySuite</className>
          <testName>SPARK-10177 timestamp</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.413</duration>
          <className>org.apache.spark.sql.hive.ParquetHiveCompatibilitySuite</className>
          <testName>array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.371</duration>
          <className>org.apache.spark.sql.hive.ParquetHiveCompatibilitySuite</className>
          <testName>map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.ParquetHiveCompatibilitySuite</className>
          <testName>map entries with null keys</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.403</duration>
          <className>org.apache.spark.sql.hive.ParquetHiveCompatibilitySuite</className>
          <testName>struct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.343</duration>
          <className>org.apache.spark.sql.hive.ParquetHiveCompatibilitySuite</className>
          <testName>SPARK-16344: array of struct with a single field named &apos;array_element&apos;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.ParquetMetastoreSuite.xml</file>
      <name>org.apache.spark.sql.hive.ParquetMetastoreSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>14.561002</duration>
      <cases>
        <case>
          <duration>0.305</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>ordering of the partitioning columns partitioned_parquet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.461</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>project the partitioning column partitioned_parquet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.477</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>project partitioning and non-partitioning columns partitioned_parquet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.152</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>simple count partitioned_parquet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.127</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>pruned count partitioned_parquet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.124</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>non-existent partition partitioned_parquet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.137</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>multi-partition pruned count partitioned_parquet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.155</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>non-partition predicates partitioned_parquet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.154</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>sum partitioned_parquet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.241</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>hive udfs partitioned_parquet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.229</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>ordering of the partitioning columns partitioned_parquet_with_key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.388</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>project the partitioning column partitioned_parquet_with_key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.371</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>project partitioning and non-partitioning columns partitioned_parquet_with_key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.132</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>simple count partitioned_parquet_with_key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.146</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>pruned count partitioned_parquet_with_key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.11</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>non-existent partition partitioned_parquet_with_key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.136</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>multi-partition pruned count partitioned_parquet_with_key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.163</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>non-partition predicates partitioned_parquet_with_key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.156</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>sum partitioned_parquet_with_key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.241</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>hive udfs partitioned_parquet_with_key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.24</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>ordering of the partitioning columns partitioned_parquet_with_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.435</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>project the partitioning column partitioned_parquet_with_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.421</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>project partitioning and non-partitioning columns partitioned_parquet_with_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.171</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>simple count partitioned_parquet_with_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.147</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>pruned count partitioned_parquet_with_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.12</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>non-existent partition partitioned_parquet_with_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.145</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>multi-partition pruned count partitioned_parquet_with_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.171</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>non-partition predicates partitioned_parquet_with_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.168</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>sum partitioned_parquet_with_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.215</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>hive udfs partitioned_parquet_with_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.232</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>ordering of the partitioning columns partitioned_parquet_with_key_and_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.403</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>project the partitioning column partitioned_parquet_with_key_and_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.373</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>project partitioning and non-partitioning columns partitioned_parquet_with_key_and_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.165</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>simple count partitioned_parquet_with_key_and_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.131</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>pruned count partitioned_parquet_with_key_and_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.112</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>non-existent partition partitioned_parquet_with_key_and_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.128</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>multi-partition pruned count partitioned_parquet_with_key_and_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.159</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>non-partition predicates partitioned_parquet_with_key_and_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.166</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>sum partitioned_parquet_with_key_and_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.202</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>hive udfs partitioned_parquet_with_key_and_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.17</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>SPARK-5775 read struct from partitioned_parquet_with_key_and_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.159</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>SPARK-5775 read array from partitioned_parquet_with_key_and_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.151</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>SPARK-5775 read struct from partitioned_parquet_with_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.142</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>SPARK-5775 read array from partitioned_parquet_with_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.116</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>non-part select(*)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>conversion is working</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.087</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>scan an empty parquet table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.096</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>scan an empty parquet table with upper case</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.989</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>insert into an empty parquet table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.239</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>scan a parquet table created through a CTAS statement</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.292</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>MetastoreRelation in InsertIntoTable will be converted</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.267</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>MetastoreRelation in InsertIntoHiveTable will be converted</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.081</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>SPARK-6450 regression test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.086</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>SPARK-7749: non-partitioned metastore Parquet table lookup should use cached relation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.072</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>SPARK-7749: partitioned metastore Parquet table lookup should use cached relation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.316</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>SPARK-15968: nonempty partitioned metastore Parquet table lookup should use cached relation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.819</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>Caching converted data source Parquet Relations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.478</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>SPARK-15248: explicitly added partitions should be readable</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.276</duration>
          <className>org.apache.spark.sql.hive.ParquetMetastoreSuite</className>
          <testName>self-join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.ParquetSourceSuite.xml</file>
      <name>org.apache.spark.sql.hive.ParquetSourceSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>9.736001</duration>
      <cases>
        <case>
          <duration>0.141</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>ordering of the partitioning columns partitioned_parquet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.34</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>project the partitioning column partitioned_parquet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.403</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>project partitioning and non-partitioning columns partitioned_parquet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.122</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>simple count partitioned_parquet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.099</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>pruned count partitioned_parquet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.082</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>non-existent partition partitioned_parquet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.142</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>multi-partition pruned count partitioned_parquet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.143</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>non-partition predicates partitioned_parquet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.117</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>sum partitioned_parquet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.157</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>hive udfs partitioned_parquet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.15</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>ordering of the partitioning columns partitioned_parquet_with_key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.353</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>project the partitioning column partitioned_parquet_with_key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.388</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>project partitioning and non-partitioning columns partitioned_parquet_with_key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.138</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>simple count partitioned_parquet_with_key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.096</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>pruned count partitioned_parquet_with_key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.078</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>non-existent partition partitioned_parquet_with_key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.099</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>multi-partition pruned count partitioned_parquet_with_key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.125</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>non-partition predicates partitioned_parquet_with_key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.127</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>sum partitioned_parquet_with_key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.149</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>hive udfs partitioned_parquet_with_key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.145</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>ordering of the partitioning columns partitioned_parquet_with_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.348</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>project the partitioning column partitioned_parquet_with_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.354</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>project partitioning and non-partitioning columns partitioned_parquet_with_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.147</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>simple count partitioned_parquet_with_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.099</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>pruned count partitioned_parquet_with_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.086</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>non-existent partition partitioned_parquet_with_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.116</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>multi-partition pruned count partitioned_parquet_with_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.135</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>non-partition predicates partitioned_parquet_with_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.138</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>sum partitioned_parquet_with_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.158</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>hive udfs partitioned_parquet_with_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.159</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>ordering of the partitioning columns partitioned_parquet_with_key_and_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.383</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>project the partitioning column partitioned_parquet_with_key_and_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.361</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>project partitioning and non-partitioning columns partitioned_parquet_with_key_and_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.139</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>simple count partitioned_parquet_with_key_and_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.102</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>pruned count partitioned_parquet_with_key_and_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.082</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>non-existent partition partitioned_parquet_with_key_and_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.107</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>multi-partition pruned count partitioned_parquet_with_key_and_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.136</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>non-partition predicates partitioned_parquet_with_key_and_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.119</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>sum partitioned_parquet_with_key_and_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.178</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>hive udfs partitioned_parquet_with_key_and_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.1</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>SPARK-5775 read struct from partitioned_parquet_with_key_and_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.121</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>SPARK-5775 read array from partitioned_parquet_with_key_and_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.098</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>SPARK-5775 read struct from partitioned_parquet_with_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.099</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>SPARK-5775 read array from partitioned_parquet_with_complextypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.124</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>non-part select(*)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.459</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>SPARK-6016 make sure to use the latest footers</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.396</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>SPARK-8811: compatibility with array of struct in Hive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.534</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>Verify the PARQUET conversion parameter: CONVERT_METASTORE_PARQUET</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.291</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>values in arrays and maps stored in parquet are always nullable</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.573</duration>
          <className>org.apache.spark.sql.hive.ParquetSourceSuite</className>
          <testName>Aggregation attribute names can&apos;t contain special chars &quot; ,;{}()\n\t=&quot;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite.xml</file>
      <name>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>28.466</duration>
      <cases>
        <case>
          <duration>0.768</duration>
          <className>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite</className>
          <testName>convert partition provider to hive with repair table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.561</duration>
          <className>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite</className>
          <testName>when partition management is enabled, new tables have partition provider hive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.316</duration>
          <className>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite</className>
          <testName>when partition management is disabled, new tables have no partition provider</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.474</duration>
          <className>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite</className>
          <testName>when partition management is disabled, we preserve the old behavior even for new tables</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.651</duration>
          <className>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite</className>
          <testName>insert overwrite partition of legacy datasource table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.066</duration>
          <className>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite</className>
          <testName>insert overwrite partition of new datasource table overwrites just partition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.361</duration>
          <className>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite</className>
          <testName>sanity check table setup</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.737</duration>
          <className>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite</className>
          <testName>insert into partial dynamic partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.331</duration>
          <className>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite</className>
          <testName>insert into fully dynamic partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.078</duration>
          <className>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite</className>
          <testName>insert into static partition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>9.658</duration>
          <className>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite</className>
          <testName>overwrite partial dynamic partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.058</duration>
          <className>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite</className>
          <testName>overwrite fully dynamic partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.407</duration>
          <className>org.apache.spark.sql.hive.PartitionProviderCompatibilitySuite</className>
          <testName>overwrite static partition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite.xml</file>
      <name>org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>15.702001</duration>
      <cases>
        <case>
          <duration>0.859</duration>
          <className>org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite</className>
          <testName>hive table: partitioned pruned table reports only selected files</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.808</duration>
          <className>org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite</className>
          <testName>datasource table: partitioned pruned table reports only selected files</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.868</duration>
          <className>org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite</className>
          <testName>hive table: lazy partition pruning reads only necessary partition data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.924</duration>
          <className>org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite</className>
          <testName>datasource table: lazy partition pruning reads only necessary partition data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.965</duration>
          <className>org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite</className>
          <testName>hive table: lazy partition pruning with file status caching enabled</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.984</duration>
          <className>org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite</className>
          <testName>datasource table: lazy partition pruning with file status caching enabled</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.672</duration>
          <className>org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite</className>
          <testName>hive table: file status caching respects refresh table and refreshByPath</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.623</duration>
          <className>org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite</className>
          <testName>datasource table: file status caching respects refresh table and refreshByPath</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.545</duration>
          <className>org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite</className>
          <testName>hive table: file status cache respects size limit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.514</duration>
          <className>org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite</className>
          <testName>datasource table: file status cache respects size limit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.2</duration>
          <className>org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite</className>
          <testName>hive table: num hive client calls does not scale with partition count</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.5</duration>
          <className>org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite</className>
          <testName>datasource table: num hive client calls does not scale with partition count</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.669</duration>
          <className>org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite</className>
          <testName>hive table: files read and cached when filesource partition management is off</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.571</duration>
          <className>org.apache.spark.sql.hive.PartitionedTablePerfStatsSuite</className>
          <testName>datasource table: all partition data cached in memory when partition management is off</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.QueryPartitionSuite.xml</file>
      <name>org.apache.spark.sql.hive.QueryPartitionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.9580001</duration>
      <cases>
        <case>
          <duration>1.213</duration>
          <className>org.apache.spark.sql.hive.QueryPartitionSuite</className>
          <testName>SPARK-5068: query data when path doesn&apos;t exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.745</duration>
          <className>org.apache.spark.sql.hive.QueryPartitionSuite</className>
          <testName>SPARK-13709: reading partitioned Avro table with nested schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.ShowCreateTableSuite.xml</file>
      <name>org.apache.spark.sql.hive.ShowCreateTableSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>3.8059998</duration>
      <cases>
        <case>
          <duration>0.226</duration>
          <className>org.apache.spark.sql.hive.ShowCreateTableSuite</className>
          <testName>data source table with user specified schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.239</duration>
          <className>org.apache.spark.sql.hive.ShowCreateTableSuite</className>
          <testName>data source table CTAS</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.421</duration>
          <className>org.apache.spark.sql.hive.ShowCreateTableSuite</className>
          <testName>partitioned data source table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.307</duration>
          <className>org.apache.spark.sql.hive.ShowCreateTableSuite</className>
          <testName>bucketed data source table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.418</duration>
          <className>org.apache.spark.sql.hive.ShowCreateTableSuite</className>
          <testName>partitioned bucketed data source table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.633</duration>
          <className>org.apache.spark.sql.hive.ShowCreateTableSuite</className>
          <testName>data source table using Dataset API</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.167</duration>
          <className>org.apache.spark.sql.hive.ShowCreateTableSuite</className>
          <testName>simple hive table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.162</duration>
          <className>org.apache.spark.sql.hive.ShowCreateTableSuite</className>
          <testName>simple external hive table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.168</duration>
          <className>org.apache.spark.sql.hive.ShowCreateTableSuite</className>
          <testName>partitioned hive table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.177</duration>
          <className>org.apache.spark.sql.hive.ShowCreateTableSuite</className>
          <testName>hive table with explicit storage info</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.165</duration>
          <className>org.apache.spark.sql.hive.ShowCreateTableSuite</className>
          <testName>hive table with STORED AS clause</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.185</duration>
          <className>org.apache.spark.sql.hive.ShowCreateTableSuite</className>
          <testName>hive table with serde info</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.155</duration>
          <className>org.apache.spark.sql.hive.ShowCreateTableSuite</className>
          <testName>hive view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.138</duration>
          <className>org.apache.spark.sql.hive.ShowCreateTableSuite</className>
          <testName>hive view with output columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.086</duration>
          <className>org.apache.spark.sql.hive.ShowCreateTableSuite</className>
          <testName>hive bucketing is not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.159</duration>
          <className>org.apache.spark.sql.hive.ShowCreateTableSuite</className>
          <testName>hive partitioned view is not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.StatisticsSuite.xml</file>
      <name>org.apache.spark.sql.hive.StatisticsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>8.941</duration>
      <cases>
        <case>
          <duration>0.134</duration>
          <className>org.apache.spark.sql.hive.StatisticsSuite</className>
          <testName>column stats round trip serialization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.573</duration>
          <className>org.apache.spark.sql.hive.StatisticsSuite</className>
          <testName>analyze column command - result verification</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.095</duration>
          <className>org.apache.spark.sql.hive.StatisticsSuite</className>
          <testName>MetastoreRelations fallback to HDFS for size estimation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.227</duration>
          <className>org.apache.spark.sql.hive.StatisticsSuite</className>
          <testName>analyze MetastoreRelations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.267</duration>
          <className>org.apache.spark.sql.hive.StatisticsSuite</className>
          <testName>analyzing views is not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.356</duration>
          <className>org.apache.spark.sql.hive.StatisticsSuite</className>
          <testName>test table-level statistics for hive tables created in HiveExternalCatalog</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.483</duration>
          <className>org.apache.spark.sql.hive.StatisticsSuite</className>
          <testName>test elimination of the influences of the old stats</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.683</duration>
          <className>org.apache.spark.sql.hive.StatisticsSuite</className>
          <testName>test statistics of LogicalRelation converted from MetastoreRelation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.417</duration>
          <className>org.apache.spark.sql.hive.StatisticsSuite</className>
          <testName>verify serialized column stats after analyzing columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.545</duration>
          <className>org.apache.spark.sql.hive.StatisticsSuite</className>
          <testName>test table-level statistics for data source table created in HiveExternalCatalog</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.909</duration>
          <className>org.apache.spark.sql.hive.StatisticsSuite</className>
          <testName>test table-level statistics for partitioned data source table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.267</duration>
          <className>org.apache.spark.sql.hive.StatisticsSuite</className>
          <testName>statistics collection of a table with zero column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.389</duration>
          <className>org.apache.spark.sql.hive.StatisticsSuite</className>
          <testName>test refreshing table stats of cached data source table by `ANALYZE TABLE` statement</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.sql.hive.StatisticsSuite</className>
          <testName>estimates the size of a test MetastoreRelation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.316</duration>
          <className>org.apache.spark.sql.hive.StatisticsSuite</className>
          <testName>auto converts to broadcast hash join, by size estimate of a relation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.259</duration>
          <className>org.apache.spark.sql.hive.StatisticsSuite</className>
          <testName>auto converts to broadcast left semi join, by size estimate of a relation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.UDFSuite.xml</file>
      <name>org.apache.spark.sql.hive.UDFSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.72800004</duration>
      <cases>
        <case>
          <duration>0.272</duration>
          <className>org.apache.spark.sql.hive.UDFSuite</className>
          <testName>UDF case insensitive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.045</duration>
          <className>org.apache.spark.sql.hive.UDFSuite</className>
          <testName>temporary function: create and drop</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.097</duration>
          <className>org.apache.spark.sql.hive.UDFSuite</className>
          <testName>permanent function: create and drop without specifying db name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.121</duration>
          <className>org.apache.spark.sql.hive.UDFSuite</className>
          <testName>permanent function: create and drop with a db name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.193</duration>
          <className>org.apache.spark.sql.hive.UDFSuite</className>
          <testName>permanent function: create and drop a function in another db</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.client.FiltersSuite.xml</file>
      <name>org.apache.spark.sql.hive.client.FiltersSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.004</duration>
      <cases>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.hive.client.FiltersSuite</className>
          <testName>string filter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.hive.client.FiltersSuite</className>
          <testName>string filter backwards</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.hive.client.FiltersSuite</className>
          <testName>int filter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.hive.client.FiltersSuite</className>
          <testName>int filter backwards</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.hive.client.FiltersSuite</className>
          <testName>int and string filter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.hive.client.FiltersSuite</className>
          <testName>skip varchar</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.client.HiveClientSuite.xml</file>
      <name>org.apache.spark.sql.hive.client.HiveClientSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>-0.001</duration>
      <cases>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.client.HiveClientSuite</className>
          <testName>sql=false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.execution.BigDataBenchmarkSuite.xml</file>
      <name>org.apache.spark.sql.hive.execution.BigDataBenchmarkSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>-0.001</duration>
      <cases>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.BigDataBenchmarkSuite</className>
          <testName>No data files found for BigDataBenchmark tests</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.execution.ConcurrentHiveSuite.xml</file>
      <name>org.apache.spark.sql.hive.execution.ConcurrentHiveSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>-0.001</duration>
      <cases>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.ConcurrentHiveSuite</className>
          <testName>multiple instances not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.execution.HashAggregationQuerySuite.xml</file>
      <name>org.apache.spark.sql.hive.execution.HashAggregationQuerySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>24.947</duration>
      <cases>
        <case>
          <duration>0.438</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQuerySuite</className>
          <testName>group by function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.312</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQuerySuite</className>
          <testName>empty table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.126</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQuerySuite</className>
          <testName>null literal</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.924</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQuerySuite</className>
          <testName>only do grouping</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.784</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQuerySuite</className>
          <testName>case in-sensitive resolution</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.424</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQuerySuite</className>
          <testName>test average no key in output</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.577</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQuerySuite</className>
          <testName>test average</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.489</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQuerySuite</className>
          <testName>first_value and last_value</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.553</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQuerySuite</className>
          <testName>udaf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.802</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQuerySuite</className>
          <testName>interpreted aggregate function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.946</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQuerySuite</className>
          <testName>interpreted and expression-based aggregation functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.703</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQuerySuite</className>
          <testName>single distinct column set</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.905</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQuerySuite</className>
          <testName>single distinct multiple columns set</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.486</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQuerySuite</className>
          <testName>multiple distinct multiple columns sets</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.511</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQuerySuite</className>
          <testName>test count</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.935</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQuerySuite</className>
          <testName>pearson correlation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.497</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQuerySuite</className>
          <testName>covariance: covar_pop and covar_samp</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.42</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQuerySuite</className>
          <testName>no aggregation function (SPARK-11486)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.318</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQuerySuite</className>
          <testName>udaf with all data types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.697</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQuerySuite</className>
          <testName>udaf without specifying inputSchema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.042</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQuerySuite</className>
          <testName>SPARK-15206: single distinct aggregate function in having clause</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.058</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQuerySuite</className>
          <testName>SPARK-15206: multiple distinct aggregate function in having clause</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite.xml</file>
      <name>org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>214.519</duration>
      <cases>
        <case>
          <duration>2.314</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite</className>
          <testName>group by function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>5.589</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite</className>
          <testName>empty table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.636</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite</className>
          <testName>null literal</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>19.36</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite</className>
          <testName>only do grouping</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>18.951</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite</className>
          <testName>case in-sensitive resolution</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.48</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite</className>
          <testName>test average no key in output</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>14.378</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite</className>
          <testName>test average</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.636</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite</className>
          <testName>first_value and last_value</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.125</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite</className>
          <testName>udaf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>5.895</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite</className>
          <testName>interpreted aggregate function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>7.743</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite</className>
          <testName>interpreted and expression-based aggregation functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>29.5</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite</className>
          <testName>single distinct column set</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>8.026</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite</className>
          <testName>single distinct multiple columns set</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>34.106</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite</className>
          <testName>multiple distinct multiple columns sets</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>12.706</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite</className>
          <testName>test count</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>8.457</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite</className>
          <testName>pearson correlation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.435</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite</className>
          <testName>covariance: covar_pop and covar_samp</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>5.716</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite</className>
          <testName>no aggregation function (SPARK-11486)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>5.299</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite</className>
          <testName>udaf with all data types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.783</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite</className>
          <testName>udaf without specifying inputSchema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>7.725</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite</className>
          <testName>SPARK-15206: single distinct aggregate function in having clause</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>12.659</duration>
          <className>org.apache.spark.sql.hive.execution.HashAggregationQueryWithControlledFallbackSuite</className>
          <testName>SPARK-15206: multiple distinct aggregate function in having clause</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.execution.HiveCommandSuite.xml</file>
      <name>org.apache.spark.sql.hive.execution.HiveCommandSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>7.952</duration>
      <cases>
        <case>
          <duration>0.531</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCommandSuite</className>
          <testName>show tables</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.194</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCommandSuite</className>
          <testName>show tblproperties of data source tables - basic</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCommandSuite</className>
          <testName>show tblproperties for datasource table - errors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.094</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCommandSuite</className>
          <testName>show tblproperties for hive table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.058</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCommandSuite</className>
          <testName>show tblproperties for spark temporary table - empty row</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.865</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCommandSuite</className>
          <testName>LOAD DATA</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.072</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCommandSuite</className>
          <testName>LOAD DATA: input path</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.627</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCommandSuite</className>
          <testName>Truncate Table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.163</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCommandSuite</className>
          <testName>show partitions - show everything</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.098</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCommandSuite</className>
          <testName>show partitions - show everything more than 5 part keys</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.258</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCommandSuite</className>
          <testName>show partitions - filter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.063</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCommandSuite</className>
          <testName>show partitions - empty row</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.882</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCommandSuite</className>
          <testName>show partitions - datasource</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.xml</file>
      <name>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>534.44086</duration>
      <cases>
        <case>
          <duration>1.789</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>add_part_exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.559</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>add_part_multiple</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.343</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>add_partition_no_whitelist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.29</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>add_partition_with_whitelist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.148</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>alias_casted_column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>alter_char1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>alter_char2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>alter_db_owner</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>alter_partition_coltype</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.33</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>alter_partition_with_whitelist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>alter_rename_partition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>alter_view_rename</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.318</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ambiguous_col</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ansi_sql_arithmetic</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>archive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>archive_excludeHadoop20</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>archive_multi</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>auto_join16</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>auto_join_without_localtask</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ba_table1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ba_table2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ba_table3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ba_table_udfs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ba_table_union</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.122</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>binary_constant</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>binary_output_format</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>binary_table_bincolserde</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>binary_table_colserde</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.747</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>binarysortable_1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.565</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>cast1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>cast_to_int</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>char_1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>char_2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>char_cast</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>char_comparison</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>char_join1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>char_nested_types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>char_serde</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>char_udf1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>char_union1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>char_varchar_udf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.54</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>cluster</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>columnarserde_create_shortcut</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>combine2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>combine2_hadoop20</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>combine2_win</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>combine3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>compile_processor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>constant_prop</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.176</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>convert_enum_to_string</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.124</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>correlationoptimizer10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.843</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>correlationoptimizer11</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>correlationoptimizer12</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.236</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>correlationoptimizer13</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.398</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>correlationoptimizer14</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.273</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>correlationoptimizer15</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>correlationoptimizer5</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>13.063</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>correlationoptimizer6</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.516</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>correlationoptimizer7</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.685</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>correlationoptimizer8</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>5.434</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>correlationoptimizer9</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.039</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>count</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.09</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>cp_mj_rc</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>create_1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>create_big_view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>create_escape</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>create_func1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>create_genericudaf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>create_genericudf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.774</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>create_insert_outputformat</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>create_like</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.358</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>create_nested_type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>create_or_replace_view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.269</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>create_struct_table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>create_udaf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>create_union_table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.159</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>cross_join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.914</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>cross_product_check_1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.909</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>cross_product_check_2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ctas_char</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ctas_colname</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ctas_date</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ctas_uses_database_location</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ctas_varchar</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>cte_1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>cte_2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>custom_input_output_format</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.906</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>date_2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.911</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>date_comparison</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>dbtxnmgr_compact1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>dbtxnmgr_compact2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>dbtxnmgr_compact3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>dbtxnmgr_ddl1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>dbtxnmgr_query1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>dbtxnmgr_query2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>dbtxnmgr_query3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>dbtxnmgr_query4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>dbtxnmgr_query5</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>dbtxnmgr_showlocks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ddltime</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.264</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>decimal_1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>decimal_2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>decimal_3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.104</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>decimal_4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>decimal_5</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>decimal_6</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.545</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>decimal_join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>decimal_precision</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>decimal_serde</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>decimal_udf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.264</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>default_partition_name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.395</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>delimiter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>desc_non_existent_tbl</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>desc_tbl_part_cols</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>describe_table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>describe_xpath</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.202</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>disable_file_format_check</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.095</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>distinct_stats</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>driverhook</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>drop_function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.497</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>drop_multi_partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>drop_table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.585</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>drop_table2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>drop_udf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>drop_view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>drop_with_concurrency</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.143</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>dynamic_partition_skip_default</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>dynpart_sort_opt_vectorization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>dynpart_sort_optimization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>enforce_order</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>escape1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>escape2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>escape_clusterby1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>escape_distributeby1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.018</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>escape_orderby1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>escape_sortby1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>exchange_partition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>exchange_partition2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>exchange_partition3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>exim_00_nonpart_empty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>exim_01_nonpart</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>exim_02_00_part_empty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>exim_02_part</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>exim_03_nonpart_over_compat</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>exim_04_all_part</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>exim_04_evolved_parts</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>exim_05_some_part</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>exim_06_one_part</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>exim_07_all_part_over_nonoverlap</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>exim_08_nonpart_rename</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>exim_09_part_spec_nonoverlap</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>exim_10_external_managed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>exim_11_managed_external</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>exim_12_external_location</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>exim_13_managed_location</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>exim_14_managed_location_over_existing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>exim_15_external_part</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>exim_16_part_external</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>exim_17_part_managed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>exim_18_part_external</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>exim_19_00_part_external_location</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>exim_19_part_external_location</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>exim_20_part_managed_location</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>exim_21_export_authsuccess</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>exim_22_import_exist_authsuccess</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>exim_23_import_part_authsuccess</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>exim_24_import_nonexist_authsuccess</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>exim_hidden_files</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>explain_dependency</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>explain_dependency2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>explain_logical</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>explode_null</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>external_table_with_space_in_location_path</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>fetch_aggregation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>file_with_header_footer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.568</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>fileformat_sequencefile</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.459</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>fileformat_text</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.475</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>filter_join_breaktask</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.957</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>filter_join_breaktask2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>filter_numeric</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>global_limit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.194</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.303</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby11</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.355</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby12</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.493</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby1_limit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.234</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby1_map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.725</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby1_map_nomap</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.108</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby1_map_skew</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.23</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby1_noskew</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.225</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.821</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby2_limit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.083</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby2_map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby2_map_multi_distinct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.085</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby2_map_skew</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.714</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby2_noskew</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby2_noskew_multi_distinct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby3_map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby3_map_multi_distinct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby3_map_skew</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby3_noskew</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby3_noskew_multi_distinct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.847</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.545</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby4_map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.467</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby4_map_skew</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.354</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby4_noskew</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.82</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby5</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.533</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby5_map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.507</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby5_map_skew</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.065</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby5_noskew</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.734</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby6</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.134</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby6_map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.107</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby6_map_skew</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.177</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby6_noskew</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.337</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby7</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>5.782</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby7_map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>5.844</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby7_map_multi_single_reducer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>5.884</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby7_map_skew</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>5.811</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby7_noskew</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.57</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby7_noskew_multi_single_reducer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.107</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby8</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>7.981</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby8_map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>8.678</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby8_map_skew</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>7.942</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby8_noskew</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>6.793</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby9</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby_bigdata</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby_complex_types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby_complex_types_multi_single_reducer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby_cube1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.451</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby_distinct_samekey</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.307</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby_grouping_sets2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.953</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby_grouping_sets3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.776</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby_grouping_sets4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.206</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby_grouping_sets5</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.373</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby_map_ppr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby_map_ppr_multi_distinct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.863</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby_multi_insert_common_distinct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby_multi_single_reducer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.574</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby_multi_single_reducer2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.827</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby_multi_single_reducer3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby_mutli_insert_common_distinct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.336</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby_neg_float</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby_position</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.274</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby_ppd</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.635</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby_ppr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby_ppr_multi_distinct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby_resolution</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby_rollup1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby_sort_1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby_sort_11</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.999</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby_sort_6</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>groupby_sort_skew_1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.447</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>having</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.342</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>implicit_cast1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>import_exported_table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>index_auto</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>index_bitmap</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>index_bitmap1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>index_bitmap2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>index_bitmap3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>index_bitmap_auto</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>index_bitmap_rc</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>index_compact</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>index_compact_1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>index_compact_2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>index_compact_3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>index_creation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>infer_const_type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>init_file</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.164</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input0</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.123</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.136</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.412</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input11</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.643</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input11_limit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.002</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input12</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.004</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input12_hadoop20</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input13</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.905</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input14</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input14_limit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.143</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input15</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input16_cc</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input17</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input18</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.253</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input19</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.995</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input1_limit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.793</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input20</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.537</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input21</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.294</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input22</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.167</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input23</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.413</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input24</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.972</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input28</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.097</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input2_limit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input30</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input31</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input32</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input33</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input34</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input35</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input36</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input37</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input38</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input39</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input39_hadoop20</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input3_limit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.013</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input40</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.105</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input41</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input43</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input45</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.646</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input49</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.24</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input4_cb_delim</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input4_limit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input5</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.489</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input6</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.554</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input7</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.584</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input8</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.611</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input9</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input_columnarserde</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input_dynamicserde</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input_lazyserde</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.144</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input_limit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.689</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input_part0</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.276</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input_part1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.748</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input_part2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.156</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input_part3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.114</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input_part4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.21</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input_part5</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.145</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input_part6</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.291</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input_part7</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.113</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input_part8</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.322</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input_part9</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.461</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input_testsequencefile</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input_testxpath</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input_testxpath2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input_testxpath3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>input_testxpath4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.157</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>inputddl1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.14</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>inputddl2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.115</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>inputddl3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.472</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>insert1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.831</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>insert1_overwrite_partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.463</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>insert2_overwrite_partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>insert_into1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>insert_into2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>insert_into3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>insert_into4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>insert_into5</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>insert_into6</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>insert_overwrite_local_directory_1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>insertexternal1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.837</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.152</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.154</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join11</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.228</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join12</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.235</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join13</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.437</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join14</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.445</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join14_hadoop20</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.174</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join15</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join16</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.88</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join17</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.043</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join18</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.329</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join19</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.038</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.041</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join20</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.502</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join21</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join22</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.141</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join23</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.844</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join24</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.173</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join25</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.919</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join26</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.202</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join27</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.071</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join28</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.523</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join29</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.029</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.291</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join30</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.602</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join31</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.998</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join32</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.496</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join32_lessSize</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.103</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join33</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.357</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join34</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.562</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join35</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.845</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join36</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.179</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join37</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.92</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join38</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.106</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join39</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.998</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.22</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join40</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.779</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join41</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.829</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join5</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.807</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join6</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.002</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join7</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.791</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join8</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.485</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join9</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>17.458</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join_1to1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join_alt_syntax</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.65</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join_array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.521</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join_casesensitive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join_cond_pushdown_1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join_cond_pushdown_2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join_cond_pushdown_3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join_cond_pushdown_4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join_cond_pushdown_unqual1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join_cond_pushdown_unqual2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join_cond_pushdown_unqual3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join_cond_pushdown_unqual4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.118</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join_empty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join_filters_overlap</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.66</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join_hive_626</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join_literals</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.687</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join_map_ppr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join_merging</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.874</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join_rc</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join_reorder</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.234</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join_reorder2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.557</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join_reorder3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.279</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join_reorder4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.371</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join_star</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join_thrift</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>join_vc</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.296</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>lateral_view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.594</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>lateral_view_noalias</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.2</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>lateral_view_ppd</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>lb_fs_stats</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>leadlag</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>leadlag_queries</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.789</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>leftsemijoin_mr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>limit_partition_metadataonly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>limit_pushdown</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.428</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>limit_pushdown_negative</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.31</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>lineage1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>literal_decimal</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.121</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>literal_ints</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.089</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>literal_string</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>load_binary_data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.178</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>load_dyn_part1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.691</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>load_dyn_part10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.599</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>load_dyn_part11</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.644</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>load_dyn_part12</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.388</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>load_dyn_part13</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>load_dyn_part15</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.863</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>load_dyn_part3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.834</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>load_dyn_part4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>12.423</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>load_dyn_part5</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.458</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>load_dyn_part6</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.477</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>load_dyn_part7</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.983</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>load_dyn_part8</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.594</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>load_dyn_part9</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>load_exist_part_authsuccess</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.468</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>load_file_with_space_in_the_name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>load_fs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>load_fs2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>load_fs_overwrite</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>load_hdfs_file_with_space_in_the_name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>load_nonpart_authsuccess</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>load_part_authsuccess</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.998</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>loadpart1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>loadpart2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>loadpart_err</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>lock1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>lock2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>lock3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>lock4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.567</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>louter_join_ppr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>mapjoin1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>mapjoin_addjar</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>mapjoin_decimal</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.519</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>mapjoin_distinct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.961</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>mapjoin_filter_on_outerjoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.086</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>mapjoin_mapjoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>mapjoin_memcheck</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.671</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>mapjoin_subquery</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.466</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>mapjoin_test_outer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.87</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>mapreduce1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.885</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>mapreduce2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.554</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>mapreduce3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.809</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>mapreduce4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.755</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>mapreduce5</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.7</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>mapreduce6</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.593</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>mapreduce7</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.777</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>mapreduce8</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.55</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>merge1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.573</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>merge2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>merge3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>merge_dynamic_partition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>merge_dynamic_partition2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>merge_dynamic_partition3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>merge_dynamic_partition4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>merge_dynamic_partition5</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.494</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>mergejoins</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>metadata_only_queries</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>metadata_only_queries_with_filters</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>metadataonly1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>mi</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>mrr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>8.012</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>multiMapJoin2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>multi_insert</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.72</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>multi_insert_gby</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>multi_insert_gby2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.435</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>multi_insert_gby3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>10.647</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>multi_insert_lateral_view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>multi_insert_move_tasks_share_dependencies</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.913</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>multi_join_union</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.631</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>multigroupby_singlemr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>nested_complex</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>nestedvirtual</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>newline</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>no_hooks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.188</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>noalias_subq1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.673</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>nonblock_op_deduplicate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>nonmr_fetch</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>nonmr_fetch_threshold</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>nonreserved_keywords_input37</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>nonreserved_keywords_insert_into1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.88</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>notable_alias1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.821</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>notable_alias2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>notable_alias3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>null_cast</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>null_column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>nullformat</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>nullformatdir</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.319</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>nullgroup</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.543</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>nullgroup2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.506</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>nullgroup3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.09</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>nullgroup4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.503</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>nullgroup4_multi_distinct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.577</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>nullgroup5</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.437</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>nullinput</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.237</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>nullinput2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.352</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>nullscript</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>num_op_type_conv</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.177</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>optional_outer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>orc_analyze</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>orc_create</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>orc_createas1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>orc_diff_part_cols</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>orc_diff_part_cols2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>orc_empty_strings</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>orc_min_max</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>orc_ppd_char</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>orc_ppd_date</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>orc_ppd_decimal</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>orc_ppd_varchar</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>orc_split_elimination</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>orc_vectorization_ppd</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.174</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>order</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.093</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>order2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>order_within_subquery</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.938</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>outer_join_ppr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.539</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>parallel</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>parallel_orderby</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.36</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>parenthesis_star_by</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>parquet_create</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>parquet_ctas</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>parquet_partitioned</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>parquet_types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>partInit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.661</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>partcols1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>partition_date2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>partition_decode_name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.596</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>partition_serde_format</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>partition_special_char</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.054</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>partition_type_check</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>partition_varchar2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>partition_vs_table_metadata</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>partition_wise_fileformat</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>partition_wise_fileformat10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>partition_wise_fileformat11</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>partition_wise_fileformat12</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>partition_wise_fileformat13</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>partition_wise_fileformat14</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>partition_wise_fileformat15</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>partition_wise_fileformat16</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>partition_wise_fileformat17</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>partition_wise_fileformat18</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>partition_wise_fileformat2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>partition_wise_fileformat3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>partition_wise_fileformat8</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.808</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>partition_wise_fileformat9</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>pcr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.127</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ppd1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.43</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ppd2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.578</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ppd_clusterby</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.886</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ppd_constant_expr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.677</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ppd_constant_where</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.491</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ppd_gby</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.005</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ppd_gby2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.137</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ppd_gby_join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.543</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ppd_join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.91</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ppd_join2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.77</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ppd_join3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ppd_join4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.31</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ppd_join_filter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ppd_multi_insert</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.321</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ppd_outer_join1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.343</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ppd_outer_join2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.262</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ppd_outer_join3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.535</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ppd_outer_join4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.635</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ppd_outer_join5</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.156</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ppd_random</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ppd_transform</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ppd_udf_case</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.151</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ppd_udf_col</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ppd_udtf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.311</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ppd_union</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ppd_union_view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ppd_vc</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.763</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ppr_allchildsarenull</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.955</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ppr_pushdown</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.049</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ppr_pushdown2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.821</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ppr_pushdown3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>print_header</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.783</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>progress_1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ptf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ptf_decimal</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ptf_general_queries</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ptf_matchpath</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ptf_rcfile</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ptf_register_tblfn</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>ptf_seqfile</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>query_result_fileformat</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.458</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>query_with_semi</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.6</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>quote1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.135</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>quote2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>quotedid_alter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>quotedid_basic</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>quotedid_partition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>quotedid_skew</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>quotedid_smb</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>quotedid_tblproperty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>rand_partitionpruner1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>rand_partitionpruner2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>rand_partitionpruner3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>rcfile_bigdata</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.672</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>rcfile_columnar</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>rcfile_createas1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>rcfile_default_format</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.091</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>rcfile_lazydecompress</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>rcfile_merge1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>rcfile_merge2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>rcfile_merge3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>rcfile_merge4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.229</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>rcfile_null_value</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.697</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>rcfile_toleratecorruptions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.112</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>rcfile_union</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>recursive_dir</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.368</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>reduce_deduplicate_exclude_gby</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.126</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>reduce_deduplicate_exclude_join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>6.686</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>reduce_deduplicate_extended</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>regex_col</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>regexp_extract</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>remote_script</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>rename_external_partition_location</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>rename_partition_location</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>rename_table_location</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>reset_conf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>root_dir_external_table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.236</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>router_join_ppr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>sample1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>sample2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>sample3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>sample4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>sample5</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>sample6</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>sample7</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>sample8</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>sample9</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>sample_islocalmode_hook</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>sample_islocalmode_hook_hadoop20</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>schemeAuthority</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>schemeAuthority2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>script_env_var1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>script_env_var2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>script_pipe</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>scriptfile1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>scriptfile1_win</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.094</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>select_as_omitted</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>select_dummy_source</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>select_transform_hint</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.939</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>select_unquote_and</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.745</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>select_unquote_not</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.732</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>select_unquote_or</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.05</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>semicolon</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>7.016</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>semijoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.286</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>serde_regex</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>serde_user_properties</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>set_processor_namespaces</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.141</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>set_variable_sub</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.29</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>show_columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>show_describe_func_quotes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.061</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>show_functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>show_indexes_edge_cases</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>show_indexes_syntax</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.525</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>show_partitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>show_roles</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>show_tables</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>show_tablestatus</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>showparts</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>skewjoin_noskew</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>skewjoin_union_remove_1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>skewjoin_union_remove_2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>skewjoinopt1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>skewjoinopt10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>skewjoinopt11</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>skewjoinopt12</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>skewjoinopt14</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>skewjoinopt15</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>skewjoinopt16</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>skewjoinopt17</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>skewjoinopt19</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>skewjoinopt2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>skewjoinopt20</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>skewjoinopt3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>skewjoinopt4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>skewjoinopt5</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>skewjoinopt6</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>skewjoinopt7</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>skewjoinopt8</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>smb_mapjoin_11</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>smb_mapjoin_12</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>smb_mapjoin_18</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>smb_mapjoin_19</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>smb_mapjoin_20</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>smb_mapjoin_22</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.206</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>sort</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>source</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>split</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>split_sample</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>stats2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>stats3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>stats4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>stats5</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>stats6</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>stats7</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>stats8</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>stats9</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.116</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>stats_aggregator_error_1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>stats_counter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>stats_counter_partitioned</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>stats_empty_dyn_part</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>stats_invalidation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>stats_noscan_1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>stats_noscan_2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>stats_only_null</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>stats_partscan_1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>stats_partscan_1_23</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>stats_publisher_error_1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>statsfs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>str_to_map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>subq</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.262</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>subq2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>subq_where_serialization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>subquery_alias</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.639</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>subquery_exists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.443</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>subquery_exists_having</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.328</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>subquery_in_having</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>subquery_multiinsert</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.434</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>subquery_notexists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.918</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>subquery_notexists_having</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>subquery_notin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>subquery_unqualcolumnrefs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>subquery_views</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>table_access_keys_stats</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.63</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>tablename_with_select</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>test_boolean_whereclause</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>tez_dml</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>tez_fsstat</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>tez_insert_overwrite_local_directory_1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>tez_join_tests</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>tez_joins_explain</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>tez_schema_evolution</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>tez_union</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.563</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>timestamp_comparison</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.31</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>timestamp_null</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>transform1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>transform2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.04</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>transform_ppr1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.42</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>transform_ppr2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>truncate_column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>truncate_column_merge</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.109</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>type_cast_1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>type_conversions_1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.307</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>type_widening</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.278</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udaf_collect_set</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udaf_context_ngrams</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.321</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udaf_histogram_numeric</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udaf_ngrams</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udaf_number_format</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udaf_percentile</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udaf_percentile_approx_20</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udaf_percentile_approx_23</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udaf_sum_list</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.48</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.551</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf5</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.573</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf6</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.584</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf8</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.107</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf9</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.393</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_10_trims</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.129</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_E</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.14</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_PI</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.26</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_acos</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_add</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_array</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_array_contains</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.114</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_ascii</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.25</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_asin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.247</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_atan</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_avg</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_between</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_bigint</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.142</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_bin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_bitwise_and</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_bitwise_not</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_bitwise_or</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_bitwise_xor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_boolean</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_case_column_pruning</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_case_thrift</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_ceil</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_ceiling</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_coalesce</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_compare_java_string</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.184</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_concat</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.709</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_concat_insert1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.455</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_concat_insert2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.765</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_concat_ws</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_context_aware</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.589</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_conv</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.141</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_cos</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.8</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_count</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_current_database</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_date_add</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_date_sub</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_datediff</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_day</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_dayofmonth</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.135</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_degrees</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.065</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_div</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_divide</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_double</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.142</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_elt</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.159</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_equal</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_exp</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_explode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.097</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_find_in_set</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_float</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_floor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_from_unixtime</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.077</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_greaterthan</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.09</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_greaterthanorequal</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.201</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_hex</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_hour</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.217</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_if</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_in</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_in_file</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_index</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_inline</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.13</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_instr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_int</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_isnotnull</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_isnull</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_isnull_isnotnull</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_lcase</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.793</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_length</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.085</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_lessthan</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.073</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_lessthanorequal</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.168</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_like</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_ln</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.148</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_locate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_log</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_log10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_log2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_logic_java_boolean</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.069</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_lower</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.067</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_lpad</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_ltrim</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.117</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_map_keys</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_map_values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_max</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_min</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_modulo</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_month</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.081</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_named_struct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.37</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_negative</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_not</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.145</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_notequal</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.095</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_notop</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.074</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_nvl</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_or</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.096</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_parse_url</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_percentile</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.603</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_pmod</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_positive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_pow</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_power</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_printf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.261</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_radians</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_rand</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.084</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_regexp</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_regexp_extract</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_regexp_replace</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.079</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_repeat</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_reverse</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_rlike</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_round_2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.078</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_rpad</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_rtrim</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_sentences</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.396</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_sign</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.135</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_sin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_size</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_smallint</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.156</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_space</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_split</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_sqrt</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_std</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_stddev</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_stddev_pop</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_stddev_samp</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_string</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.1</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_struct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_substr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_substring</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_subtract</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_sum</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.264</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_tan</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_testlength</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_testlength2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_tinyint</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_to_boolean</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.562</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_to_byte</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_to_date</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.543</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_to_double</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.547</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_to_float</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.503</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_to_long</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.503</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_to_short</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_to_string</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.068</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_translate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_trim</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_ucase</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.13</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_unhex</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_union</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.419</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_unix_timestamp</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_upper</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_using</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_var_pop</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_var_samp</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_variance</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.187</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udf_weekofyear</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udtf_explode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udtf_json_tuple</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udtf_parse_url_tuple</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udtf_posexplode</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>udtf_stack</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.742</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.476</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union11</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union12</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.106</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union13</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.541</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union14</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.372</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union15</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.053</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union16</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.034</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union17</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.032</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union18</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.203</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union19</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.091</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.339</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union20</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union21</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.558</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union22</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.399</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union23</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.569</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union24</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.176</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union25</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.104</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union26</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.922</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union27</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.237</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union28</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.597</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union29</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.902</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.172</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union30</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union32</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.399</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union33</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.901</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union34</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.614</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.307</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union5</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.651</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union6</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.403</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union7</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.119</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union8</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.15</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union9</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.75</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union_lateralview</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union_null</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.335</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union_ppr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union_remove_1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union_remove_10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union_remove_12</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union_remove_13</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union_remove_14</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union_remove_15</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union_remove_16</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union_remove_17</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union_remove_18</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union_remove_19</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union_remove_2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union_remove_20</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union_remove_21</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union_remove_22</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union_remove_23</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union_remove_24</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union_remove_4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union_remove_5</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.218</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union_remove_6</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union_remove_7</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union_remove_8</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union_remove_9</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.789</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union_script</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union_top_level</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>union_view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>unset_table_view_property</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>varchar_1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>varchar_cast</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>varchar_comparison</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>varchar_nested_types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>varchar_serde</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>varchar_udf1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vector_between_in</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vector_coalesce</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vector_decimal_aggregate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vector_decimal_cast</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vector_decimal_expressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vector_decimal_mapjoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vector_decimal_math_funcs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vector_left_outer_join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vector_non_string_partition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorization_0</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorization_1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorization_10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorization_11</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorization_12</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorization_13</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorization_14</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorization_15</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorization_16</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorization_2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorization_3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorization_4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorization_5</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorization_6</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorization_7</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorization_8</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorization_9</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorization_decimal_date</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorization_div0</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorization_limit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorization_nested_udf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorization_not</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorization_part</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorization_part_project</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorization_pushdown</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorization_short_regress</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorized_case</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorized_casts</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorized_context</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorized_date_funcs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorized_distinct_gby</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorized_mapjoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorized_math_funcs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorized_nested_mapjoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorized_rcfile_columnar</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorized_shufflejoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorized_string_funcs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>vectorized_timestamp_funcs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.488</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.581</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>view_inputs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveCompatibilitySuite</className>
          <testName>virtual_column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.execution.HiveDDLSuite.xml</file>
      <name>org.apache.spark.sql.hive.execution.HiveDDLSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>21.349998</duration>
      <cases>
        <case>
          <duration>0.094</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>drop tables</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.179</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>drop external tables in default database</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.213</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>drop external data source table in default database</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.192</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>create table and view with comment</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>create table: partition column names exist in table definition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.144</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>add/drop partitions - external table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.37</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>drop views</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.34</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>alter views - rename</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.511</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>alter views - set/unset tblproperties</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>create table - SET TBLPROPERTIES EXTERNAL to TRUE</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.17</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>alter table - SET TBLPROPERTIES EXTERNAL to TRUE</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.436</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>alter views and alter table - misuse</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.37</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>alter table partition - storage information</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.276</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>MSCK REPAIR RABLE</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.117</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>drop table using drop view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.304</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>drop view using drop table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.177</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>create view with mismatched schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.141</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>create view with specified schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.142</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>desc table for Hive table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.112</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>desc table for Hive table - partitioned table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.19</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>desc formatted table for permanent view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.166</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>desc table for data source table using Hive Metastore</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.264</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>create/drop database - location without pre-created directory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.214</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>create/drop database - location with pre-created directory</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.201</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>drop database containing tables - CASCADE</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.219</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>drop an empty database - CASCADE</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.177</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>drop database containing tables - RESTRICT</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.207</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>drop an empty database - RESTRICT</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>drop default database</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.132</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>Create Cataloged Table As Select - Drop Table After Runtime Exception</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.529</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>CREATE TABLE LIKE a temporary view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.782</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>CREATE TABLE LIKE a data source table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.719</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>CREATE TABLE LIKE an external data source table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.784</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>CREATE TABLE LIKE a managed Hive serde table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.488</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>CREATE TABLE LIKE an external Hive serde table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.932</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>CREATE TABLE LIKE a view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.25</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>desc table for data source table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.406</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>create table with the same name as an index table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.168</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>insert skewed table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.976</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>desc table for data source table - no user-defined schema</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.493</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>desc table for data source table - partitioned bucketed table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.136</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>datasource and statistics table property keys are not allowed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.577</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>truncate table - datasource table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.031</duration>
          <className>org.apache.spark.sql.hive.execution.HiveDDLSuite</className>
          <testName>truncate partitioned table - datasource table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.execution.HiveExplainSuite.xml</file>
      <name>org.apache.spark.sql.hive.execution.HiveExplainSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.282</duration>
      <cases>
        <case>
          <duration>0.186</duration>
          <className>org.apache.spark.sql.hive.execution.HiveExplainSuite</className>
          <testName>explain extended command</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.044</duration>
          <className>org.apache.spark.sql.hive.execution.HiveExplainSuite</className>
          <testName>explain create table command</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.032</duration>
          <className>org.apache.spark.sql.hive.execution.HiveExplainSuite</className>
          <testName>SPARK-17409: The EXPLAIN output of CTAS only shows the analyzed plan</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.sql.hive.execution.HiveExplainSuite</className>
          <testName>EXPLAIN CODEGEN command</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.execution.HiveOperatorQueryableSuite.xml</file>
      <name>org.apache.spark.sql.hive.execution.HiveOperatorQueryableSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.331</duration>
      <cases>
        <case>
          <duration>0.331</duration>
          <className>org.apache.spark.sql.hive.execution.HiveOperatorQueryableSuite</className>
          <testName>SPARK-5324 query result of describe command</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.execution.HivePlanTest.xml</file>
      <name>org.apache.spark.sql.hive.execution.HivePlanTest</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.041</duration>
      <cases>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.sql.hive.execution.HivePlanTest</className>
          <testName>udf constant folding</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.sql.hive.execution.HivePlanTest</className>
          <testName>window expressions sharing the same partition by and order by clause</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.execution.HiveQuerySuite.xml</file>
      <name>org.apache.spark.sql.hive.execution.HiveQuerySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>37.865993</duration>
      <cases>
        <case>
          <duration>0.467</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>SPARK-10484 Optimize the Cartesian (Cross) Join with broadcast based JOIN #1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.331</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>SPARK-10484 Optimize the Cartesian (Cross) Join with broadcast based JOIN #2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.382</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>SPARK-10484 Optimize the Cartesian (Cross) Join with broadcast based JOIN #3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.364</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>SPARK-10484 Optimize the Cartesian (Cross) Join with broadcast based JOIN #4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.08</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>SPARK-10484 Optimize the Cartesian (Cross) Join with broadcast based JOIN</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.571</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>insert table with generator with column name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.606</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>insert table with generator with multiple column names</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.535</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>insert table with generator without column name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>multiple generators in projection</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.135</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>! operator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.283</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>constant object inspector for generic udf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.243</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>NaN to Decimal</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.297</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>constant null testing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.04</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>constant null testing timestamp</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.271</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>null case</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.24</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>single case</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.258</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>double case</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.27</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>case else null</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.445</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>having no references</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.11</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>no from clause</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.294</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>boolean = number</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.261</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>CREATE TABLE AS runs once</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.254</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>between</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.292</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>div</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.069</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>division</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.286</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>modulus</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>Query expressed in HiveQL</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.044</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>Query with constant folding the CAST</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.49</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>Constant Folding Optimization for AVG_SUM_COUNT</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.302</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>Cast Timestamp to Timestamp in UDF</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.254</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>Date comparison test 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.262</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>Simple Average</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.236</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>Simple Average + 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.459</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>Simple Average + 1 with group</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.221</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>string literal</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.227</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>Escape sequences</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.182</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>IgnoreExplain</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.532</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>trivial join where clause</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.518</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>trivial join ON clause</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.325</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>udf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.751</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>partitioned table scan</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.483</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>create table as</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.456</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>create table as with db name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.399</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>create table as with db name within backticks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.44</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>insert table with db name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.777</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>insert into and insert overwrite</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.135</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>SPARK-7270: consider dynamic partition when comparing table output</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.431</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>transform</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.418</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>schema-less transform</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.329</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>transform with custom field delimiter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.295</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>transform with custom field delimiter2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.31</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>transform with custom field delimiter3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.293</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>transform with SerDe</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.357</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>transform with SerDe2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.316</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>transform with SerDe3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.292</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>transform with SerDe4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.23</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>LIKE</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.448</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>DISTINCT</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.267</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>empty aggregate input</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.272</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>lateral view1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.273</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>lateral view2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.294</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>lateral view3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.57</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>lateral view4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.294</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>lateral view5</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.302</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>lateral view6</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.281</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>Specify the udtf output</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.316</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>SPARK-9034 Reflect field names defined in GenericUDTF #1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.281</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>SPARK-9034 Reflect field names defined in GenericUDTF #2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>sampling</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>DataFrame toString</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.263</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>case statements with key #1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.267</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>case statements with key #2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.281</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>case statements with key #3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.234</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>case statements with key #4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.231</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>case statements WITHOUT key #1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.228</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>case statements WITHOUT key #2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.256</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>case statements WITHOUT key #3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.239</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>case statements WITHOUT key #4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.049</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>timestamp cast #1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.24</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>timestamp cast #2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>timestamp cast #3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.263</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>timestamp cast #4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.039</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>timestamp cast #5</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.236</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>timestamp cast #6</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.032</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>timestamp cast #7</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.227</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>timestamp cast #8</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.244</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>select null from table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.258</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>CTE feature #1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.277</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>CTE feature #2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.231</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>CTE feature #3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.335</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>get_json_object #1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.353</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>get_json_object #2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.316</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>get_json_object #3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.316</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>get_json_object #4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.362</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>get_json_object #5</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.312</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>get_json_object #6</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.321</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>get_json_object #7</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.347</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>get_json_object #8</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.341</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>get_json_object #9</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.321</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>get_json_object #10</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.045</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>predicates contains an empty AttributeSet() references</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.072</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>implement identity function using case statement</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>non-boolean conditions in a CaseWhen are illegal</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.25</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>case sensitivity when query Hive table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.062</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>case sensitivity: created temporary view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.147</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>SPARK-1704: Explain commands as a DataFrame</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.303</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>SPARK-2180: HAVING support in GROUP BY clauses (positive)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.341</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>SPARK-2180: HAVING with non-boolean clause raises no exceptions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.053</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>SPARK-2225: turn HAVING without GROUP BY into a simple filter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.559</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>union/except/intersect</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.081</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>SPARK-5383 alias for udfs with multi output columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.201</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>SPARK-5367: resolve star expression in udf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.093</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>Exactly once semantics for DDL and command statements</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.284</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>DESCRIBE commands</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.305</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>SPARK-2263: Insert Map&lt;K, V&gt; values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.103</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>ADD JAR command</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.403</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>ADD JAR command 2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>CREATE TEMPORARY FUNCTION</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.189</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>ADD FILE command</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.202</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>dynamic_partition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>Dynamic partition folder layout</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.854</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>URISyntaxException when dynamic partitioning</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.081</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>Partition spec validation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>SPARK-3414 regression: should store analyzed logical plan when creating a temporary view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.582</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>SPARK-3810: PreprocessTableInsertion static partitioning support</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.449</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>SPARK-3810: PreprocessTableInsertion dynamic partitioning support</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>parse HQL set commands</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.275</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>current_database with multiple sessions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.083</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>use database</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.051</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>lookup hive UDF in another thread</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.687</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>select from thrift based table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>role management commands are not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>import/export commands are not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>some show commands are not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>lock/unlock table and database commands are not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>create/drop/alter index commands are not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>create/drop macro commands are not supported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.688</duration>
          <className>org.apache.spark.sql.hive.execution.HiveQuerySuite</className>
          <testName>mode is nonstrict</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.execution.HiveResolutionSuite.xml</file>
      <name>org.apache.spark.sql.hive.execution.HiveResolutionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>4.217</duration>
      <cases>
        <case>
          <duration>0.072</duration>
          <className>org.apache.spark.sql.hive.execution.HiveResolutionSuite</className>
          <testName>SPARK-3698: case insensitive test for nested data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.sql.hive.execution.HiveResolutionSuite</className>
          <testName>SPARK-5278: check ambiguous reference to fields</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.371</duration>
          <className>org.apache.spark.sql.hive.execution.HiveResolutionSuite</className>
          <testName>attr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.341</duration>
          <className>org.apache.spark.sql.hive.execution.HiveResolutionSuite</className>
          <testName>table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.317</duration>
          <className>org.apache.spark.sql.hive.execution.HiveResolutionSuite</className>
          <testName>attr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.272</duration>
          <className>org.apache.spark.sql.hive.execution.HiveResolutionSuite</className>
          <testName>attr case insensitive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.282</duration>
          <className>org.apache.spark.sql.hive.execution.HiveResolutionSuite</className>
          <testName>attr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.285</duration>
          <className>org.apache.spark.sql.hive.execution.HiveResolutionSuite</className>
          <testName>attr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.318</duration>
          <className>org.apache.spark.sql.hive.execution.HiveResolutionSuite</className>
          <testName>attr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.298</duration>
          <className>org.apache.spark.sql.hive.execution.HiveResolutionSuite</className>
          <testName>attr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.305</duration>
          <className>org.apache.spark.sql.hive.execution.HiveResolutionSuite</className>
          <testName>star</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.265</duration>
          <className>org.apache.spark.sql.hive.execution.HiveResolutionSuite</className>
          <testName>case insensitivity with scala reflection</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.HiveResolutionSuite</className>
          <testName>case insensitivity with scala reflection joins</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.192</duration>
          <className>org.apache.spark.sql.hive.execution.HiveResolutionSuite</className>
          <testName>nested repeated resolution</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.875</duration>
          <className>org.apache.spark.sql.hive.execution.HiveResolutionSuite</className>
          <testName>test ambiguousReferences resolved as hive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.execution.HiveSerDeSuite.xml</file>
      <name>org.apache.spark.sql.hive.execution.HiveSerDeSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.162</duration>
      <cases>
        <case>
          <duration>0.067</duration>
          <className>org.apache.spark.sql.hive.execution.HiveSerDeSuite</className>
          <testName>Read with RegexSerDe</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.376</duration>
          <className>org.apache.spark.sql.hive.execution.HiveSerDeSuite</className>
          <testName>Read and write with LazySimpleSerDe (tab separated)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.235</duration>
          <className>org.apache.spark.sql.hive.execution.HiveSerDeSuite</className>
          <testName>Read with AvroSerDe</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.484</duration>
          <className>org.apache.spark.sql.hive.execution.HiveSerDeSuite</className>
          <testName>Read Partitioned with AvroSerDe</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.execution.HiveTableScanSuite.xml</file>
      <name>org.apache.spark.sql.hive.execution.HiveTableScanSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>3.03</duration>
      <cases>
        <case>
          <duration>0.628</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTableScanSuite</className>
          <testName>partition_based_table_scan_with_different_serde</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.353</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTableScanSuite</className>
          <testName>file_split_for_small_table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.305</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTableScanSuite</className>
          <testName>Spark-4041: lowercase issue</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.17</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTableScanSuite</className>
          <testName>Spark-4077: timestamp query for null value</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.349</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTableScanSuite</className>
          <testName>Spark-4959 Attributes are case sensitive when using a select query from a projection</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.689</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTableScanSuite</className>
          <testName>Verify SQLConf HIVE_METASTORE_PARTITION_PRUNING</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.536</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTableScanSuite</className>
          <testName>SPARK-16926: number of table and partition columns match for new partitioned table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite.xml</file>
      <name>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>18.619999</duration>
      <cases>
        <case>
          <duration>0.745</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>1 + 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.702</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>0</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.531</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>1 + 1L</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.46</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>1 + 1S</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.495</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>1 + 1Y</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.489</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>1 + &apos;1&apos;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.469</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>0 + 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.445</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>0</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.435</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>0 + 1L</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.476</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>0 + 1S</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.445</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>0 + 1Y</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.448</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>0 + &apos;1&apos;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.472</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>1L + 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.415</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>0</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.419</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>1L + 1L</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.388</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>1L + 1S</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.382</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>1L + 1Y</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.367</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>1L + &apos;1&apos;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.377</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>1S + 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.391</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>0</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.389</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>1S + 1L</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.416</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>1S + 1S</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.44</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>1S + 1Y</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.44</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>1S + &apos;1&apos;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.385</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>1Y + 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.362</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>0</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.359</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>1Y + 1L</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.344</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>1Y + 1S</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.351</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>1Y + 1Y</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.364</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>1Y + &apos;1&apos;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.335</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>&apos;1&apos; + 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.33</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>0</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.366</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>&apos;1&apos; + 1L</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.331</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>&apos;1&apos; + 1S</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.326</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>&apos;1&apos; + 1Y</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.336</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>&apos;1&apos; + &apos;1&apos;</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.354</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>case when then 1 else null end</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.343</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>case when then null else 1 end</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.332</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>0 else null end</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.334</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>0 end</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.342</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>case when then 1L else null end</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.365</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>case when then null else 1L end</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.319</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>case when then 1S else null end</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.321</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>case when then null else 1S end</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.324</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>case when then 1Y else null end</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.325</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>case when then null else 1Y end</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.sql.hive.execution.HiveTypeCoercionSuite</className>
          <testName>[SPARK-2210] boolean cast on boolean value should be removed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.execution.HiveUDFSuite.xml</file>
      <name>org.apache.spark.sql.hive.execution.HiveUDFSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>13.594</duration>
      <cases>
        <case>
          <duration>0.357</duration>
          <className>org.apache.spark.sql.hive.execution.HiveUDFSuite</className>
          <testName>spark sql udf test that returns a struct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.201</duration>
          <className>org.apache.spark.sql.hive.execution.HiveUDFSuite</className>
          <testName>SPARK-4785 When called with arguments referring column fields, PMOD throws NPE</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.336</duration>
          <className>org.apache.spark.sql.hive.execution.HiveUDFSuite</className>
          <testName>hive struct udf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.166</duration>
          <className>org.apache.spark.sql.hive.execution.HiveUDFSuite</className>
          <testName>Max/Min on named_struct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.738</duration>
          <className>org.apache.spark.sql.hive.execution.HiveUDFSuite</className>
          <testName>SPARK-6409 UDAF Average test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.993</duration>
          <className>org.apache.spark.sql.hive.execution.HiveUDFSuite</className>
          <testName>SPARK-2693 udaf aggregates test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.069</duration>
          <className>org.apache.spark.sql.hive.execution.HiveUDFSuite</className>
          <testName>SPARK-16228 Percentile needs explicit cast to double</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>5.64</duration>
          <className>org.apache.spark.sql.hive.execution.HiveUDFSuite</className>
          <testName>Generic UDAF aggregates</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.304</duration>
          <className>org.apache.spark.sql.hive.execution.HiveUDFSuite</className>
          <testName>UDFIntegerToString</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.131</duration>
          <className>org.apache.spark.sql.hive.execution.HiveUDFSuite</className>
          <testName>UDFToListString</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.116</duration>
          <className>org.apache.spark.sql.hive.execution.HiveUDFSuite</className>
          <testName>UDFToListInt</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.145</duration>
          <className>org.apache.spark.sql.hive.execution.HiveUDFSuite</className>
          <testName>UDFToStringIntMap</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.126</duration>
          <className>org.apache.spark.sql.hive.execution.HiveUDFSuite</className>
          <testName>UDFToIntIntMap</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.373</duration>
          <className>org.apache.spark.sql.hive.execution.HiveUDFSuite</className>
          <testName>UDFListListInt</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.225</duration>
          <className>org.apache.spark.sql.hive.execution.HiveUDFSuite</className>
          <testName>UDFListString</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.259</duration>
          <className>org.apache.spark.sql.hive.execution.HiveUDFSuite</className>
          <testName>UDFStringString</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.315</duration>
          <className>org.apache.spark.sql.hive.execution.HiveUDFSuite</className>
          <testName>UDFTwoListList</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.106</duration>
          <className>org.apache.spark.sql.hive.execution.HiveUDFSuite</className>
          <testName>Hive UDFs with insufficient number of input arguments should trigger an analysis error</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.311</duration>
          <className>org.apache.spark.sql.hive.execution.HiveUDFSuite</className>
          <testName>Hive UDF in group by</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.683</duration>
          <className>org.apache.spark.sql.hive.execution.HiveUDFSuite</className>
          <testName>SPARK-11522 select input_file_name from non-parquet table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.execution.HiveWindowFunctionQueryFileSuite.xml</file>
      <name>org.apache.spark.sql.hive.execution.HiveWindowFunctionQueryFileSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.29</duration>
      <cases>
        <case>
          <duration>0.29</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQueryFileSuite</className>
          <testName>windowing_udaf2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite.xml</file>
      <name>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>32.995007</duration>
      <cases>
        <case>
          <duration>0.812</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName>q (deterministic) 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.381</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName>q (deterministic) 3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.451</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName>q (deterministic) 4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.299</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName>q (deterministic) 5</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.437</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName>q (deterministic) 6</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.667</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName>q (deterministic)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.565</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName>q (deterministic)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.076</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName>q (deterministic)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>5.572</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName>q (deterministic)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.55</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName>q (deterministic) 1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.888</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName>q (deterministic) 2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.584</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName>q (deterministic) 3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.673</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName>q (deterministic) 4</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.314</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName> testWindowing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.564</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName> testGroupByWithPartitioning</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.455</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName> testGroupByHavingWithSWQ</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.251</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName> testCount</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.313</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName> testCountWithWindowingUDAF</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.315</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName> testCountInSubQ</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.275</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName> testMixedCaseAlias</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.278</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName> testHavingWithWindowingNoGBY</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.221</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName> testHavingWithWindowingCondRankNoGBY</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.3</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName> testFirstLast</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.297</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName> testFirstLastWithWhere</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.256</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName> testSumWindow</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.283</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName> testNoSortClause</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.269</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName> testMultipleWindows</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.332</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName> testCountStar</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.283</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName> testUDAFs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.482</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName> testUDAFsWithGBY</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.503</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName> testDISTs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.259</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName> testLateralViews</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.504</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName> testGroupByHavingWithSWQAndAlias</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.238</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName> testMultipleRangeWindows</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.224</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName> testPartOrderInUDAFInvoke</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.233</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName> testPartOrderInWdwDef</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.219</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName> testDefaultPartitioningSpecRules</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.199</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName> testRankWithPartitioning</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.453</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName> testPartitioningVariousForms</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.416</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName> testPartitioningVariousForms2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.274</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName> testUDFOnOrderCols</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.234</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName> testNoBetweenForRows</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.201</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName> testNoBetweenForRange</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.199</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName> testUnboundedFollowingForRows</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.221</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName> testUnboundedFollowingForRange</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.175</duration>
          <className>org.apache.spark.sql.hive.execution.HiveWindowFunctionQuerySuite</className>
          <testName> testOverNoPartitionSingleAggregate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite.xml</file>
      <name>org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.09</duration>
      <cases>
        <case>
          <duration>0.09</duration>
          <className>org.apache.spark.sql.hive.execution.PruneFileSourcePartitionsSuite</className>
          <testName>PruneFileSourcePartitions should not change the output of LogicalRelation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.execution.PruningSuite.xml</file>
      <name>org.apache.spark.sql.hive.execution.PruningSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>10.149</duration>
      <cases>
        <case>
          <duration>0.469</duration>
          <className>org.apache.spark.sql.hive.execution.PruningSuite</className>
          <testName>Column pruning - with partitioned table - pruning test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.723</duration>
          <className>org.apache.spark.sql.hive.execution.PruningSuite</className>
          <testName>Column pruning - with partitioned table - query test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.sql.hive.execution.PruningSuite</className>
          <testName>Column pruning - with non-partitioned table - pruning test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.352</duration>
          <className>org.apache.spark.sql.hive.execution.PruningSuite</className>
          <testName>Column pruning - with non-partitioned table - query test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.sql.hive.execution.PruningSuite</className>
          <testName>Column pruning - with multiple projects - pruning test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.242</duration>
          <className>org.apache.spark.sql.hive.execution.PruningSuite</className>
          <testName>Column pruning - with multiple projects - query test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.sql.hive.execution.PruningSuite</className>
          <testName>Column pruning - projects alias substituting - pruning test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.253</duration>
          <className>org.apache.spark.sql.hive.execution.PruningSuite</className>
          <testName>Column pruning - projects alias substituting - query test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.sql.hive.execution.PruningSuite</className>
          <testName>Column pruning - filter alias in-lining - pruning test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.296</duration>
          <className>org.apache.spark.sql.hive.execution.PruningSuite</className>
          <testName>Column pruning - filter alias in-lining - query test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.sql.hive.execution.PruningSuite</className>
          <testName>Column pruning - without filters - pruning test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.238</duration>
          <className>org.apache.spark.sql.hive.execution.PruningSuite</className>
          <testName>Column pruning - without filters - query test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.sql.hive.execution.PruningSuite</className>
          <testName>Column pruning - simple top project without aliases - pruning test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.255</duration>
          <className>org.apache.spark.sql.hive.execution.PruningSuite</className>
          <testName>Column pruning - simple top project without aliases - query test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.sql.hive.execution.PruningSuite</className>
          <testName>Column pruning - non-trivial top project with aliases - pruning test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.265</duration>
          <className>org.apache.spark.sql.hive.execution.PruningSuite</className>
          <testName>Column pruning - non-trivial top project with aliases - query test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.sql.hive.execution.PruningSuite</className>
          <testName>Partition pruning - non-partitioned, non-trivial project - pruning test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.295</duration>
          <className>org.apache.spark.sql.hive.execution.PruningSuite</className>
          <testName>Partition pruning - non-partitioned, non-trivial project - query test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.sql.hive.execution.PruningSuite</className>
          <testName>Partition pruning - non-partitioned table - pruning test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.254</duration>
          <className>org.apache.spark.sql.hive.execution.PruningSuite</className>
          <testName>Partition pruning - non-partitioned table - query test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.423</duration>
          <className>org.apache.spark.sql.hive.execution.PruningSuite</className>
          <testName>Partition pruning - with filter on string partition key - pruning test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.156</duration>
          <className>org.apache.spark.sql.hive.execution.PruningSuite</className>
          <testName>Partition pruning - with filter on string partition key - query test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.05</duration>
          <className>org.apache.spark.sql.hive.execution.PruningSuite</className>
          <testName>Partition pruning - with filter on int partition key - pruning test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.117</duration>
          <className>org.apache.spark.sql.hive.execution.PruningSuite</className>
          <testName>Partition pruning - with filter on int partition key - query test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.054</duration>
          <className>org.apache.spark.sql.hive.execution.PruningSuite</className>
          <testName>Partition pruning - left only 1 partition - pruning test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.166</duration>
          <className>org.apache.spark.sql.hive.execution.PruningSuite</className>
          <testName>Partition pruning - left only 1 partition - query test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.049</duration>
          <className>org.apache.spark.sql.hive.execution.PruningSuite</className>
          <testName>Partition pruning - all partitions pruned - pruning test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.085</duration>
          <className>org.apache.spark.sql.hive.execution.PruningSuite</className>
          <testName>Partition pruning - all partitions pruned - query test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.055</duration>
          <className>org.apache.spark.sql.hive.execution.PruningSuite</className>
          <testName>Partition pruning - pruning with both column key and partition key - pruning test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.136</duration>
          <className>org.apache.spark.sql.hive.execution.PruningSuite</className>
          <testName>Partition pruning - pruning with both column key and partition key - query test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.execution.SQLQuerySuite.xml</file>
      <name>org.apache.spark.sql.hive.execution.SQLQuerySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>71.600006</duration>
      <cases>
        <case>
          <duration>0.081</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>query global temp view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>non-existent global temp view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.22</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>script</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.528</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>UDTF</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.352</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>permanent UDTF</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.166</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-6835: udtf in lateral view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.108</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-13651: generator outputs shouldn&apos;t be resolved from its child&apos;s output</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.322</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-6851: Self-joined converted parquet tables</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.325</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>show functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.044</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>describe functions - built-in functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.16</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>describe functions - user defined functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.133</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>describe functions - temporary user defined functions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.388</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>describe partition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.874</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>describe partition - error handling</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.258</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-5371: union with null and sum</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.268</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>CTAS with WITH clause</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.29</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>explode nested Field</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.661</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-4512 Fix attribute reference resolution error when using SORT BY</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.221</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>CTAS without serde without location</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.376</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>CTAS with default fileformat</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.17</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>CTAS without serde with location</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.964</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>CTAS with serde</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.393</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>specifying the column list for CTAS</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.399</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>command substitution</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.54</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>ordering not in select</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.813</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>ordering not in agg</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.511</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>double nested data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.648</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>test CTAS</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.024</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-4825 save join to table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.172</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-3708 Backticks aren&apos;t handled correctly is aliases</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.137</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-3834 Backticks not correctly handled in subquery aliases</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.139</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-3814 Support Bitwise &amp; operator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.139</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-3814 Support Bitwise | operator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.142</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-3814 Support Bitwise ^ operator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.153</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-3814 Support Bitwise ~ operator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.533</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-4154 Query does not work if it has &apos;not between&apos; in Spark SQL and HQL</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.787</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-2554 SumDistinct partial aggregation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.059</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-4963 DataFrame sample on mutable row return wrong result</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.576</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-4699 SparkSession with Hive Support should be case insensitive by default</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.467</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-5284 Insert into Hive throws NPE when a inner complex type field has a null value</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.498</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-4296 Grouping field with Hive UDF as sub expression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>resolve udtf in projection #1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.211</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>resolve udtf in projection #2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.106</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>TGF with non-TGF in projection</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.3</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>Project should not be resolved if it contains aggregates or generators</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>6.642</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>sanity test for SPARK-6618</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.081</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-5203 union with different decimal precision</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.144</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>Star Expansion - script transform</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.927</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>test script transform for stdout</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.283</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>test script transform for stderr</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.239</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>test script transform data type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.089</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>Sorting columns are not in Generate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.192</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>rank window function - push filter into window</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.366</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>rank window function - push filter into window - correctness</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.388</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>rank window function - push filter into window - with duplicate ranks - correctness</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.06</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>test case key when</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.038</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-7269 Check analysis failed in case in-sensitive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.049</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>Cast STRING to BIGINT</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.95</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>dynamic partition value test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>Call add jar in a different thread (SPARK-8306)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.156</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-6785: HiveQuerySuite - Date comparison test 2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.172</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-6785: HiveQuerySuite - Date cast</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.082</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>inConversion fires too early</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.115</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-9371: fix the support for special chars in column names for hive context</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.351</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>Convert hive interval term into Literal of CalendarIntervalType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.256</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>specifying database name for a temporary table is not allowed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.084</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-10593 same column names in lateral view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-10310: script transformation using default input/output SerDe and record reader/writer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-10310: script transformation using LazySimpleSerDe</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.479</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-10741: Sort on Aggregate using parquet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.527</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>run sql directly on files - parquet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.393</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>run sql directly on files - orc</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.642</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>run sql directly on files - csv</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.472</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>run sql directly on files - json</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.489</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-8976 Wrong Result for Rollup #1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.558</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-8976 Wrong Result for Rollup #2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.53</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-8976 Wrong Result for Rollup #3</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.434</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-8976 Wrong Result for CUBE #1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.619</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-8976 Wrong Result for CUBE #2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.571</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-8976 Wrong Result for GroupingSet</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-10562: partition by column with mixed case name</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.76</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-11453: append data to partitioned table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.27</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-11590: use native json_tuple in lateral view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.674</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>multi-insert with lateral view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.631</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>q</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.548</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>q</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.717</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>select partitioned table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-14981: DESC not supported for sorting columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.248</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>insert into datasource table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.501</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>spark-15557 promote string test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>7.456</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-15752 optimize metadata only query for hive table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.564</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-17354: Partitioning by dates/timestamps works with Parquet vectorized reader</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.102</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-17108: Fix BIGINT and INT comparison failure in spark sql</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.304</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>SPARK-17796 Support wildcard character in filename for LOAD DATA LOCAL INPATH</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.731</duration>
          <className>org.apache.spark.sql.hive.execution.SQLQuerySuite</className>
          <testName>Insert overwrite with partition</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.execution.SQLViewSuite.xml</file>
      <name>org.apache.spark.sql.hive.execution.SQLViewSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>11.233001</duration>
      <cases>
        <case>
          <duration>0.309</duration>
          <className>org.apache.spark.sql.hive.execution.SQLViewSuite</className>
          <testName>create a permanent view on a permanent view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.249</duration>
          <className>org.apache.spark.sql.hive.execution.SQLViewSuite</className>
          <testName>create a temp view on a permanent view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.123</duration>
          <className>org.apache.spark.sql.hive.execution.SQLViewSuite</className>
          <testName>create a temp view on a temp view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.sql.hive.execution.SQLViewSuite</className>
          <testName>create a permanent view on a temp view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.105</duration>
          <className>org.apache.spark.sql.hive.execution.SQLViewSuite</className>
          <testName>error handling: existing a table with the duplicate name when creating/altering a view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.172</duration>
          <className>org.apache.spark.sql.hive.execution.SQLViewSuite</className>
          <testName>existing a table with the duplicate name when CREATE VIEW IF NOT EXISTS</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.065</duration>
          <className>org.apache.spark.sql.hive.execution.SQLViewSuite</className>
          <testName>Issue exceptions for ALTER VIEW on the temporary view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.sql.hive.execution.SQLViewSuite</className>
          <testName>Issue exceptions for ALTER TABLE on the temporary view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.sql.hive.execution.SQLViewSuite</className>
          <testName>Issue exceptions for other table DDL on the temporary view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.095</duration>
          <className>org.apache.spark.sql.hive.execution.SQLViewSuite</className>
          <testName>error handling: insert/load/truncate table commands against a view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.sql.hive.execution.SQLViewSuite</className>
          <testName>error handling: fail if the view sql itself is invalid</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.hive.execution.SQLViewSuite</className>
          <testName>error handling: fail if the temp view name contains the database prefix</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.hive.execution.SQLViewSuite</className>
          <testName>error handling: disallow IF NOT EXISTS for CREATE TEMPORARY VIEW</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.hive.execution.SQLViewSuite</className>
          <testName>error handling: fail if the temp view sql itself is invalid</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.503</duration>
          <className>org.apache.spark.sql.hive.execution.SQLViewSuite</className>
          <testName>correctly parse CREATE VIEW statement</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.325</duration>
          <className>org.apache.spark.sql.hive.execution.SQLViewSuite</className>
          <testName>correctly parse CREATE TEMPORARY VIEW statement</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.sql.hive.execution.SQLViewSuite</className>
          <testName>should NOT allow CREATE TEMPORARY VIEW when TEMPORARY VIEW with same name exists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.077</duration>
          <className>org.apache.spark.sql.hive.execution.SQLViewSuite</className>
          <testName>should allow CREATE TEMPORARY VIEW when a permanent VIEW with same name exists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.081</duration>
          <className>org.apache.spark.sql.hive.execution.SQLViewSuite</className>
          <testName>should allow CREATE permanent VIEW when a TEMPORARY VIEW with same name exists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.693</duration>
          <className>org.apache.spark.sql.hive.execution.SQLViewSuite</className>
          <testName>correctly handle CREATE VIEW IF NOT EXISTS</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.692</duration>
          <className>org.apache.spark.sql.hive.execution.SQLViewSuite</className>
          <testName>correctly handle CREATE OR REPLACE TEMPORARY VIEW</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.017</duration>
          <className>org.apache.spark.sql.hive.execution.SQLViewSuite</className>
          <testName>correctly handle CREATE OR REPLACE VIEW</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.658</duration>
          <className>org.apache.spark.sql.hive.execution.SQLViewSuite</className>
          <testName>correctly handle ALTER VIEW</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.hive.execution.SQLViewSuite</className>
          <testName>should not allow ALTER VIEW AS when the view does not exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.175</duration>
          <className>org.apache.spark.sql.hive.execution.SQLViewSuite</className>
          <testName>ALTER VIEW AS should try to alter temp view first if view name has no database part</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.166</duration>
          <className>org.apache.spark.sql.hive.execution.SQLViewSuite</className>
          <testName>ALTER VIEW AS should alter permanent view if view name has database part</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.152</duration>
          <className>org.apache.spark.sql.hive.execution.SQLViewSuite</className>
          <testName>ALTER VIEW AS should keep the previous table properties, comment, create_time, etc</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.425</duration>
          <className>org.apache.spark.sql.hive.execution.SQLViewSuite</className>
          <testName>create hive view for json table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.643</duration>
          <className>org.apache.spark.sql.hive.execution.SQLViewSuite</className>
          <testName>create hive view for partitioned parquet table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.124</duration>
          <className>org.apache.spark.sql.hive.execution.SQLViewSuite</className>
          <testName>CTE within view</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.571</duration>
          <className>org.apache.spark.sql.hive.execution.SQLViewSuite</className>
          <testName>Using view after switching current database</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.536</duration>
          <className>org.apache.spark.sql.hive.execution.SQLViewSuite</className>
          <testName>Using view after adding more columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.507</duration>
          <className>org.apache.spark.sql.hive.execution.SQLViewSuite</className>
          <testName>create hive view for joined tables</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.347</duration>
          <className>org.apache.spark.sql.hive.execution.SQLViewSuite</className>
          <testName>SPARK-14933 - create view from hive parquet table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.402</duration>
          <className>org.apache.spark.sql.hive.execution.SQLViewSuite</className>
          <testName>SPARK-14933 - create view from hive orc table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.594</duration>
          <className>org.apache.spark.sql.hive.execution.SQLViewSuite</className>
          <testName>create a permanent/temp view using a hive, built-in, and permanent user function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.299</duration>
          <className>org.apache.spark.sql.hive.execution.SQLViewSuite</className>
          <testName>create a permanent/temp view using a temporary function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.execution.ScriptTransformationSuite.xml</file>
      <name>org.apache.spark.sql.hive.execution.ScriptTransformationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.4369998</duration>
      <cases>
        <case>
          <duration>0.095</duration>
          <className>org.apache.spark.sql.hive.execution.ScriptTransformationSuite</className>
          <testName>cat without SerDe</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.076</duration>
          <className>org.apache.spark.sql.hive.execution.ScriptTransformationSuite</className>
          <testName>cat with LazySimpleSerDe</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.081</duration>
          <className>org.apache.spark.sql.hive.execution.ScriptTransformationSuite</className>
          <testName>script transformation should not swallow errors from upstream operators (no serde)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.073</duration>
          <className>org.apache.spark.sql.hive.execution.ScriptTransformationSuite</className>
          <testName>script transformation should not swallow errors from upstream operators (with serde)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.112</duration>
          <className>org.apache.spark.sql.hive.execution.ScriptTransformationSuite</className>
          <testName>SPARK-14400 script transformation should fail for bad script command</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.execution.WindowQuerySuite.xml</file>
      <name>org.apache.spark.sql.hive.execution.WindowQuerySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>3.531</duration>
      <cases>
        <case>
          <duration>0.992</duration>
          <className>org.apache.spark.sql.hive.execution.WindowQuerySuite</className>
          <testName> testExpressions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.514</duration>
          <className>org.apache.spark.sql.hive.execution.WindowQuerySuite</className>
          <testName> testSTATs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.482</duration>
          <className>org.apache.spark.sql.hive.execution.WindowQuerySuite</className>
          <testName>null arguments</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.302</duration>
          <className>org.apache.spark.sql.hive.execution.WindowQuerySuite</className>
          <testName>SPARK-16646: LAST_VALUE(FALSE) OVER ()</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.241</duration>
          <className>org.apache.spark.sql.hive.execution.WindowQuerySuite</className>
          <testName>SPARK-16646: FIRST_VALUE(FALSE) OVER ()</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.orc.OrcFilterSuite.xml</file>
      <name>org.apache.spark.sql.hive.orc.OrcFilterSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.936</duration>
      <cases>
        <case>
          <duration>0.203</duration>
          <className>org.apache.spark.sql.hive.orc.OrcFilterSuite</className>
          <testName>filter pushdown - integer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.169</duration>
          <className>org.apache.spark.sql.hive.orc.OrcFilterSuite</className>
          <testName>filter pushdown - long</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.163</duration>
          <className>org.apache.spark.sql.hive.orc.OrcFilterSuite</className>
          <testName>filter pushdown - float</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.151</duration>
          <className>org.apache.spark.sql.hive.orc.OrcFilterSuite</className>
          <testName>filter pushdown - double</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.156</duration>
          <className>org.apache.spark.sql.hive.orc.OrcFilterSuite</className>
          <testName>filter pushdown - string</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.154</duration>
          <className>org.apache.spark.sql.hive.orc.OrcFilterSuite</className>
          <testName>filter pushdown - boolean</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.157</duration>
          <className>org.apache.spark.sql.hive.orc.OrcFilterSuite</className>
          <testName>filter pushdown - decimal</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.197</duration>
          <className>org.apache.spark.sql.hive.orc.OrcFilterSuite</className>
          <testName>filter pushdown - timestamp</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.114</duration>
          <className>org.apache.spark.sql.hive.orc.OrcFilterSuite</className>
          <testName>filter pushdown - combinations with logical operators</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.472</duration>
          <className>org.apache.spark.sql.hive.orc.OrcFilterSuite</className>
          <testName>no filter pushdown - non-supported types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite.xml</file>
      <name>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>46.519005</duration>
      <cases>
        <case>
          <duration>0.772</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.712</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.669</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.681</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.756</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.697</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.746</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.705</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.706</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.625</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.685</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.178</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.713</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.712</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.654</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.713</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.736</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.697</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.748</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.662</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.663</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.59</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.707</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.659</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.674</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.632</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.732</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.71</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.172</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.098</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.844</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.817</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.338</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>save()/load() - non-partitioned table - Overwrite</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.544</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>save()/load() - non-partitioned table - Append</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>save()/load() - non-partitioned table - ErrorIfExists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>save()/load() - non-partitioned table - Ignore</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.235</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>save()/load() - partitioned table - simple queries</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.471</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>save()/load() - partitioned table - Overwrite</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.455</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>save()/load() - partitioned table - Append</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.336</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>save()/load() - partitioned table - Append - new partition values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>save()/load() - partitioned table - ErrorIfExists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>save()/load() - partitioned table - Ignore</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.204</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - non-partitioned table - Overwrite</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.431</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - non-partitioned table - Append</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.072</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - non-partitioned table - ErrorIfExists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.12</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - non-partitioned table - Ignore</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.034</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - partitioned table - simple queries</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.848</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - partitioned table - boolean type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.664</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - partitioned table - Overwrite</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.006</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - partitioned table - Append</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.859</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - partitioned table - Append - new partition values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.302</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - partitioned table - Append - mismatched partition columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.089</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - partitioned table - ErrorIfExists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.033</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - partitioned table - Ignore</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.306</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>load() - with directory of unpartitioned data in nested subdirs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.723</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>Hadoop style globbing - unpartitioned data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.193</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>Hadoop style globbing - partitioned data with schema inference</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.916</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>SPARK-9735 Partition column type casting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.733</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>SPARK-7616: adjust column name order accordingly when saving partitioned table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>5.725</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>SPARK-8406: Avoids name collision while writing files</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.47</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>SPARK-8578 specified custom output committer will not be used to append data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.157</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>SPARK-8887: Explicitly define which data types can be used as dynamic partition columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.294</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>Locality support for FileScanRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.275</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>save()/load() - partitioned table - simple queries - partition columns in data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.467</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>SPARK-12218: &apos;Not&apos; is included in ORC filter pushdown</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.228</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>SPARK-13543: Support for specifying compression codec for ORC via option()</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.102</duration>
          <className>org.apache.spark.sql.hive.orc.OrcHadoopFsRelationSuite</className>
          <testName>Default compression codec is snappy for ORC compression</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.orc.OrcPartitionDiscoverySuite.xml</file>
      <name>org.apache.spark.sql.hive.orc.OrcPartitionDiscoverySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.882</duration>
      <cases>
        <case>
          <duration>0.816</duration>
          <className>org.apache.spark.sql.hive.orc.OrcPartitionDiscoverySuite</className>
          <testName>read partitioned table - normal case</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.846</duration>
          <className>org.apache.spark.sql.hive.orc.OrcPartitionDiscoverySuite</className>
          <testName>read partitioned table - partition key included in orc file</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.637</duration>
          <className>org.apache.spark.sql.hive.orc.OrcPartitionDiscoverySuite</className>
          <testName>read partitioned table - with nulls</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.583</duration>
          <className>org.apache.spark.sql.hive.orc.OrcPartitionDiscoverySuite</className>
          <testName>read partitioned table - with nulls and partition keys are included in Orc file</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.orc.OrcQuerySuite.xml</file>
      <name>org.apache.spark.sql.hive.orc.OrcQuerySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>13.493001</duration>
      <cases>
        <case>
          <duration>0.405</duration>
          <className>org.apache.spark.sql.hive.orc.OrcQuerySuite</className>
          <testName>Read/write All Types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.171</duration>
          <className>org.apache.spark.sql.hive.orc.OrcQuerySuite</className>
          <testName>Read/write binary data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.932</duration>
          <className>org.apache.spark.sql.hive.orc.OrcQuerySuite</className>
          <testName>Read/write all types with non-primitive type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.194</duration>
          <className>org.apache.spark.sql.hive.orc.OrcQuerySuite</className>
          <testName>Read/write UserDefinedType</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.097</duration>
          <className>org.apache.spark.sql.hive.orc.OrcQuerySuite</className>
          <testName>Creating case class RDD table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.633</duration>
          <className>org.apache.spark.sql.hive.orc.OrcQuerySuite</className>
          <testName>Simple selection form ORC table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.223</duration>
          <className>org.apache.spark.sql.hive.orc.OrcQuerySuite</className>
          <testName>save and load case class RDD with `None`s as orc</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.189</duration>
          <className>org.apache.spark.sql.hive.orc.OrcQuerySuite</className>
          <testName>compress option when compression is unset</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.267</duration>
          <className>org.apache.spark.sql.hive.orc.OrcQuerySuite</className>
          <testName>Compression options for writing to an ORC file (SNAPPY, ZLIB and NONE)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.hive.orc.OrcQuerySuite</className>
          <testName>1</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.308</duration>
          <className>org.apache.spark.sql.hive.orc.OrcQuerySuite</className>
          <testName>simple select queries</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.231</duration>
          <className>org.apache.spark.sql.hive.orc.OrcQuerySuite</className>
          <testName>appending</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.256</duration>
          <className>org.apache.spark.sql.hive.orc.OrcQuerySuite</className>
          <testName>overwriting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.313</duration>
          <className>org.apache.spark.sql.hive.orc.OrcQuerySuite</className>
          <testName>self-join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.276</duration>
          <className>org.apache.spark.sql.hive.orc.OrcQuerySuite</className>
          <testName>nested data - struct with array field</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.246</duration>
          <className>org.apache.spark.sql.hive.orc.OrcQuerySuite</className>
          <testName>nested data - array of struct</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.186</duration>
          <className>org.apache.spark.sql.hive.orc.OrcQuerySuite</className>
          <testName>columns only referenced by pushed down filters should remain</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.873</duration>
          <className>org.apache.spark.sql.hive.orc.OrcQuerySuite</className>
          <testName>SPARK-5309 strings stored using dictionary compression in orc</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.436</duration>
          <className>org.apache.spark.sql.hive.orc.OrcQuerySuite</className>
          <testName>SPARK-9170: Don&apos;t implicitly lowercase of user-provided columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.433</duration>
          <className>org.apache.spark.sql.hive.orc.OrcQuerySuite</className>
          <testName>SPARK-8501: Avoids discovery schema from empty ORC files</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.94</duration>
          <className>org.apache.spark.sql.hive.orc.OrcQuerySuite</className>
          <testName>SPARK-10623 Enable ORC PPD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.596</duration>
          <className>org.apache.spark.sql.hive.orc.OrcQuerySuite</className>
          <testName>Verify the ORC conversion parameter: CONVERT_METASTORE_ORC</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.622</duration>
          <className>org.apache.spark.sql.hive.orc.OrcQuerySuite</className>
          <testName>converted ORC table supports resolving mixed case field</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.262</duration>
          <className>org.apache.spark.sql.hive.orc.OrcQuerySuite</className>
          <testName>SPARK-14962 Produce correct results on array type with isnotnull</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.208</duration>
          <className>org.apache.spark.sql.hive.orc.OrcQuerySuite</className>
          <testName>SPARK-15198 Support for pushing down filters for boolean types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.383</duration>
          <className>org.apache.spark.sql.hive.orc.OrcQuerySuite</className>
          <testName>Support for pushing down filters for decimal types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.395</duration>
          <className>org.apache.spark.sql.hive.orc.OrcQuerySuite</className>
          <testName>Support for pushing down filters for timestamp types</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.26</duration>
          <className>org.apache.spark.sql.hive.orc.OrcQuerySuite</className>
          <testName>column nullability and comment - write and then read</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.159</duration>
          <className>org.apache.spark.sql.hive.orc.OrcQuerySuite</className>
          <testName>Empty schema does not read data from ORC file</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.hive.orc.OrcSourceSuite.xml</file>
      <name>org.apache.spark.sql.hive.orc.OrcSourceSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.049</duration>
      <cases>
        <case>
          <duration>0.745</duration>
          <className>org.apache.spark.sql.hive.orc.OrcSourceSuite</className>
          <testName>create temporary orc table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.65</duration>
          <className>org.apache.spark.sql.hive.orc.OrcSourceSuite</className>
          <testName>create temporary orc table as</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.202</duration>
          <className>org.apache.spark.sql.hive.orc.OrcSourceSuite</className>
          <testName>appending insert</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.147</duration>
          <className>org.apache.spark.sql.hive.orc.OrcSourceSuite</className>
          <testName>overwrite insert</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.301</duration>
          <className>org.apache.spark.sql.hive.orc.OrcSourceSuite</className>
          <testName>write null values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.sql.hive.orc.OrcSourceSuite</className>
          <testName>SPARK-18433: Improve DataSource option keys to be more case-insensitive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.hive.orc.OrcSourceSuite</className>
          <testName>SPARK-12218 Converting conjunctions into ORC SearchArguments</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.sources.BucketedReadSuite.xml</file>
      <name>org.apache.spark.sql.sources.BucketedReadSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>96.821</duration>
      <cases>
        <case>
          <duration>1.7</duration>
          <className>org.apache.spark.sql.sources.BucketedReadSuite</className>
          <testName>read bucketed data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>32.639</duration>
          <className>org.apache.spark.sql.sources.BucketedReadSuite</className>
          <testName>read partitioning bucketed tables with bucket pruning filters</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>9.122</duration>
          <className>org.apache.spark.sql.sources.BucketedReadSuite</className>
          <testName>read non-partitioning bucketed tables with bucket pruning filters</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.453</duration>
          <className>org.apache.spark.sql.sources.BucketedReadSuite</className>
          <testName>read partitioning bucketed tables having null in bucketing key</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>19.873</duration>
          <className>org.apache.spark.sql.sources.BucketedReadSuite</className>
          <testName>read partitioning bucketed tables having composite filters</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.475</duration>
          <className>org.apache.spark.sql.sources.BucketedReadSuite</className>
          <testName>avoid shuffle when join 2 bucketed tables</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.sql.sources.BucketedReadSuite</className>
          <testName>avoid shuffle when join keys are a super-set of bucket keys</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.571</duration>
          <className>org.apache.spark.sql.sources.BucketedReadSuite</className>
          <testName>only shuffle one side when join bucketed table and non-bucketed table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.705</duration>
          <className>org.apache.spark.sql.sources.BucketedReadSuite</className>
          <testName>only shuffle one side when 2 bucketed tables have different bucket number</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.482</duration>
          <className>org.apache.spark.sql.sources.BucketedReadSuite</className>
          <testName>only shuffle one side when 2 bucketed tables have different bucket keys</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.212</duration>
          <className>org.apache.spark.sql.sources.BucketedReadSuite</className>
          <testName>shuffle when join keys are not equal to bucket keys</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.246</duration>
          <className>org.apache.spark.sql.sources.BucketedReadSuite</className>
          <testName>shuffle when join 2 bucketed tables with bucketing disabled</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.899</duration>
          <className>org.apache.spark.sql.sources.BucketedReadSuite</className>
          <testName>avoid shuffle and sort when bucket and sort columns are join keys</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.658</duration>
          <className>org.apache.spark.sql.sources.BucketedReadSuite</className>
          <testName>avoid shuffle and sort when sort columns are a super set of join keys</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.324</duration>
          <className>org.apache.spark.sql.sources.BucketedReadSuite</className>
          <testName>only sort one side when sort columns are different</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.43</duration>
          <className>org.apache.spark.sql.sources.BucketedReadSuite</className>
          <testName>only sort one side when sort columns are same but their ordering is different</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.856</duration>
          <className>org.apache.spark.sql.sources.BucketedReadSuite</className>
          <testName>avoid shuffle when grouping keys are equal to bucket keys</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.255</duration>
          <className>org.apache.spark.sql.sources.BucketedReadSuite</className>
          <testName>avoid shuffle when grouping keys are a super-set of bucket keys</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.612</duration>
          <className>org.apache.spark.sql.sources.BucketedReadSuite</className>
          <testName>SPARK-17698 Join predicates should not contain filter clauses</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.315</duration>
          <className>org.apache.spark.sql.sources.BucketedReadSuite</className>
          <testName>error if there exists any malformed bucket files</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.995</duration>
          <className>org.apache.spark.sql.sources.BucketedReadSuite</className>
          <testName>disable bucketing when the output doesn&apos;t contain all bucketing columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.sources.BucketedWriteSuite.xml</file>
      <name>org.apache.spark.sql.sources.BucketedWriteSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>57.725998</duration>
      <cases>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.sql.sources.BucketedWriteSuite</className>
          <testName>bucketed by non-existing column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.018</duration>
          <className>org.apache.spark.sql.sources.BucketedWriteSuite</className>
          <testName>numBuckets not greater than 0 or less than 100000</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.sql.sources.BucketedWriteSuite</className>
          <testName>specify sorting columns without bucketing columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.056</duration>
          <className>org.apache.spark.sql.sources.BucketedWriteSuite</className>
          <testName>sorting by non-orderable column</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.sql.sources.BucketedWriteSuite</className>
          <testName>write bucketed data using save()</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.013</duration>
          <className>org.apache.spark.sql.sources.BucketedWriteSuite</className>
          <testName>write bucketed data using insertInto()</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>8.565</duration>
          <className>org.apache.spark.sql.sources.BucketedWriteSuite</className>
          <testName>write bucketed data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>27.758</duration>
          <className>org.apache.spark.sql.sources.BucketedWriteSuite</className>
          <testName>write bucketed data with sortBy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.sql.sources.BucketedWriteSuite</className>
          <testName>write bucketed data with the overlapping bucketBy and partitionBy columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.sql.sources.BucketedWriteSuite</className>
          <testName>write bucketed data with the identical bucketBy and partitionBy columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.525</duration>
          <className>org.apache.spark.sql.sources.BucketedWriteSuite</className>
          <testName>write bucketed data without partitionBy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>10.481</duration>
          <className>org.apache.spark.sql.sources.BucketedWriteSuite</className>
          <testName>write bucketed data without partitionBy with sortBy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>8.247</duration>
          <className>org.apache.spark.sql.sources.BucketedWriteSuite</className>
          <testName>write bucketed data with bucketing disabled</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.sources.CommitFailureTestRelationSuite.xml</file>
      <name>org.apache.spark.sql.sources.CommitFailureTestRelationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.384</duration>
      <cases>
        <case>
          <duration>0.168</duration>
          <className>org.apache.spark.sql.sources.CommitFailureTestRelationSuite</className>
          <testName>SPARK-7684: commitTask() failure should fallback to abortTask()</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.1</duration>
          <className>org.apache.spark.sql.sources.CommitFailureTestRelationSuite</className>
          <testName>call failure callbacks before close writer - default</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.116</duration>
          <className>org.apache.spark.sql.sources.CommitFailureTestRelationSuite</className>
          <testName>call failure callbacks before close writer - partitioned</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.sources.JsonHadoopFsRelationSuite.xml</file>
      <name>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>43.050995</duration>
      <cases>
        <case>
          <duration>0.765</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.614</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.711</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.676</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.68</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.664</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.649</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.658</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.711</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.624</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.654</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.583</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.653</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.616</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.651</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.648</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.629</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.64</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.714</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.637</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.709</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.647</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.774</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.686</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.728</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.66</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.003</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.918</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.742</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.74</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.765</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.717</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.268</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>save()/load() - non-partitioned table - Overwrite</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.54</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>save()/load() - non-partitioned table - Append</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>save()/load() - non-partitioned table - ErrorIfExists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>save()/load() - non-partitioned table - Ignore</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.037</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>save()/load() - partitioned table - simple queries</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.484</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>save()/load() - partitioned table - Overwrite</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.447</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>save()/load() - partitioned table - Append</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.329</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>save()/load() - partitioned table - Append - new partition values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>save()/load() - partitioned table - ErrorIfExists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>save()/load() - partitioned table - Ignore</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.245</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - non-partitioned table - Overwrite</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.427</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - non-partitioned table - Append</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.076</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - non-partitioned table - ErrorIfExists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.119</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - non-partitioned table - Ignore</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.976</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - partitioned table - simple queries</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.333</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - partitioned table - boolean type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.97</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - partitioned table - Overwrite</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.849</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - partitioned table - Append</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.711</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - partitioned table - Append - new partition values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.292</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - partitioned table - Append - mismatched partition columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.073</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - partitioned table - ErrorIfExists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - partitioned table - Ignore</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.289</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>load() - with directory of unpartitioned data in nested subdirs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.628</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>Hadoop style globbing - unpartitioned data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.292</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>Hadoop style globbing - partitioned data with schema inference</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.884</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>SPARK-9735 Partition column type casting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.676</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>SPARK-7616: adjust column name order accordingly when saving partitioned table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>5.484</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>SPARK-8406: Avoids name collision while writing files</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.462</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>SPARK-8578 specified custom output committer will not be used to append data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.135</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>SPARK-8887: Explicitly define which data types can be used as dynamic partition columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.336</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>Locality support for FileScanRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.907</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>save()/load() - partitioned table - simple queries - partition columns in data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.298</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>SPARK-9894: save complex types to JSON</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.17</duration>
          <className>org.apache.spark.sql.sources.JsonHadoopFsRelationSuite</className>
          <testName>SPARK-10196: save decimal type to JSON</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite.xml</file>
      <name>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>47.730007</duration>
      <cases>
        <case>
          <duration>0.665</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.617</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.643</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.603</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.642</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.629</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.658</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.627</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.646</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.612</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.674</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.634</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.751</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.645</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.715</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.653</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.645</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.597</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.741</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.651</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.679</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.609</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.682</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.615</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.697</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.63</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.685</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.68</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.057</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.977</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.805</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.7</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.68</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.647</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.352</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>save()/load() - non-partitioned table - Overwrite</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.542</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>save()/load() - non-partitioned table - Append</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>save()/load() - non-partitioned table - ErrorIfExists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>save()/load() - non-partitioned table - Ignore</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.221</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>save()/load() - partitioned table - simple queries</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.524</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>save()/load() - partitioned table - Overwrite</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.514</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>save()/load() - partitioned table - Append</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.359</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>save()/load() - partitioned table - Append - new partition values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>save()/load() - partitioned table - ErrorIfExists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>save()/load() - partitioned table - Ignore</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.198</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - non-partitioned table - Overwrite</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.471</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - non-partitioned table - Append</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.08</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - non-partitioned table - ErrorIfExists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.119</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - non-partitioned table - Ignore</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.981</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - partitioned table - simple queries</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.729</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - partitioned table - boolean type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.976</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - partitioned table - Overwrite</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.97</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - partitioned table - Append</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.746</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - partitioned table - Append - new partition values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.277</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - partitioned table - Append - mismatched partition columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.087</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - partitioned table - ErrorIfExists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.031</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - partitioned table - Ignore</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.388</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>load() - with directory of unpartitioned data in nested subdirs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.733</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>Hadoop style globbing - unpartitioned data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.714</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>Hadoop style globbing - partitioned data with schema inference</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.185</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>SPARK-9735 Partition column type casting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.792</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>SPARK-7616: adjust column name order accordingly when saving partitioned table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>5.326</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>SPARK-8406: Avoids name collision while writing files</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.518</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>SPARK-8578 specified custom output committer will not be used to append data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.214</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>SPARK-8887: Explicitly define which data types can be used as dynamic partition columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.4</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>Locality support for FileScanRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.369</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>save()/load() - partitioned table - simple queries - partition columns in data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.396</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>SPARK-7868: _temporary directories should be ignored</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.276</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>Append</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.072</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>abortJob</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.339</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>SPARK-8604: Parquet data source should write summary file while doing appending</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.242</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>SPARK-10334 Projections and filters should be kept in physical plan</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.386</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>SPARK-11500: Not deterministic order of columns when using merging schemas</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.708</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>SPARK-13537: Fix readBytes in VectorizedPlainValuesReader</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.271</duration>
          <className>org.apache.spark.sql.sources.ParquetHadoopFsRelationSuite</className>
          <testName>SPARK-13543: Support for specifying compression codec for Parquet via option()</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/sql/hive/target/test-reports/org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite.xml</file>
      <name>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>30.887999</duration>
      <cases>
        <case>
          <duration>0.762</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.667</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.643</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.66</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.664</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.641</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.676</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.603</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.699</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.631</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.73</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.619</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.715</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.636</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.686</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.677</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.672</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>dictionary = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.694</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>dictionary = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.281</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>save()/load() - non-partitioned table - Overwrite</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.55</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>save()/load() - non-partitioned table - Append</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>save()/load() - non-partitioned table - ErrorIfExists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>save()/load() - non-partitioned table - Ignore</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.084</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>save()/load() - partitioned table - simple queries</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.458</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>save()/load() - partitioned table - Overwrite</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.474</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>save()/load() - partitioned table - Append</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.316</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>save()/load() - partitioned table - Append - new partition values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>save()/load() - partitioned table - ErrorIfExists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.009</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>save()/load() - partitioned table - Ignore</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.214</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - non-partitioned table - Overwrite</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.484</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - non-partitioned table - Append</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.08</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - non-partitioned table - ErrorIfExists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.125</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - non-partitioned table - Ignore</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.793</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - partitioned table - simple queries</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.791</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - partitioned table - boolean type</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.002</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - partitioned table - Overwrite</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.833</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - partitioned table - Append</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.828</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - partitioned table - Append - new partition values</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.319</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - partitioned table - Append - mismatched partition columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - partitioned table - ErrorIfExists</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>saveAsTable()/load() - partitioned table - Ignore</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.312</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>load() - with directory of unpartitioned data in nested subdirs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.682</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>Hadoop style globbing - unpartitioned data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.184</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>Hadoop style globbing - partitioned data with schema inference</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.911</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>SPARK-9735 Partition column type casting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.663</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>SPARK-7616: adjust column name order accordingly when saving partitioned table</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>4.6</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>SPARK-8406: Avoids name collision while writing files</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.375</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>SPARK-8578 specified custom output committer will not be used to append data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.108</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>SPARK-8887: Explicitly define which data types can be used as dynamic partition columns</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.258</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>Locality support for FileScanRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.877</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>save()/load() - partitioned table - simple queries - partition columns in data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.135</duration>
          <className>org.apache.spark.sql.sources.SimpleTextHadoopFsRelationSuite</className>
          <testName>test hadoop conf option propagation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.BasicOperationsSuite.xml</file>
      <name>org.apache.spark.streaming.BasicOperationsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>14.038</duration>
      <cases>
        <case>
          <duration>0.208</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.208</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>flatMap</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.212</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>filter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.21</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>glom</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.259</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>mapPartitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.322</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>repartition (more partitions)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.314</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>repartition (fewer partitions)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.261</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>groupByKey</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.259</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>reduceByKey</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.26</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>reduce</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.311</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>count</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.309</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>countByValue</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.268</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>mapValues</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.264</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>flatMapValues</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.217</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>union</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.12</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>union with input stream return None</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.259</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>union</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.211</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>transform</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.064</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>transform with NULL</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.087</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>transform with input stream return None</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.32</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>transformWith</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.139</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>transformWith with input stream return None</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.262</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>transform</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.122</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>transform with input stream return None</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.321</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>cogroup</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.325</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>join</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.321</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>leftOuterJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.32</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>rightOuterJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.323</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>fullOuterJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.38</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>updateStateByKey</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.367</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>updateStateByKey - simple with initial value RDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.374</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>updateStateByKey - testing time stamps as input</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.362</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>updateStateByKey - with initial value RDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.319</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>updateStateByKey - object lifecycle</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.129</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>slice</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.045</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>slice - has not been initialized</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.313</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>rdd cleanup - map and window</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.564</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>rdd cleanup - updateStateByKey</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.109</duration>
          <className>org.apache.spark.streaming.BasicOperationsSuite</className>
          <testName>rdd cleanup - input blocks and persisted RDDs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.CheckpointSuite.xml</file>
      <name>org.apache.spark.streaming.CheckpointSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>20.992998</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.streaming.CheckpointSuite</className>
          <testName>non-existent checkpoint dir</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>8.781</duration>
          <className>org.apache.spark.streaming.CheckpointSuite</className>
          <testName>basic rdd checkpoints + dstream graph checkpoint recovery</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.113</duration>
          <className>org.apache.spark.streaming.CheckpointSuite</className>
          <testName>recovery of conf through checkpoints</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.116</duration>
          <className>org.apache.spark.streaming.CheckpointSuite</className>
          <testName>[host|port] from checkpoint</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.365</duration>
          <className>org.apache.spark.streaming.CheckpointSuite</className>
          <testName>recovery with map and reduceByKey operations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.891</duration>
          <className>org.apache.spark.streaming.CheckpointSuite</className>
          <testName>recovery with invertible reduceByKeyAndWindow operation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.954</duration>
          <className>org.apache.spark.streaming.CheckpointSuite</className>
          <testName>recovery with saveAsHadoopFiles operation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.879</duration>
          <className>org.apache.spark.streaming.CheckpointSuite</className>
          <testName>recovery with saveAsNewAPIHadoopFiles operation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.858</duration>
          <className>org.apache.spark.streaming.CheckpointSuite</className>
          <testName>recovery with saveAsHadoopFile inside transform operation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.802</duration>
          <className>org.apache.spark.streaming.CheckpointSuite</className>
          <testName>recovery with updateStateByKey operation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.614</duration>
          <className>org.apache.spark.streaming.CheckpointSuite</className>
          <testName>recovery maintains rate controller</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.821</duration>
          <className>org.apache.spark.streaming.CheckpointSuite</className>
          <testName>recovery with file input stream</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.198</duration>
          <className>org.apache.spark.streaming.CheckpointSuite</className>
          <testName>restore invoking times</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.897</duration>
          <className>org.apache.spark.streaming.CheckpointSuite</className>
          <testName>recovery from checkpoint contains array object</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.046</duration>
          <className>org.apache.spark.streaming.CheckpointSuite</className>
          <testName>SPARK-11267: the race condition of two checkpoints in a batch</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.657</duration>
          <className>org.apache.spark.streaming.CheckpointSuite</className>
          <testName>SPARK-6847: stack overflow when updateStateByKey is followed by a checkpointed dstream</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.DStreamClosureSuite.xml</file>
      <name>org.apache.spark.streaming.DStreamClosureSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.061</duration>
      <cases>
        <case>
          <duration>0.061</duration>
          <className>org.apache.spark.streaming.DStreamClosureSuite</className>
          <testName>user provided closures are actually cleaned</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.DStreamScopeSuite.xml</file>
      <name>org.apache.spark.streaming.DStreamScopeSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.082</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.streaming.DStreamScopeSuite</className>
          <testName>dstream without scope</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.streaming.DStreamScopeSuite</className>
          <testName>input dstream without scope</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.streaming.DStreamScopeSuite</className>
          <testName>scoping simple operations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.streaming.DStreamScopeSuite</className>
          <testName>scoping nested operations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.017</duration>
          <className>org.apache.spark.streaming.DStreamScopeSuite</className>
          <testName>transform should allow RDD operations to be captured in scopes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.streaming.DStreamScopeSuite</className>
          <testName>foreachRDD should allow RDD operations to be captured in scope</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.DurationSuite.xml</file>
      <name>org.apache.spark.streaming.DurationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.036999993</duration>
      <cases>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.streaming.DurationSuite</className>
          <testName>less</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.streaming.DurationSuite</className>
          <testName>lessEq</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.streaming.DurationSuite</className>
          <testName>greater</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.DurationSuite</className>
          <testName>greaterEq</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.streaming.DurationSuite</className>
          <testName>plus</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.streaming.DurationSuite</className>
          <testName>minus</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.DurationSuite</className>
          <testName>times</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.streaming.DurationSuite</className>
          <testName>div</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.streaming.DurationSuite</className>
          <testName>isMultipleOf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.streaming.DurationSuite</className>
          <testName>min</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.DurationSuite</className>
          <testName>max</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.streaming.DurationSuite</className>
          <testName>isZero</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.streaming.DurationSuite</className>
          <testName>Milliseconds</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.streaming.DurationSuite</className>
          <testName>Seconds</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.streaming.DurationSuite</className>
          <testName>Minutes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.FailureSuite.xml</file>
      <name>org.apache.spark.streaming.FailureSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>74.511</duration>
      <cases>
        <case>
          <duration>35.347</duration>
          <className>org.apache.spark.streaming.FailureSuite</className>
          <testName>multiple failures with map</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>39.164</duration>
          <className>org.apache.spark.streaming.FailureSuite</className>
          <testName>multiple failures with updateStateByKey</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.InputStreamsSuite.xml</file>
      <name>org.apache.spark.streaming.InputStreamsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>19.302</duration>
      <cases>
        <case>
          <duration>2.941</duration>
          <className>org.apache.spark.streaming.InputStreamsSuite</className>
          <testName>socket input stream</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.396</duration>
          <className>org.apache.spark.streaming.InputStreamsSuite</className>
          <testName>socket input stream - no block in a batch</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>10.244</duration>
          <className>org.apache.spark.streaming.InputStreamsSuite</className>
          <testName>binary records stream</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.208</duration>
          <className>org.apache.spark.streaming.InputStreamsSuite</className>
          <testName>file input stream - newFilesOnly = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.23</duration>
          <className>org.apache.spark.streaming.InputStreamsSuite</className>
          <testName>file input stream - newFilesOnly = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.365</duration>
          <className>org.apache.spark.streaming.InputStreamsSuite</className>
          <testName>file input stream - wildcard</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.753</duration>
          <className>org.apache.spark.streaming.InputStreamsSuite</className>
          <testName>multi-thread receiver</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.072</duration>
          <className>org.apache.spark.streaming.InputStreamsSuite</className>
          <testName>queue input stream - oneAtATime = true</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.053</duration>
          <className>org.apache.spark.streaming.InputStreamsSuite</className>
          <testName>queue input stream - oneAtATime = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.04</duration>
          <className>org.apache.spark.streaming.InputStreamsSuite</className>
          <testName>test track the number of input stream</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.JavaAPISuite.xml</file>
      <name>org.apache.spark.streaming.JavaAPISuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>14.511001</duration>
      <cases>
        <case>
          <duration>0.306</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testStreamingContextTransform</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.202</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testFlatMapValues</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.308</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testReduceByWindowWithInverse</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.205</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testMapPartitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.202</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testPairFilter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.257</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testRepartitionFewerPartitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.202</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testCombineByKey</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.547</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testContextGetOrCreate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.254</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testWindowWithSlideDuration</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.265</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testQueueStream</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.255</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testCountByValue</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.203</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testMap</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.205</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testPairToNormalRDDTransform</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.206</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testPairReduceByKey</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.272</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testCount</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>3.359</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testCheckpointMasterRecovery</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.198</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testPairMap</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.2</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testUnion</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.206</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testFlatMap</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.249</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testReduceByKeyAndWindowWithInverse</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.196</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testGlom</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.251</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.2</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testPairFlatMap</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.2</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testPairToPairFlatMapWithChangingTypes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.2</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testPairMapPartitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.25</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testRepartitionMorePartitions</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.303</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testReduceByWindowWithoutInverse</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.264</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testLeftOuterJoin</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testVariousTransform</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.212</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testTransformWith</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.052</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testVariousTransformWith</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.202</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testTextFileStream</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.201</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testPairGroupByKey</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.26</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testCoGroup</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.037</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testInitialization</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.041</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testSocketString</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.255</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testGroupByKeyAndWindow</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.249</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testReduceByKeyAndWindow</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.266</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testForeachRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.205</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testFileStream</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.253</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testPairTransform</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.204</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testFilter</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.203</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testPairMap2</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.201</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testMapValues</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.253</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testReduce</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.256</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testUpdateStateByKey</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.205</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testTransform</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.248</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testWindow</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.307</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testCountByValueAndWindow</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testRawSocketStream</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testSocketTextStream</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.254</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testUpdateStateByKeyWithInitial</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.049</duration>
          <className>org.apache.spark.streaming.JavaAPISuite</className>
          <testName>testContextState</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.JavaDurationSuite.xml</file>
      <name>org.apache.spark.streaming.JavaDurationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.002</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.streaming.JavaDurationSuite</className>
          <testName>testGreaterEq</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.JavaDurationSuite</className>
          <testName>testDiv</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.JavaDurationSuite</className>
          <testName>testMinus</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.JavaDurationSuite</className>
          <testName>testTimes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.JavaDurationSuite</className>
          <testName>testLess</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.JavaDurationSuite</className>
          <testName>testPlus</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.JavaDurationSuite</className>
          <testName>testGreater</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.streaming.JavaDurationSuite</className>
          <testName>testMinutes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.JavaDurationSuite</className>
          <testName>testMilliseconds</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.JavaDurationSuite</className>
          <testName>testLessEq</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.JavaDurationSuite</className>
          <testName>testSeconds</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.JavaMapWithStateSuite.xml</file>
      <name>org.apache.spark.streaming.JavaMapWithStateSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.325</duration>
      <cases>
        <case>
          <duration>0.325</duration>
          <className>org.apache.spark.streaming.JavaMapWithStateSuite</className>
          <testName>testBasicFunction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.JavaReceiverAPISuite.xml</file>
      <name>org.apache.spark.streaming.JavaReceiverAPISuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.15</duration>
      <cases>
        <case>
          <duration>1.15</duration>
          <className>org.apache.spark.streaming.JavaReceiverAPISuite</className>
          <testName>testReceiver</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.JavaTimeSuite.xml</file>
      <name>org.apache.spark.streaming.JavaTimeSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.0</duration>
      <cases>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.JavaTimeSuite</className>
          <testName>testGreaterEq</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.JavaTimeSuite</className>
          <testName>testLess</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.JavaTimeSuite</className>
          <testName>testPlus</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.JavaTimeSuite</className>
          <testName>testMinusDuration</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.JavaTimeSuite</className>
          <testName>testGreater</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.JavaTimeSuite</className>
          <testName>testLessEq</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.JavaTimeSuite</className>
          <testName>testMinusTime</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.JavaWriteAheadLogSuite.xml</file>
      <name>org.apache.spark.streaming.JavaWriteAheadLogSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.016</duration>
      <cases>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.streaming.JavaWriteAheadLogSuite</className>
          <testName>testCustomWAL</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.MapWithStateSuite.xml</file>
      <name>org.apache.spark.streaming.MapWithStateSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.9589999</duration>
      <cases>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.streaming.MapWithStateSuite</className>
          <testName>state - get, exists, update, remove,</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.337</duration>
          <className>org.apache.spark.streaming.MapWithStateSuite</className>
          <testName>mapWithState - basic operations with simple API</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.315</duration>
          <className>org.apache.spark.streaming.MapWithStateSuite</className>
          <testName>mapWithState - basic operations with advanced API</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.streaming.MapWithStateSuite</className>
          <testName>mapWithState - type inferencing and class tags</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.28</duration>
          <className>org.apache.spark.streaming.MapWithStateSuite</className>
          <testName>mapWithState - states as mapped data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.311</duration>
          <className>org.apache.spark.streaming.MapWithStateSuite</className>
          <testName>mapWithState - initial states, with nothing returned as from mapping function</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.337</duration>
          <className>org.apache.spark.streaming.MapWithStateSuite</className>
          <testName>mapWithState - state removing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.873</duration>
          <className>org.apache.spark.streaming.MapWithStateSuite</className>
          <testName>mapWithState - state timing out</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.streaming.MapWithStateSuite</className>
          <testName>mapWithState - checkpoint durations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.461</duration>
          <className>org.apache.spark.streaming.MapWithStateSuite</className>
          <testName>mapWithState - driver failure recovery</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.ReceivedBlockHandlerSuite.xml</file>
      <name>org.apache.spark.streaming.ReceivedBlockHandlerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.47000003</duration>
      <cases>
        <case>
          <duration>0.116</duration>
          <className>org.apache.spark.streaming.ReceivedBlockHandlerSuite</className>
          <testName>BlockManagerBasedBlockHandler - store blocks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.streaming.ReceivedBlockHandlerSuite</className>
          <testName>BlockManagerBasedBlockHandler - handle errors in storing block</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.19</duration>
          <className>org.apache.spark.streaming.ReceivedBlockHandlerSuite</className>
          <testName>WriteAheadLogBasedBlockHandler - store blocks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.streaming.ReceivedBlockHandlerSuite</className>
          <testName>WriteAheadLogBasedBlockHandler - handle errors in storing block</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.streaming.ReceivedBlockHandlerSuite</className>
          <testName>WriteAheadLogBasedBlockHandler - clean old blocks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.079</duration>
          <className>org.apache.spark.streaming.ReceivedBlockHandlerSuite</className>
          <testName>Test Block - count messages</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.streaming.ReceivedBlockHandlerSuite</className>
          <testName>Test Block - isFullyConsumed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.ReceivedBlockTrackerSuite.xml</file>
      <name>org.apache.spark.streaming.ReceivedBlockTrackerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.078</duration>
      <cases>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.streaming.ReceivedBlockTrackerSuite</className>
          <testName>block addition, and block to batch allocation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.streaming.ReceivedBlockTrackerSuite</className>
          <testName>recovery and cleanup with write ahead logs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.streaming.ReceivedBlockTrackerSuite</className>
          <testName>disable write ahead log when checkpoint directory is not set</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.streaming.ReceivedBlockTrackerSuite</className>
          <testName>parallel file deletion in FileBasedWriteAheadLog is robust to deletion error</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.ReceiverInputDStreamSuite.xml</file>
      <name>org.apache.spark.streaming.ReceiverInputDStreamSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.291</duration>
      <cases>
        <case>
          <duration>0.044</duration>
          <className>org.apache.spark.streaming.ReceiverInputDStreamSuite</className>
          <testName>Without WAL enabled: createBlockRDD creates empty BlockRDD when no block info</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.059</duration>
          <className>org.apache.spark.streaming.ReceiverInputDStreamSuite</className>
          <testName>Without WAL enabled: createBlockRDD creates correct BlockRDD with block info</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.049</duration>
          <className>org.apache.spark.streaming.ReceiverInputDStreamSuite</className>
          <testName>Without WAL enabled: createBlockRDD filters non-existent blocks before creating BlockRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.streaming.ReceiverInputDStreamSuite</className>
          <testName>With WAL enabled: createBlockRDD creates empty WALBackedBlockRDD when no block info</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.streaming.ReceiverInputDStreamSuite</className>
          <testName>With WAL enabled: createBlockRDD creates correct WALBackedBlockRDD with all block info having WAL info</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.streaming.ReceiverInputDStreamSuite</className>
          <testName>With WAL enabled: createBlockRDD creates BlockRDD when some block info don&apos;t have WAL info</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.ReceiverSuite.xml</file>
      <name>org.apache.spark.streaming.ReceiverSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>9.475</duration>
      <cases>
        <case>
          <duration>0.422</duration>
          <className>org.apache.spark.streaming.ReceiverSuite</className>
          <testName>receiver life cycle</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>-0.001</duration>
          <className>org.apache.spark.streaming.ReceiverSuite</className>
          <testName>block generator throttling</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>9.054</duration>
          <className>org.apache.spark.streaming.ReceiverSuite</className>
          <testName>write ahead log - generating and cleaning</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.StateMapSuite.xml</file>
      <name>org.apache.spark.streaming.StateMapSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>7.4799995</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.streaming.StateMapSuite</className>
          <testName>EmptyStateMap</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.StateMapSuite</className>
          <testName>OpenHashMapBasedStateMap - put, get, getByTime, getAll, remove</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.streaming.StateMapSuite</className>
          <testName>OpenHashMapBasedStateMap - put, get, getByTime, getAll, remove with copy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.036</duration>
          <className>org.apache.spark.streaming.StateMapSuite</className>
          <testName>OpenHashMapBasedStateMap - serializing and deserializing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.streaming.StateMapSuite</className>
          <testName>OpenHashMapBasedStateMap - serializing and deserializing with compaction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>7.403</duration>
          <className>org.apache.spark.streaming.StateMapSuite</className>
          <testName>OpenHashMapBasedStateMap - all possible sequences of operations with copies</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.streaming.StateMapSuite</className>
          <testName>OpenHashMapBasedStateMap - serializing and deserializing with KryoSerializable states</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.streaming.StateMapSuite</className>
          <testName>EmptyStateMap - serializing and deserializing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.streaming.StateMapSuite</className>
          <testName>MapWithStateRDDRecord - serializing and deserializing with KryoSerializable states</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.StreamingContextSuite.xml</file>
      <name>org.apache.spark.streaming.StreamingContextSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>30.989002</duration>
      <cases>
        <case>
          <duration>0.045</duration>
          <className>org.apache.spark.streaming.StreamingContextSuite</className>
          <testName>from no conf constructor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.043</duration>
          <className>org.apache.spark.streaming.StreamingContextSuite</className>
          <testName>from no conf + spark home</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.048</duration>
          <className>org.apache.spark.streaming.StreamingContextSuite</className>
          <testName>from no conf + spark home + env</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.228</duration>
          <className>org.apache.spark.streaming.StreamingContextSuite</className>
          <testName>from conf with settings</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.streaming.StreamingContextSuite</className>
          <testName>from existing SparkContext</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.061</duration>
          <className>org.apache.spark.streaming.StreamingContextSuite</className>
          <testName>from existing SparkContext with settings</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.177</duration>
          <className>org.apache.spark.streaming.StreamingContextSuite</className>
          <testName>from checkpoint</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.061</duration>
          <className>org.apache.spark.streaming.StreamingContextSuite</className>
          <testName>checkPoint from conf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.StreamingContextSuite</className>
          <testName>state matching</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.057</duration>
          <className>org.apache.spark.streaming.StreamingContextSuite</className>
          <testName>start and stop state check</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.126</duration>
          <className>org.apache.spark.streaming.StreamingContextSuite</className>
          <testName>start with non-serializable DStream checkpoints</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.055</duration>
          <className>org.apache.spark.streaming.StreamingContextSuite</className>
          <testName>start failure should stop internal components</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.149</duration>
          <className>org.apache.spark.streaming.StreamingContextSuite</className>
          <testName>start should set local properties of streaming jobs correctly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.058</duration>
          <className>org.apache.spark.streaming.StreamingContextSuite</className>
          <testName>start multiple times</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.07</duration>
          <className>org.apache.spark.streaming.StreamingContextSuite</className>
          <testName>stop multiple times</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.067</duration>
          <className>org.apache.spark.streaming.StreamingContextSuite</className>
          <testName>stop before start</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.07</duration>
          <className>org.apache.spark.streaming.StreamingContextSuite</className>
          <testName>start after stop</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.172</duration>
          <className>org.apache.spark.streaming.StreamingContextSuite</className>
          <testName>stop only streaming context</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.072</duration>
          <className>org.apache.spark.streaming.StreamingContextSuite</className>
          <testName>stop(stopSparkContext=true) after stop(stopSparkContext=false)</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>6.341</duration>
          <className>org.apache.spark.streaming.StreamingContextSuite</className>
          <testName>stop gracefully</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.591</duration>
          <className>org.apache.spark.streaming.StreamingContextSuite</className>
          <testName>stop gracefully even if a receiver misses StopReceiver</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>15.699</duration>
          <className>org.apache.spark.streaming.StreamingContextSuite</className>
          <testName>stop slow receiver gracefully</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.052</duration>
          <className>org.apache.spark.streaming.StreamingContextSuite</className>
          <testName>registering and de-registering of streamingSource</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>2.068</duration>
          <className>org.apache.spark.streaming.StreamingContextSuite</className>
          <testName>awaitTermination</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.049</duration>
          <className>org.apache.spark.streaming.StreamingContextSuite</className>
          <testName>awaitTermination after stop</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.121</duration>
          <className>org.apache.spark.streaming.StreamingContextSuite</className>
          <testName>awaitTermination with error in task</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.469</duration>
          <className>org.apache.spark.streaming.StreamingContextSuite</className>
          <testName>awaitTermination with error in job generation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.062</duration>
          <className>org.apache.spark.streaming.StreamingContextSuite</className>
          <testName>awaitTerminationOrTimeout</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.589</duration>
          <className>org.apache.spark.streaming.StreamingContextSuite</className>
          <testName>getOrCreate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.065</duration>
          <className>org.apache.spark.streaming.StreamingContextSuite</className>
          <testName>getActive and getActiveOrCreate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.547</duration>
          <className>org.apache.spark.streaming.StreamingContextSuite</className>
          <testName>getActiveOrCreate with checkpoint</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.055</duration>
          <className>org.apache.spark.streaming.StreamingContextSuite</className>
          <testName>multiple streaming contexts</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.478</duration>
          <className>org.apache.spark.streaming.StreamingContextSuite</className>
          <testName>DStream and generated RDD creation sites</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.088</duration>
          <className>org.apache.spark.streaming.StreamingContextSuite</className>
          <testName>throw exception on using active or stopped context</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.14</duration>
          <className>org.apache.spark.streaming.StreamingContextSuite</className>
          <testName>queueStream doesn&apos;t support checkpointing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.974</duration>
          <className>org.apache.spark.streaming.StreamingContextSuite</className>
          <testName>Creating an InputDStream but not using it should not crash</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.StreamingListenerSuite.xml</file>
      <name>org.apache.spark.streaming.StreamingListenerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>5.6359997</duration>
      <cases>
        <case>
          <duration>0.558</duration>
          <className>org.apache.spark.streaming.StreamingListenerSuite</className>
          <testName>batch info reporting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.121</duration>
          <className>org.apache.spark.streaming.StreamingListenerSuite</className>
          <testName>receiver info reporting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.575</duration>
          <className>org.apache.spark.streaming.StreamingListenerSuite</className>
          <testName>output operation reporting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.977</duration>
          <className>org.apache.spark.streaming.StreamingListenerSuite</className>
          <testName>stop in listener</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.0</duration>
          <className>org.apache.spark.streaming.StreamingListenerSuite</className>
          <testName>onBatchCompleted with successful batch</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.999</duration>
          <className>org.apache.spark.streaming.StreamingListenerSuite</className>
          <testName>onBatchCompleted with failed batch and one failed job</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.003</duration>
          <className>org.apache.spark.streaming.StreamingListenerSuite</className>
          <testName>onBatchCompleted with failed batch and multiple failed jobs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.403</duration>
          <className>org.apache.spark.streaming.StreamingListenerSuite</className>
          <testName>StreamingListener receives no events after stopping StreamingListenerBus</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.TimeSuite.xml</file>
      <name>org.apache.spark.streaming.TimeSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.002</duration>
      <cases>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.TimeSuite</className>
          <testName>less</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.TimeSuite</className>
          <testName>lessEq</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.TimeSuite</className>
          <testName>greater</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.TimeSuite</className>
          <testName>greaterEq</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.TimeSuite</className>
          <testName>plus</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.TimeSuite</className>
          <testName>minus Time</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.TimeSuite</className>
          <testName>minus Duration</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.streaming.TimeSuite</className>
          <testName>floor</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.TimeSuite</className>
          <testName>isMultipleOf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.TimeSuite</className>
          <testName>min</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.TimeSuite</className>
          <testName>max</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.TimeSuite</className>
          <testName>until</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.streaming.TimeSuite</className>
          <testName>to</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.UISeleniumSuite.xml</file>
      <name>org.apache.spark.streaming.UISeleniumSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.405</duration>
      <cases>
        <case>
          <duration>2.405</duration>
          <className>org.apache.spark.streaming.UISeleniumSuite</className>
          <testName>attaching and detaching a Streaming tab</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.WindowOperationsSuite.xml</file>
      <name>org.apache.spark.streaming.WindowOperationsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>12.789001</duration>
      <cases>
        <case>
          <duration>3.623</duration>
          <className>org.apache.spark.streaming.WindowOperationsSuite</className>
          <testName>window - basic window</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.566</duration>
          <className>org.apache.spark.streaming.WindowOperationsSuite</className>
          <testName>window - tumbling window</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.569</duration>
          <className>org.apache.spark.streaming.WindowOperationsSuite</className>
          <testName>window - larger window</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.352</duration>
          <className>org.apache.spark.streaming.WindowOperationsSuite</className>
          <testName>window - non-overlapping window</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.102</duration>
          <className>org.apache.spark.streaming.WindowOperationsSuite</className>
          <testName>window - persistence level</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.413</duration>
          <className>org.apache.spark.streaming.WindowOperationsSuite</className>
          <testName>reduceByKeyAndWindow - basic reduction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.347</duration>
          <className>org.apache.spark.streaming.WindowOperationsSuite</className>
          <testName>reduceByKeyAndWindow - key already in window and new value added into window</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.343</duration>
          <className>org.apache.spark.streaming.WindowOperationsSuite</className>
          <testName>reduceByKeyAndWindow - new key added into window</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.472</duration>
          <className>org.apache.spark.streaming.WindowOperationsSuite</className>
          <testName>reduceByKeyAndWindow - key removed from window</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.5</duration>
          <className>org.apache.spark.streaming.WindowOperationsSuite</className>
          <testName>reduceByKeyAndWindow - larger slide time</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.753</duration>
          <className>org.apache.spark.streaming.WindowOperationsSuite</className>
          <testName>reduceByKeyAndWindow - big test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.286</duration>
          <className>org.apache.spark.streaming.WindowOperationsSuite</className>
          <testName>reduceByKeyAndWindow with inverse function - basic reduction</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.333</duration>
          <className>org.apache.spark.streaming.WindowOperationsSuite</className>
          <testName>reduceByKeyAndWindow with inverse function - key already in window and new value added into window</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.342</duration>
          <className>org.apache.spark.streaming.WindowOperationsSuite</className>
          <testName>reduceByKeyAndWindow with inverse function - new key added into window</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.43</duration>
          <className>org.apache.spark.streaming.WindowOperationsSuite</className>
          <testName>reduceByKeyAndWindow with inverse function - key removed from window</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.488</duration>
          <className>org.apache.spark.streaming.WindowOperationsSuite</className>
          <testName>reduceByKeyAndWindow with inverse function - larger slide time</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.739</duration>
          <className>org.apache.spark.streaming.WindowOperationsSuite</className>
          <testName>reduceByKeyAndWindow with inverse function - big test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.744</duration>
          <className>org.apache.spark.streaming.WindowOperationsSuite</className>
          <testName>reduceByKeyAndWindow with inverse and filter functions - big test</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.598</duration>
          <className>org.apache.spark.streaming.WindowOperationsSuite</className>
          <testName>groupByKeyAndWindow</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.441</duration>
          <className>org.apache.spark.streaming.WindowOperationsSuite</className>
          <testName>countByWindow</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.348</duration>
          <className>org.apache.spark.streaming.WindowOperationsSuite</className>
          <testName>countByValueAndWindow</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.api.java.JavaStreamingListenerWrapperSuite.xml</file>
      <name>org.apache.spark.streaming.api.java.JavaStreamingListenerWrapperSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.019</duration>
      <cases>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.streaming.api.java.JavaStreamingListenerWrapperSuite</className>
          <testName>basic</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.rdd.MapWithStateRDDSuite.xml</file>
      <name>org.apache.spark.streaming.rdd.MapWithStateRDDSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.671</duration>
      <cases>
        <case>
          <duration>0.172</duration>
          <className>org.apache.spark.streaming.rdd.MapWithStateRDDSuite</className>
          <testName>creation from pair RDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.streaming.rdd.MapWithStateRDDSuite</className>
          <testName>updating state and generating mapped data in MapWithStateRDDRecord</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.15</duration>
          <className>org.apache.spark.streaming.rdd.MapWithStateRDDSuite</className>
          <testName>states generated by MapWithStateRDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.054</duration>
          <className>org.apache.spark.streaming.rdd.MapWithStateRDDSuite</className>
          <testName>checkpointing</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.289</duration>
          <className>org.apache.spark.streaming.rdd.MapWithStateRDDSuite</className>
          <testName>checkpointing empty state RDD</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.rdd.WriteAheadLogBackedBlockRDDSuite.xml</file>
      <name>org.apache.spark.streaming.rdd.WriteAheadLogBackedBlockRDDSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.089</duration>
      <cases>
        <case>
          <duration>0.24</duration>
          <className>org.apache.spark.streaming.rdd.WriteAheadLogBackedBlockRDDSuite</className>
          <testName>Read data available in both block manager and write ahead log</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.134</duration>
          <className>org.apache.spark.streaming.rdd.WriteAheadLogBackedBlockRDDSuite</className>
          <testName>Read data available only in block manager, not in write ahead log</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.114</duration>
          <className>org.apache.spark.streaming.rdd.WriteAheadLogBackedBlockRDDSuite</className>
          <testName>Read data available only in write ahead log, not in block manager</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.096</duration>
          <className>org.apache.spark.streaming.rdd.WriteAheadLogBackedBlockRDDSuite</className>
          <testName>Read data with partially available in block manager, and rest in write ahead log</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.161</duration>
          <className>org.apache.spark.streaming.rdd.WriteAheadLogBackedBlockRDDSuite</className>
          <testName>Test isBlockValid skips block fetching from BlockManager</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.185</duration>
          <className>org.apache.spark.streaming.rdd.WriteAheadLogBackedBlockRDDSuite</className>
          <testName>Test whether RDD is valid after removing blocks from block manager</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.159</duration>
          <className>org.apache.spark.streaming.rdd.WriteAheadLogBackedBlockRDDSuite</className>
          <testName>Test storing of blocks recovered from write ahead log back into block manager</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.receiver.BlockGeneratorSuite.xml</file>
      <name>org.apache.spark.streaming.receiver.BlockGeneratorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.28899997</duration>
      <cases>
        <case>
          <duration>0.026</duration>
          <className>org.apache.spark.streaming.receiver.BlockGeneratorSuite</className>
          <testName>block generation and data callbacks</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.234</duration>
          <className>org.apache.spark.streaming.receiver.BlockGeneratorSuite</className>
          <testName>stop ensures correct shutdown</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.streaming.receiver.BlockGeneratorSuite</className>
          <testName>block push errors are reported</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.receiver.RateLimiterSuite.xml</file>
      <name>org.apache.spark.streaming.receiver.RateLimiterSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.003</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.streaming.receiver.RateLimiterSuite</className>
          <testName>rate limiter initializes even without a maxRate set</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.streaming.receiver.RateLimiterSuite</className>
          <testName>rate limiter updates when below maxRate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.streaming.receiver.RateLimiterSuite</className>
          <testName>rate limiter stays below maxRate despite large updates</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.scheduler.ExecutorAllocationManagerSuite.xml</file>
      <name>org.apache.spark.streaming.scheduler.ExecutorAllocationManagerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.328</duration>
      <cases>
        <case>
          <duration>0.057</duration>
          <className>org.apache.spark.streaming.scheduler.ExecutorAllocationManagerSuite</className>
          <testName>basic functionality</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.streaming.scheduler.ExecutorAllocationManagerSuite</className>
          <testName>requestExecutors policy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.streaming.scheduler.ExecutorAllocationManagerSuite</className>
          <testName>killExecutor policy</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.015</duration>
          <className>org.apache.spark.streaming.scheduler.ExecutorAllocationManagerSuite</className>
          <testName>parameter validation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.234</duration>
          <className>org.apache.spark.streaming.scheduler.ExecutorAllocationManagerSuite</className>
          <testName>enabling and disabling</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.scheduler.InputInfoTrackerSuite.xml</file>
      <name>org.apache.spark.streaming.scheduler.InputInfoTrackerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.002</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.streaming.scheduler.InputInfoTrackerSuite</className>
          <testName>test report and get InputInfo from InputInfoTracker</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.streaming.scheduler.InputInfoTrackerSuite</className>
          <testName>test cleanup InputInfo from InputInfoTracker</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.scheduler.JobGeneratorSuite.xml</file>
      <name>org.apache.spark.streaming.scheduler.JobGeneratorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>3.797</duration>
      <cases>
        <case>
          <duration>3.797</duration>
          <className>org.apache.spark.streaming.scheduler.JobGeneratorSuite</className>
          <testName>SPARK-6222: Do not clear received block data too soon</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.scheduler.RateControllerSuite.xml</file>
      <name>org.apache.spark.streaming.scheduler.RateControllerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.144</duration>
      <cases>
        <case>
          <duration>0.55</duration>
          <className>org.apache.spark.streaming.scheduler.RateControllerSuite</className>
          <testName>RateController - rate controller publishes updates after batches complete</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.594</duration>
          <className>org.apache.spark.streaming.scheduler.RateControllerSuite</className>
          <testName>ReceiverRateController - published rates reach receivers</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.scheduler.ReceiverSchedulingPolicySuite.xml</file>
      <name>org.apache.spark.streaming.scheduler.ReceiverSchedulingPolicySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.024</duration>
      <cases>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.scheduler.ReceiverSchedulingPolicySuite</className>
          <testName>rescheduleReceiver: empty executors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.streaming.scheduler.ReceiverSchedulingPolicySuite</className>
          <testName>rescheduleReceiver: receiver preferredLocation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.streaming.scheduler.ReceiverSchedulingPolicySuite</className>
          <testName>rescheduleReceiver: return all idle executors if there are any idle executors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.streaming.scheduler.ReceiverSchedulingPolicySuite</className>
          <testName>rescheduleReceiver: return all executors that have minimum weight if no idle executors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.streaming.scheduler.ReceiverSchedulingPolicySuite</className>
          <testName>scheduleReceivers: schedule receivers evenly when there are more receivers than executors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.streaming.scheduler.ReceiverSchedulingPolicySuite</className>
          <testName>scheduleReceivers: schedule receivers evenly when there are more executors than receivers</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.streaming.scheduler.ReceiverSchedulingPolicySuite</className>
          <testName>scheduleReceivers: schedule receivers evenly when the preferredLocations are even</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.scheduler.ReceiverSchedulingPolicySuite</className>
          <testName>scheduleReceivers: return empty if no receiver</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.streaming.scheduler.ReceiverSchedulingPolicySuite</className>
          <testName>scheduleReceivers: return empty scheduled executors if no executors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.scheduler.ReceiverTrackerSuite.xml</file>
      <name>org.apache.spark.streaming.scheduler.ReceiverTrackerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>2.072</duration>
      <cases>
        <case>
          <duration>0.431</duration>
          <className>org.apache.spark.streaming.scheduler.ReceiverTrackerSuite</className>
          <testName>send rate update to receivers</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.8</duration>
          <className>org.apache.spark.streaming.scheduler.ReceiverTrackerSuite</className>
          <testName>should restart receiver after stopping it</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.394</duration>
          <className>org.apache.spark.streaming.scheduler.ReceiverTrackerSuite</className>
          <testName>SPARK-11063: TaskSetManager should use Receiver RDD&apos;s preferredLocations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.447</duration>
          <className>org.apache.spark.streaming.scheduler.ReceiverTrackerSuite</className>
          <testName>get allocated executors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.scheduler.rate.PIDRateEstimatorSuite.xml</file>
      <name>org.apache.spark.streaming.scheduler.rate.PIDRateEstimatorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.032</duration>
      <cases>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.streaming.scheduler.rate.PIDRateEstimatorSuite</className>
          <testName>the right estimator is created</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.streaming.scheduler.rate.PIDRateEstimatorSuite</className>
          <testName>estimator checks ranges</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.streaming.scheduler.rate.PIDRateEstimatorSuite</className>
          <testName>first estimate is None</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.streaming.scheduler.rate.PIDRateEstimatorSuite</className>
          <testName>second estimate is not None</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.streaming.scheduler.rate.PIDRateEstimatorSuite</className>
          <testName>no estimate when no time difference between successive calls</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.scheduler.rate.PIDRateEstimatorSuite</className>
          <testName>no estimate when no records in previous batch</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.streaming.scheduler.rate.PIDRateEstimatorSuite</className>
          <testName>no estimate when there is no processing delay</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.streaming.scheduler.rate.PIDRateEstimatorSuite</className>
          <testName>estimate is never less than min rate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.streaming.scheduler.rate.PIDRateEstimatorSuite</className>
          <testName>with no accumulated or positive error, |I| &gt; 0, follow the processing speed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.streaming.scheduler.rate.PIDRateEstimatorSuite</className>
          <testName>with no accumulated but some positive error, |I| &gt; 0, follow the processing speed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.streaming.scheduler.rate.PIDRateEstimatorSuite</className>
          <testName>with some accumulated and some positive error, |I| &gt; 0, stay below the processing speed</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.ui.StreamingJobProgressListenerSuite.xml</file>
      <name>org.apache.spark.streaming.ui.StreamingJobProgressListenerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.32200003</duration>
      <cases>
        <case>
          <duration>0.039</duration>
          <className>org.apache.spark.streaming.ui.StreamingJobProgressListenerSuite</className>
          <testName>onBatchSubmitted, onBatchStarted, onBatchCompleted, onReceiverStarted, onReceiverError, onReceiverStopped</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.streaming.ui.StreamingJobProgressListenerSuite</className>
          <testName>Remove the old completed batches when exceeding the limit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.078</duration>
          <className>org.apache.spark.streaming.ui.StreamingJobProgressListenerSuite</className>
          <testName>out-of-order onJobStart and onBatchXXX</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.158</duration>
          <className>org.apache.spark.streaming.ui.StreamingJobProgressListenerSuite</className>
          <testName>detect memory leak</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.ui.UIUtilsSuite.xml</file>
      <name>org.apache.spark.streaming.ui.UIUtilsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.017</duration>
      <cases>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.streaming.ui.UIUtilsSuite</className>
          <testName>shortTimeUnitString</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.streaming.ui.UIUtilsSuite</className>
          <testName>normalizeDuration</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.ui.UIUtilsSuite</className>
          <testName>convertToTimeUnit</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.streaming.ui.UIUtilsSuite</className>
          <testName>formatBatchTime</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.util.BatchedWriteAheadLogSuite.xml</file>
      <name>org.apache.spark.streaming.util.BatchedWriteAheadLogSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.34100002</duration>
      <cases>
        <case>
          <duration>0.032</duration>
          <className>org.apache.spark.streaming.util.BatchedWriteAheadLogSuite</className>
          <testName>BatchedWriteAheadLog - read all logs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.034</duration>
          <className>org.apache.spark.streaming.util.BatchedWriteAheadLogSuite</className>
          <testName>BatchedWriteAheadLog - write logs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.035</duration>
          <className>org.apache.spark.streaming.util.BatchedWriteAheadLogSuite</className>
          <testName>BatchedWriteAheadLog - read all logs after write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.streaming.util.BatchedWriteAheadLogSuite</className>
          <testName>BatchedWriteAheadLog - clean old logs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.streaming.util.BatchedWriteAheadLogSuite</className>
          <testName>BatchedWriteAheadLog - clean old logs synchronously</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.089</duration>
          <className>org.apache.spark.streaming.util.BatchedWriteAheadLogSuite</className>
          <testName>BatchedWriteAheadLog - handling file errors while reading rotating logs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.streaming.util.BatchedWriteAheadLogSuite</className>
          <testName>BatchedWriteAheadLog - do not create directories or files unless write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.streaming.util.BatchedWriteAheadLogSuite</className>
          <testName>BatchedWriteAheadLog - parallel recovery not enabled if closeFileAfterWrite = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.streaming.util.BatchedWriteAheadLogSuite</className>
          <testName>BatchedWriteAheadLog - serializing and deserializing batched records</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.045</duration>
          <className>org.apache.spark.streaming.util.BatchedWriteAheadLogSuite</className>
          <testName>BatchedWriteAheadLog - failures in wrappedLog get bubbled up</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.018</duration>
          <className>org.apache.spark.streaming.util.BatchedWriteAheadLogSuite</className>
          <testName>BatchedWriteAheadLog - name log with the highest timestamp of aggregated entries</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.streaming.util.BatchedWriteAheadLogSuite</className>
          <testName>BatchedWriteAheadLog - shutdown properly</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.streaming.util.BatchedWriteAheadLogSuite</className>
          <testName>BatchedWriteAheadLog - fail everything in queue during shutdown</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.util.BatchedWriteAheadLogWithCloseFileAfterWriteSuite.xml</file>
      <name>org.apache.spark.streaming.util.BatchedWriteAheadLogWithCloseFileAfterWriteSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.45200002</duration>
      <cases>
        <case>
          <duration>0.05</duration>
          <className>org.apache.spark.streaming.util.BatchedWriteAheadLogWithCloseFileAfterWriteSuite</className>
          <testName>BatchedWriteAheadLog - read all logs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.061</duration>
          <className>org.apache.spark.streaming.util.BatchedWriteAheadLogWithCloseFileAfterWriteSuite</className>
          <testName>BatchedWriteAheadLog - write logs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.07</duration>
          <className>org.apache.spark.streaming.util.BatchedWriteAheadLogWithCloseFileAfterWriteSuite</className>
          <testName>BatchedWriteAheadLog - read all logs after write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.045</duration>
          <className>org.apache.spark.streaming.util.BatchedWriteAheadLogWithCloseFileAfterWriteSuite</className>
          <testName>BatchedWriteAheadLog - clean old logs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.047</duration>
          <className>org.apache.spark.streaming.util.BatchedWriteAheadLogWithCloseFileAfterWriteSuite</className>
          <testName>BatchedWriteAheadLog - clean old logs synchronously</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.165</duration>
          <className>org.apache.spark.streaming.util.BatchedWriteAheadLogWithCloseFileAfterWriteSuite</className>
          <testName>BatchedWriteAheadLog - handling file errors while reading rotating logs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.streaming.util.BatchedWriteAheadLogWithCloseFileAfterWriteSuite</className>
          <testName>BatchedWriteAheadLog - do not create directories or files unless write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.streaming.util.BatchedWriteAheadLogWithCloseFileAfterWriteSuite</className>
          <testName>BatchedWriteAheadLog - parallel recovery not enabled if closeFileAfterWrite = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.streaming.util.BatchedWriteAheadLogWithCloseFileAfterWriteSuite</className>
          <testName>BatchedWriteAheadLog - close after write flag</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.util.FileBasedWriteAheadLogSuite.xml</file>
      <name>org.apache.spark.streaming.util.FileBasedWriteAheadLogSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.922</duration>
      <cases>
        <case>
          <duration>0.078</duration>
          <className>org.apache.spark.streaming.util.FileBasedWriteAheadLogSuite</className>
          <testName>FileBasedWriteAheadLog - read all logs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.027</duration>
          <className>org.apache.spark.streaming.util.FileBasedWriteAheadLogSuite</className>
          <testName>FileBasedWriteAheadLog - write logs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.streaming.util.FileBasedWriteAheadLogSuite</className>
          <testName>FileBasedWriteAheadLog - read all logs after write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.streaming.util.FileBasedWriteAheadLogSuite</className>
          <testName>FileBasedWriteAheadLog - clean old logs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.019</duration>
          <className>org.apache.spark.streaming.util.FileBasedWriteAheadLogSuite</className>
          <testName>FileBasedWriteAheadLog - clean old logs synchronously</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.078</duration>
          <className>org.apache.spark.streaming.util.FileBasedWriteAheadLogSuite</className>
          <testName>FileBasedWriteAheadLog - handling file errors while reading rotating logs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.streaming.util.FileBasedWriteAheadLogSuite</className>
          <testName>FileBasedWriteAheadLog - do not create directories or files unless write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.streaming.util.FileBasedWriteAheadLogSuite</className>
          <testName>FileBasedWriteAheadLog - parallel recovery not enabled if closeFileAfterWrite = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.133</duration>
          <className>org.apache.spark.streaming.util.FileBasedWriteAheadLogSuite</className>
          <testName>FileBasedWriteAheadLog - seqToParIterator</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.024</duration>
          <className>org.apache.spark.streaming.util.FileBasedWriteAheadLogSuite</className>
          <testName>FileBasedWriteAheadLogWriter - writing data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.025</duration>
          <className>org.apache.spark.streaming.util.FileBasedWriteAheadLogSuite</className>
          <testName>FileBasedWriteAheadLogWriter - syncing of data by writing and reading immediately</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.streaming.util.FileBasedWriteAheadLogSuite</className>
          <testName>FileBasedWriteAheadLogReader - sequentially reading data</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.streaming.util.FileBasedWriteAheadLogSuite</className>
          <testName>FileBasedWriteAheadLogReader - sequentially reading data written with writer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.424</duration>
          <className>org.apache.spark.streaming.util.FileBasedWriteAheadLogSuite</className>
          <testName>FileBasedWriteAheadLogReader - reading data written with writer after corrupted write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.streaming.util.FileBasedWriteAheadLogSuite</className>
          <testName>FileBasedWriteAheadLogReader - handles errors when file doesn&apos;t exist</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.014</duration>
          <className>org.apache.spark.streaming.util.FileBasedWriteAheadLogSuite</className>
          <testName>FileBasedWriteAheadLogRandomReader - reading data using random reader</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.streaming.util.FileBasedWriteAheadLogSuite</className>
          <testName>FileBasedWriteAheadLogRandomReader- reading data using random reader written with writer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.util.FileBasedWriteAheadLogWithFileCloseAfterWriteSuite.xml</file>
      <name>org.apache.spark.streaming.util.FileBasedWriteAheadLogWithFileCloseAfterWriteSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.405</duration>
      <cases>
        <case>
          <duration>0.042</duration>
          <className>org.apache.spark.streaming.util.FileBasedWriteAheadLogWithFileCloseAfterWriteSuite</className>
          <testName>FileBasedWriteAheadLog - read all logs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.051</duration>
          <className>org.apache.spark.streaming.util.FileBasedWriteAheadLogWithFileCloseAfterWriteSuite</className>
          <testName>FileBasedWriteAheadLog - write logs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.052</duration>
          <className>org.apache.spark.streaming.util.FileBasedWriteAheadLogWithFileCloseAfterWriteSuite</className>
          <testName>FileBasedWriteAheadLog - read all logs after write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.059</duration>
          <className>org.apache.spark.streaming.util.FileBasedWriteAheadLogWithFileCloseAfterWriteSuite</className>
          <testName>FileBasedWriteAheadLog - clean old logs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.044</duration>
          <className>org.apache.spark.streaming.util.FileBasedWriteAheadLogWithFileCloseAfterWriteSuite</className>
          <testName>FileBasedWriteAheadLog - clean old logs synchronously</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.141</duration>
          <className>org.apache.spark.streaming.util.FileBasedWriteAheadLogWithFileCloseAfterWriteSuite</className>
          <testName>FileBasedWriteAheadLog - handling file errors while reading rotating logs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.streaming.util.FileBasedWriteAheadLogWithFileCloseAfterWriteSuite</className>
          <testName>FileBasedWriteAheadLog - do not create directories or files unless write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.streaming.util.FileBasedWriteAheadLogWithFileCloseAfterWriteSuite</className>
          <testName>FileBasedWriteAheadLog - parallel recovery not enabled if closeFileAfterWrite = false</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.streaming.util.FileBasedWriteAheadLogWithFileCloseAfterWriteSuite</className>
          <testName>FileBasedWriteAheadLog - close after write flag</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.util.RateLimitedOutputStreamSuite.xml</file>
      <name>org.apache.spark.streaming.util.RateLimitedOutputStreamSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>4.103</duration>
      <cases>
        <case>
          <duration>4.103</duration>
          <className>org.apache.spark.streaming.util.RateLimitedOutputStreamSuite</className>
          <testName>write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.util.RecurringTimerSuite.xml</file>
      <name>org.apache.spark.streaming.util.RecurringTimerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.007</duration>
      <cases>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.streaming.util.RecurringTimerSuite</className>
          <testName>basic</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.streaming.util.RecurringTimerSuite</className>
          <testName>SPARK-10224: call &apos;callback&apos; after stopping</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/streaming/target/test-reports/org.apache.spark.streaming.util.WriteAheadLogUtilsSuite.xml</file>
      <name>org.apache.spark.streaming.util.WriteAheadLogUtilsSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.021</duration>
      <cases>
        <case>
          <duration>0.018</duration>
          <className>org.apache.spark.streaming.util.WriteAheadLogUtilsSuite</className>
          <testName>log selection and creation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.streaming.util.WriteAheadLogUtilsSuite</className>
          <testName>wrap WriteAheadLog in BatchedWriteAheadLog when batching is enabled</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.0</duration>
          <className>org.apache.spark.streaming.util.WriteAheadLogUtilsSuite</className>
          <testName>batching is enabled by default in WriteAheadLog</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.streaming.util.WriteAheadLogUtilsSuite</className>
          <testName>closeFileAfterWrite is disabled by default in WriteAheadLog</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/yarn/target/test-reports/org.apache.spark.deploy.yarn.ClientDistributedCacheManagerSuite.xml</file>
      <name>org.apache.spark.deploy.yarn.ClientDistributedCacheManagerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.32599998</duration>
      <cases>
        <case>
          <duration>0.241</duration>
          <className>org.apache.spark.deploy.yarn.ClientDistributedCacheManagerSuite</className>
          <testName>test getFileStatus empty</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.deploy.yarn.ClientDistributedCacheManagerSuite</className>
          <testName>test getFileStatus cached</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.08</duration>
          <className>org.apache.spark.deploy.yarn.ClientDistributedCacheManagerSuite</className>
          <testName>test addResource</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.deploy.yarn.ClientDistributedCacheManagerSuite</className>
          <testName>test addResource link null</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.deploy.yarn.ClientDistributedCacheManagerSuite</className>
          <testName>test addResource appmaster only</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.deploy.yarn.ClientDistributedCacheManagerSuite</className>
          <testName>test addResource archive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/yarn/target/test-reports/org.apache.spark.deploy.yarn.ClientSuite.xml</file>
      <name>org.apache.spark.deploy.yarn.ClientSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.406</duration>
      <cases>
        <case>
          <duration>0.011</duration>
          <className>org.apache.spark.deploy.yarn.ClientSuite</className>
          <testName>default Yarn application classpath</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.001</duration>
          <className>org.apache.spark.deploy.yarn.ClientSuite</className>
          <testName>default MR application classpath</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.021</duration>
          <className>org.apache.spark.deploy.yarn.ClientSuite</className>
          <testName>resultant classpath for an application that defines a classpath for YARN</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.018</duration>
          <className>org.apache.spark.deploy.yarn.ClientSuite</className>
          <testName>resultant classpath for an application that defines a classpath for MR</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.deploy.yarn.ClientSuite</className>
          <testName>resultant classpath for an application that defines both classpaths, YARN and MR</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.05</duration>
          <className>org.apache.spark.deploy.yarn.ClientSuite</className>
          <testName>Local jar URIs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.15</duration>
          <className>org.apache.spark.deploy.yarn.ClientSuite</className>
          <testName>Jar path propagation through SparkConf</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.deploy.yarn.ClientSuite</className>
          <testName>Cluster path translation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.058</duration>
          <className>org.apache.spark.deploy.yarn.ClientSuite</className>
          <testName>configuration and args propagate through createApplicationSubmissionContext</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.239</duration>
          <className>org.apache.spark.deploy.yarn.ClientSuite</className>
          <testName>jars with multiple paths and globs</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.093</duration>
          <className>org.apache.spark.deploy.yarn.ClientSuite</className>
          <testName>distribute jars archive</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.517</duration>
          <className>org.apache.spark.deploy.yarn.ClientSuite</className>
          <testName>distribute archive multiple times</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.078</duration>
          <className>org.apache.spark.deploy.yarn.ClientSuite</className>
          <testName>distribute local spark jars</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.126</duration>
          <className>org.apache.spark.deploy.yarn.ClientSuite</className>
          <testName>ignore same name jars</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/yarn/target/test-reports/org.apache.spark.deploy.yarn.ContainerPlacementStrategySuite.xml</file>
      <name>org.apache.spark.deploy.yarn.ContainerPlacementStrategySuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.038000003</duration>
      <cases>
        <case>
          <duration>0.012</duration>
          <className>org.apache.spark.deploy.yarn.ContainerPlacementStrategySuite</className>
          <testName>allocate locality preferred containers with enough resource and no matched existed containers</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.deploy.yarn.ContainerPlacementStrategySuite</className>
          <testName>allocate locality preferred containers with enough resource and partially matched containers</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.deploy.yarn.ContainerPlacementStrategySuite</className>
          <testName>allocate locality preferred containers with limited resource and partially matched containers</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.deploy.yarn.ContainerPlacementStrategySuite</className>
          <testName>allocate locality preferred containers with fully matched containers</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.deploy.yarn.ContainerPlacementStrategySuite</className>
          <testName>allocate containers with no locality preference</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.008</duration>
          <className>org.apache.spark.deploy.yarn.ContainerPlacementStrategySuite</className>
          <testName>allocate locality preferred containers by considering the localities of pending requests</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/yarn/target/test-reports/org.apache.spark.deploy.yarn.IOEncryptionSuite.xml</file>
      <name>org.apache.spark.deploy.yarn.IOEncryptionSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.131</duration>
      <cases>
        <case>
          <duration>0.102</duration>
          <className>org.apache.spark.deploy.yarn.IOEncryptionSuite</className>
          <testName>IO encryption read and write</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.029</duration>
          <className>org.apache.spark.deploy.yarn.IOEncryptionSuite</className>
          <testName>IO encryption read and write with shuffle compression enabled</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/yarn/target/test-reports/org.apache.spark.deploy.yarn.YarnAllocatorSuite.xml</file>
      <name>org.apache.spark.deploy.yarn.YarnAllocatorSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.161</duration>
      <cases>
        <case>
          <duration>0.094</duration>
          <className>org.apache.spark.deploy.yarn.YarnAllocatorSuite</className>
          <testName>single container allocated</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.deploy.yarn.YarnAllocatorSuite</className>
          <testName>container should not be created if requested number if met</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.deploy.yarn.YarnAllocatorSuite</className>
          <testName>some containers allocated</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.deploy.yarn.YarnAllocatorSuite</className>
          <testName>receive more containers than requested</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.deploy.yarn.YarnAllocatorSuite</className>
          <testName>decrease total requested executors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.deploy.yarn.YarnAllocatorSuite</className>
          <testName>decrease total requested executors to less than currently running</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.deploy.yarn.YarnAllocatorSuite</className>
          <testName>kill executors</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.01</duration>
          <className>org.apache.spark.deploy.yarn.YarnAllocatorSuite</className>
          <testName>lost executor removed from backend</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.deploy.yarn.YarnAllocatorSuite</className>
          <testName>memory exceeded diagnostic regexes</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.deploy.yarn.YarnAllocatorSuite</className>
          <testName>window based failure executor counting</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/yarn/target/test-reports/org.apache.spark.deploy.yarn.YarnSparkHadoopUtilSuite.xml</file>
      <name>org.apache.spark.deploy.yarn.YarnSparkHadoopUtilSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.262</duration>
      <cases>
        <case>
          <duration>0.526</duration>
          <className>org.apache.spark.deploy.yarn.YarnSparkHadoopUtilSuite</className>
          <testName>shell script escaping</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.596</duration>
          <className>org.apache.spark.deploy.yarn.YarnSparkHadoopUtilSuite</className>
          <testName>Yarn configuration override</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.03</duration>
          <className>org.apache.spark.deploy.yarn.YarnSparkHadoopUtilSuite</className>
          <testName>test getApplicationAclsForYarn acls on</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.deploy.yarn.YarnSparkHadoopUtilSuite</className>
          <testName>test getApplicationAclsForYarn acls on and specify users</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.003</duration>
          <className>org.apache.spark.deploy.yarn.YarnSparkHadoopUtilSuite</className>
          <testName>test expandEnvironment result</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.deploy.yarn.YarnSparkHadoopUtilSuite</className>
          <testName>test getClassPathSeparator result</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.076</duration>
          <className>org.apache.spark.deploy.yarn.YarnSparkHadoopUtilSuite</className>
          <testName>check different hadoop utils based on env variable</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.023</duration>
          <className>org.apache.spark.deploy.yarn.YarnSparkHadoopUtilSuite</className>
          <testName>security manager token generation</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/yarn/target/test-reports/org.apache.spark.deploy.yarn.security.ConfigurableCredentialManagerSuite.xml</file>
      <name>org.apache.spark.deploy.yarn.security.ConfigurableCredentialManagerSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>11.288</duration>
      <cases>
        <case>
          <duration>0.016</duration>
          <className>org.apache.spark.deploy.yarn.security.ConfigurableCredentialManagerSuite</className>
          <testName>Correctly load default credential providers</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.deploy.yarn.security.ConfigurableCredentialManagerSuite</className>
          <testName>disable hive credential provider</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.004</duration>
          <className>org.apache.spark.deploy.yarn.security.ConfigurableCredentialManagerSuite</className>
          <testName>using deprecated configurations</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>1.732</duration>
          <className>org.apache.spark.deploy.yarn.security.ConfigurableCredentialManagerSuite</className>
          <testName>verify obtaining credentials from provider</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.005</duration>
          <className>org.apache.spark.deploy.yarn.security.ConfigurableCredentialManagerSuite</className>
          <testName>verify getting credential renewal info</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>9.501</duration>
          <className>org.apache.spark.deploy.yarn.security.ConfigurableCredentialManagerSuite</className>
          <testName>obtain tokens For HiveMetastore</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.deploy.yarn.security.ConfigurableCredentialManagerSuite</className>
          <testName>Obtain tokens For HBase</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/yarn/target/test-reports/org.apache.spark.deploy.yarn.security.HDFSCredentialProviderSuite.xml</file>
      <name>org.apache.spark.deploy.yarn.security.HDFSCredentialProviderSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.048</duration>
      <cases>
        <case>
          <duration>0.028</duration>
          <className>org.apache.spark.deploy.yarn.security.HDFSCredentialProviderSuite</className>
          <testName>check token renewer</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.02</duration>
          <className>org.apache.spark.deploy.yarn.security.HDFSCredentialProviderSuite</className>
          <testName>check token renewer default</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/yarn/target/test-reports/org.apache.spark.network.yarn.YarnShuffleServiceSuite.xml</file>
      <name>org.apache.spark.network.yarn.YarnShuffleServiceSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>1.0130001</duration>
      <cases>
        <case>
          <duration>0.771</duration>
          <className>org.apache.spark.network.yarn.YarnShuffleServiceSuite</className>
          <testName>executor state kept across NM restart</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.022</duration>
          <className>org.apache.spark.network.yarn.YarnShuffleServiceSuite</className>
          <testName>removed applications should not be in registered executor file</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.072</duration>
          <className>org.apache.spark.network.yarn.YarnShuffleServiceSuite</className>
          <testName>shuffle service should be robust to corrupt registered executor file</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.044</duration>
          <className>org.apache.spark.network.yarn.YarnShuffleServiceSuite</className>
          <testName>get correct recovery path</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.098</duration>
          <className>org.apache.spark.network.yarn.YarnShuffleServiceSuite</className>
          <testName>moving recovery file from NM local dir to recovery path</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.006</duration>
          <className>org.apache.spark.network.yarn.YarnShuffleServiceSuite</className>
          <testName>service throws error if cannot start</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
    <suite>
      <file>/usr1/ci-home/workspace/V100R002C30_Spark_UT_Test_2.0/yarn/target/test-reports/org.apache.spark.scheduler.cluster.ExtensionServiceIntegrationSuite.xml</file>
      <name>org.apache.spark.scheduler.cluster.ExtensionServiceIntegrationSuite</name>
      <stdout></stdout>
      <stderr></stderr>
      <duration>0.009000001</duration>
      <cases>
        <case>
          <duration>0.007</duration>
          <className>org.apache.spark.scheduler.cluster.ExtensionServiceIntegrationSuite</className>
          <testName>Instantiate</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
        <case>
          <duration>0.002</duration>
          <className>org.apache.spark.scheduler.cluster.ExtensionServiceIntegrationSuite</className>
          <testName>Contains SimpleExtensionService Service</testName>
          <skipped>false</skipped>
          <failedSince>0</failedSince>
        </case>
      </cases>
    </suite>
  </suites>
  <duration>5888.589</duration>
  <keepLongStdio>false</keepLongStdio>
</result>
